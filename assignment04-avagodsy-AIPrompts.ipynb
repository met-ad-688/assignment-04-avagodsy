{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfd41ce",
   "metadata": {},
   "source": [
    "---\n",
    "title: Assignment 04 AI Prompts\n",
    "author:\n",
    "  - name: Ava Godsy\n",
    "    affiliations:\n",
    "      - id: bu\n",
    "        name: Boston University\n",
    "        city: Boston\n",
    "        state: MA\n",
    "number-sections: true\n",
    "date: today\n",
    "date-modified: today\n",
    "date-format: long\n",
    "format:\n",
    "  html:\n",
    "    theme: cerulean\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "  docx: default\n",
    "  pdf: default\n",
    "execute:\n",
    "  echo: false\n",
    "  eval: false\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08351257",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## AI Prompt\n",
    "create a dataframe using python with the following: from dataframe df Drop rows with missing values in the target variable and key features.\n",
    "By now you are already familiar with the code and the data. Based on your understanding please choose any 3 (my code output has 10) variables as: 1. choose variables: MIN_YEARS_EXPERIENCE , MAX_YEARS_EXPERIENCE, SALARY_FROM, SALARY_TO, MSA_NAME, REMOTE_TYPE_NAME 2. Your dependent variable (y) is SALARY.\n",
    "Convert MSA_NAME, REMOTE_TYPE_NAME into numerical representations using StringIndexer and OneHotEncoder.\n",
    "Assemble features into a single vector using VectorAssembler.\n",
    "Split the data into training and testing sets.\n",
    "You can use pipeline to do the above steps in one go.\n",
    "Create a new column MIN_YEARS_EXPERIENCE_SQ by squaring the MIN_YEARS_EXPERIENCE column.\n",
    "Assemble the polynomial features into a new vector column features_poly using VectorAssembler.\n",
    "Show the final structure of the DataFrame with the new features.\n",
    "\n",
    "## Claude Response\n",
    "I'll create a Python script that performs all these data preprocessing steps. Since you mentioned you're working with a DataFrame df, I'll create a comprehensive solution using PySpark (based on your mention of StringIndexer, OneHotEncoder, and VectorAssembler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62981dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|echo: false\n",
    "#|eval: false\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col, pow as spark_pow\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Spark Session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"SalaryPrediction\").getOrCreate()\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "# Step 1: Drop rows with missing values in target and key features\n",
    "selected_columns = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                   'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY']\n",
    "\n",
    "df_clean = df.select(selected_columns).dropna()\n",
    "\n",
    "print(\"Original DataFrame count:\", df.count())\n",
    "print(\"Cleaned DataFrame count:\", df_clean.count())\n",
    "print(\"\\nCleaned DataFrame Schema:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Step 2: Create squared feature for MIN_YEARS_EXPERIENCE\n",
    "df_clean = df_clean.withColumn('MIN_YEARS_EXPERIENCE_SQ', \n",
    "                               spark_pow(col('MIN_YEARS_EXPERIENCE'), 2))\n",
    "\n",
    "print(\"\\nDataFrame with squared feature:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# Step 3: Create Pipeline for encoding and feature assembly\n",
    "\n",
    "# StringIndexer for categorical variables\n",
    "msa_indexer = StringIndexer(inputCol='MSA_NAME', \n",
    "                            outputCol='MSA_NAME_INDEX',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "remote_indexer = StringIndexer(inputCol='REMOTE_TYPE_NAME', \n",
    "                               outputCol='REMOTE_TYPE_NAME_INDEX',\n",
    "                               handleInvalid='keep')\n",
    "\n",
    "# OneHotEncoder for categorical variables\n",
    "msa_encoder = OneHotEncoder(inputCol='MSA_NAME_INDEX', \n",
    "                           outputCol='MSA_NAME_VEC',\n",
    "                           dropLast=True)\n",
    "\n",
    "remote_encoder = OneHotEncoder(inputCol='REMOTE_TYPE_NAME_INDEX', \n",
    "                              outputCol='REMOTE_TYPE_NAME_VEC',\n",
    "                              dropLast=True)\n",
    "\n",
    "# VectorAssembler for basic features\n",
    "feature_cols = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "               'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, \n",
    "                            outputCol='features',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "# VectorAssembler for polynomial features\n",
    "poly_feature_cols = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                    'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                    'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "poly_assembler = VectorAssembler(inputCols=poly_feature_cols, \n",
    "                                outputCol='features_poly',\n",
    "                                handleInvalid='keep')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    msa_indexer,\n",
    "    remote_indexer,\n",
    "    msa_encoder,\n",
    "    remote_encoder,\n",
    "    assembler,\n",
    "    poly_assembler\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformed DataFrame with all features:\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY',\n",
    "                     'features', 'features_poly').show(5, truncate=False)\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nTraining set count: {train_data.count()}\")\n",
    "print(f\"Testing set count: {test_data.count()}\")\n",
    "\n",
    "# Step 5: Show final structure\n",
    "print(\"\\n=== FINAL DATAFRAME STRUCTURE ===\")\n",
    "df_transformed.printSchema()\n",
    "\n",
    "print(\"\\n=== SAMPLE OF FINAL DATA ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                     'SALARY', 'features_poly').show(10)\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\n=== FEATURE STATISTICS ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'SALARY').describe().show()\n",
    "\n",
    "# Optional: Show unique values in categorical columns\n",
    "print(\"\\n=== CATEGORICAL VARIABLE COUNTS ===\")\n",
    "print(\"MSA_NAME unique values:\", df_clean.select('MSA_NAME').distinct().count())\n",
    "print(\"REMOTE_TYPE_NAME unique values:\", df_clean.select('REMOTE_TYPE_NAME').distinct().count())\n",
    "\n",
    "# Save transformed data for future use (optional)\n",
    "# df_transformed.write.parquet(\"transformed_salary_data.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"\\n✓ Data preprocessing pipeline completed successfully!\")\n",
    "print(\"✓ Ready for model training with 'features_poly' as input and 'SALARY' as target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18258ea",
   "metadata": {},
   "source": [
    "I've created a comprehensive PySpark data preprocessing pipeline that accomplishes all your requirements:\n",
    "Key Features:\n",
    "\n",
    "Data Cleaning: Drops rows with missing values in the selected features and target variable (SALARY)\n",
    "Selected Features:\n",
    "\n",
    "MIN_YEARS_EXPERIENCE\n",
    "MAX_YEARS_EXPERIENCE\n",
    "SALARY_FROM\n",
    "MSA_NAME (categorical)\n",
    "REMOTE_TYPE_NAME (categorical)\n",
    "\n",
    "\n",
    "Polynomial Feature: Creates MIN_YEARS_EXPERIENCE_SQ by squaring MIN_YEARS_EXPERIENCE\n",
    "Pipeline Components:\n",
    "\n",
    "StringIndexer for both categorical variables\n",
    "OneHotEncoder for converting indexed categories to vectors\n",
    "VectorAssembler for basic features → 'features' column\n",
    "VectorAssembler for polynomial features → 'features_poly' column\n",
    "\n",
    "\n",
    "Train-Test Split: 80-20 split with a random seed for reproducibility\n",
    "Final Output: Shows the complete structure with both 'features' and 'features_poly' vectors\n",
    "\n",
    "The pipeline approach ensures all transformations are applied consistently to both training and testing data. The 'features_poly' column includes the squared term and is ready for polynomial regression modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5656528",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "## Prompt\n",
    "(This is a continuation of the previous prompt)\n",
    "\n",
    "Perform a random split of the data into training and testing sets.\n",
    "Set a random seed for reproducibility.\n",
    "You can choose a number for splitting to your liking, justify your choice.\n",
    "\n",
    "## Claude Response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99892222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|echo: false\n",
    "#|eval: false\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col, pow as spark_pow\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Spark Session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"SalaryPrediction\").getOrCreate()\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "# Step 1: Drop rows with missing values in target and key features\n",
    "selected_columns = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                   'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY']\n",
    "\n",
    "df_clean = df.select(selected_columns).dropna()\n",
    "\n",
    "print(\"Original DataFrame count:\", df.count())\n",
    "print(\"Cleaned DataFrame count:\", df_clean.count())\n",
    "print(\"\\nCleaned DataFrame Schema:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Step 2: Create squared feature for MIN_YEARS_EXPERIENCE\n",
    "df_clean = df_clean.withColumn('MIN_YEARS_EXPERIENCE_SQ', \n",
    "                               spark_pow(col('MIN_YEARS_EXPERIENCE'), 2))\n",
    "\n",
    "print(\"\\nDataFrame with squared feature:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# Step 3: Create Pipeline for encoding and feature assembly\n",
    "\n",
    "# StringIndexer for categorical variables\n",
    "msa_indexer = StringIndexer(inputCol='MSA_NAME', \n",
    "                            outputCol='MSA_NAME_INDEX',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "remote_indexer = StringIndexer(inputCol='REMOTE_TYPE_NAME', \n",
    "                               outputCol='REMOTE_TYPE_NAME_INDEX',\n",
    "                               handleInvalid='keep')\n",
    "\n",
    "# OneHotEncoder for categorical variables\n",
    "msa_encoder = OneHotEncoder(inputCol='MSA_NAME_INDEX', \n",
    "                           outputCol='MSA_NAME_VEC',\n",
    "                           dropLast=True)\n",
    "\n",
    "remote_encoder = OneHotEncoder(inputCol='REMOTE_TYPE_NAME_INDEX', \n",
    "                              outputCol='REMOTE_TYPE_NAME_VEC',\n",
    "                              dropLast=True)\n",
    "\n",
    "# VectorAssembler for basic features\n",
    "feature_cols = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "               'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, \n",
    "                            outputCol='features',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "# VectorAssembler for polynomial features\n",
    "poly_feature_cols = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                    'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                    'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "poly_assembler = VectorAssembler(inputCols=poly_feature_cols, \n",
    "                                outputCol='features_poly',\n",
    "                                handleInvalid='keep')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    msa_indexer,\n",
    "    remote_indexer,\n",
    "    msa_encoder,\n",
    "    remote_encoder,\n",
    "    assembler,\n",
    "    poly_assembler\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformed DataFrame with all features:\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY',\n",
    "                     'features', 'features_poly').show(5, truncate=False)\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "# Using 70-30 split for better evaluation capability\n",
    "# Justification:\n",
    "# - 70% training: Provides sufficient data for model to learn patterns\n",
    "# - 30% testing: Larger test set gives more reliable performance metrics\n",
    "# - Good balance for datasets with moderate size (thousands of records)\n",
    "# - Seed=42 ensures reproducibility across runs\n",
    "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"\\n=== DATA SPLIT SUMMARY ===\")\n",
    "print(f\"Training set count: {train_data.count()} ({train_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "print(f\"Testing set count: {test_data.count()} ({test_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "print(f\"Random seed: 42 (for reproducibility)\")\n",
    "print(\"\\nSplit Justification:\")\n",
    "print(\"• 70-30 split provides robust model evaluation\")\n",
    "print(\"• Larger test set (30%) improves confidence in performance metrics\")\n",
    "print(\"• Balanced approach for moderate-sized datasets\")\n",
    "print(\"• Alternative splits: 80-20 for large datasets, 60-40 for small datasets\")\n",
    "\n",
    "# Step 5: Show final structure\n",
    "print(\"\\n=== FINAL DATAFRAME STRUCTURE ===\")\n",
    "df_transformed.printSchema()\n",
    "\n",
    "print(\"\\n=== SAMPLE OF FINAL DATA ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                     'SALARY', 'features_poly').show(10)\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\n=== FEATURE STATISTICS ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'SALARY').describe().show()\n",
    "\n",
    "# Optional: Show unique values in categorical columns\n",
    "print(\"\\n=== CATEGORICAL VARIABLE COUNTS ===\")\n",
    "print(\"MSA_NAME unique values:\", df_clean.select('MSA_NAME').distinct().count())\n",
    "print(\"REMOTE_TYPE_NAME unique values:\", df_clean.select('REMOTE_TYPE_NAME').distinct().count())\n",
    "\n",
    "# Save transformed data for future use (optional)\n",
    "# df_transformed.write.parquet(\"transformed_salary_data.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"\\n✓ Data preprocessing pipeline completed successfully!\")\n",
    "print(\"✓ Ready for model training with 'features_poly' as input and 'SALARY' as target\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
