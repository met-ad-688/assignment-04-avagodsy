{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c2536a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Assignment 04\n",
    "author:\n",
    "  - name: Ava Godsy\n",
    "    affiliations:\n",
    "      - id: bu\n",
    "        name: Boston University\n",
    "        city: Boston\n",
    "        state: MA\n",
    "number-sections: true\n",
    "date: today\n",
    "date-modified: today\n",
    "date-format: long\n",
    "format:\n",
    "  html:\n",
    "    theme: cerulean\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "  docx: default\n",
    "  pdf: default\n",
    "execute:\n",
    "  echo: false\n",
    "  eval: false\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1404da",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72b3d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/08 23:11:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, pow as spark_pow, when, trim\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import when, trim\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from scipy import stats as scipy_stats\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pio.renderers.default = \"notebook+notebook_connected+vscode\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"lightcast_job_postings.csv\")\n",
    "\n",
    "# Show Schema and Sample Data\n",
    "# print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n",
    "\n",
    "# df.printSchema() # comment this line when rendering the submission\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfbdd3",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80ab217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REMOTE_TYPE_NAME VALUE COUNTS AFTER COMBINING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|REMOTE_TYPE_NAME|count|\n",
      "+----------------+-----+\n",
      "|          Onsite|57741|\n",
      "|          Remote|12497|\n",
      "|          Hybrid| 2260|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame count: 72498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame count: 3596\n",
      "\n",
      "Cleaned DataFrame Schema:\n",
      "root\n",
      " |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- SALARY_FROM: integer (nullable = true)\n",
      " |-- MSA_NAME: string (nullable = true)\n",
      " |-- REMOTE_TYPE_NAME: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      "\n",
      "\n",
      "DataFrame with squared feature:\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|SALARY_FROM|            MSA_NAME|REMOTE_TYPE_NAME|SALARY|MIN_YEARS_EXPERIENCE_SQ|\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "|                   2|                   2|      79500|New York-Newark-J...|          Onsite| 92962|                    4.0|\n",
      "|                   2|                   2|      75026|         Jackson, MS|          Onsite| 75026|                    4.0|\n",
      "|                   1|                   1|      60923|       Rochester, NY|          Remote| 60923|                    1.0|\n",
      "|                   2|                   2|     113400|Phoenix-Mesa-Chan...|          Onsite|131100|                    4.0|\n",
      "|                   3|                   3|     115300|Augusta-Watervill...|          Remote|136950|                    9.0|\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed DataFrame with all features:\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "|MIN_YEARS_EXPERIENCE|MIN_YEARS_EXPERIENCE_SQ|MAX_YEARS_EXPERIENCE|SALARY_FROM|MSA_NAME                             |REMOTE_TYPE_NAME|SALARY|features                                       |features_poly                                        |\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "|2                   |4.0                    |2                   |79500      |New York-Newark-Jersey City, NY-NJ-PA|Onsite          |92962 |(217,[0,1,2,3,214],[2.0,2.0,79500.0,1.0,1.0])  |(218,[0,1,2,3,4,215],[2.0,4.0,2.0,79500.0,1.0,1.0])  |\n",
      "|2                   |4.0                    |2                   |75026      |Jackson, MS                          |Onsite          |75026 |(217,[0,1,2,20,214],[2.0,2.0,75026.0,1.0,1.0]) |(218,[0,1,2,3,21,215],[2.0,4.0,2.0,75026.0,1.0,1.0]) |\n",
      "|1                   |1.0                    |1                   |60923      |Rochester, NY                        |Remote          |60923 |(217,[0,1,2,70,215],[1.0,1.0,60923.0,1.0,1.0]) |(218,[0,1,2,3,71,216],[1.0,1.0,1.0,60923.0,1.0,1.0]) |\n",
      "|2                   |4.0                    |2                   |113400     |Phoenix-Mesa-Chandler, AZ            |Onsite          |131100|(217,[0,1,2,14,214],[2.0,2.0,113400.0,1.0,1.0])|(218,[0,1,2,3,15,215],[2.0,4.0,2.0,113400.0,1.0,1.0])|\n",
      "|3                   |9.0                    |3                   |115300     |Augusta-Waterville, ME               |Remote          |136950|(217,[0,1,2,53,215],[3.0,3.0,115300.0,1.0,1.0])|(218,[0,1,2,3,54,216],[3.0,9.0,3.0,115300.0,1.0,1.0])|\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your existing DataFrame\n",
    "# Step 1: Combine remote type values before cleaning\n",
    "\n",
    "df_processed = df.withColumn('REMOTE_TYPE_NAME',\n",
    "    when((col('REMOTE_TYPE_NAME').isNull()) | \n",
    "         (trim(col('REMOTE_TYPE_NAME')) == '[None]') |\n",
    "         (trim(col('REMOTE_TYPE_NAME')) == 'Not Remote' ) |\n",
    "         (trim(col('REMOTE_TYPE_NAME')) == 'Onsite'), 'Onsite')\n",
    "    .when((col('REMOTE_TYPE_NAME') == 'Hybrid Remote'), 'Hybrid')\n",
    "    .otherwise(col('REMOTE_TYPE_NAME'))\n",
    ")\n",
    "\n",
    "print(\"=== REMOTE_TYPE_NAME VALUE COUNTS AFTER COMBINING ===\")\n",
    "df_processed.groupBy('REMOTE_TYPE_NAME').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# Step 2: Drop rows with missing values in target and key features\n",
    "selected_columns = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                   'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY']\n",
    "\n",
    "df_clean = df_processed.select(selected_columns).dropna()\n",
    "\n",
    "print(\"Original DataFrame count:\", df.count())\n",
    "print(\"Cleaned DataFrame count:\", df_clean.count())\n",
    "print(\"\\nCleaned DataFrame Schema:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Step 2: Create squared feature for MIN_YEARS_EXPERIENCE\n",
    "df_clean = df_clean.withColumn('MIN_YEARS_EXPERIENCE_SQ', \n",
    "                               spark_pow(col('MIN_YEARS_EXPERIENCE'), 2))\n",
    "\n",
    "print(\"\\nDataFrame with squared feature:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# Step 3: Create Pipeline for encoding and feature assembly\n",
    "\n",
    "# StringIndexer for categorical variables\n",
    "msa_indexer = StringIndexer(inputCol='MSA_NAME', \n",
    "                            outputCol='MSA_NAME_INDEX',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "remote_indexer = StringIndexer(inputCol='REMOTE_TYPE_NAME', \n",
    "                               outputCol='REMOTE_TYPE_NAME_INDEX',\n",
    "                               handleInvalid='keep')\n",
    "\n",
    "# OneHotEncoder for categorical variables\n",
    "msa_encoder = OneHotEncoder(inputCol='MSA_NAME_INDEX', \n",
    "                           outputCol='MSA_NAME_VEC',\n",
    "                           dropLast=True)\n",
    "\n",
    "remote_encoder = OneHotEncoder(inputCol='REMOTE_TYPE_NAME_INDEX', \n",
    "                              outputCol='REMOTE_TYPE_NAME_VEC',\n",
    "                              dropLast=True)\n",
    "\n",
    "# VectorAssembler for basic features\n",
    "feature_cols = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "               'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, \n",
    "                            outputCol='features',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "# VectorAssembler for polynomial features\n",
    "poly_feature_cols = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                    'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                    'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "poly_assembler = VectorAssembler(inputCols=poly_feature_cols, \n",
    "                                outputCol='features_poly',\n",
    "                                handleInvalid='keep')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    msa_indexer,\n",
    "    remote_indexer,\n",
    "    msa_encoder,\n",
    "    remote_encoder,\n",
    "    assembler,\n",
    "    poly_assembler\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformed DataFrame with all features:\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY',\n",
    "                     'features', 'features_poly').show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390c120",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f65dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA SPLIT SUMMARY ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 2574 (71.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set count: 1022 (28.4%)\n",
      "Random seed: 42 (for reproducibility)\n",
      "\n",
      "Split Justification:\n",
      "• 70-30 split provides robust model evaluation\n",
      "• Larger test set (30%) improves confidence in performance metrics\n",
      "• Balanced approach for moderate-sized datasets\n",
      "• Alternative splits: 80-20 for large datasets, 60-40 for small datasets\n",
      "\n",
      "=== FINAL DATAFRAME STRUCTURE ===\n",
      "root\n",
      " |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- SALARY_FROM: integer (nullable = true)\n",
      " |-- MSA_NAME: string (nullable = true)\n",
      " |-- REMOTE_TYPE_NAME: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      " |-- MIN_YEARS_EXPERIENCE_SQ: double (nullable = true)\n",
      " |-- MSA_NAME_INDEX: double (nullable = false)\n",
      " |-- REMOTE_TYPE_NAME_INDEX: double (nullable = false)\n",
      " |-- MSA_NAME_VEC: vector (nullable = true)\n",
      " |-- REMOTE_TYPE_NAME_VEC: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_poly: vector (nullable = true)\n",
      "\n",
      "\n",
      "=== SAMPLE OF FINAL DATA ===\n",
      "+--------------------+-----------------------+--------------------+-----------+------+--------------------+\n",
      "|MIN_YEARS_EXPERIENCE|MIN_YEARS_EXPERIENCE_SQ|MAX_YEARS_EXPERIENCE|SALARY_FROM|SALARY|       features_poly|\n",
      "+--------------------+-----------------------+--------------------+-----------+------+--------------------+\n",
      "|                   2|                    4.0|                   2|      79500| 92962|(218,[0,1,2,3,4,2...|\n",
      "|                   2|                    4.0|                   2|      75026| 75026|(218,[0,1,2,3,21,...|\n",
      "|                   1|                    1.0|                   1|      60923| 60923|(218,[0,1,2,3,71,...|\n",
      "|                   2|                    4.0|                   2|     113400|131100|(218,[0,1,2,3,15,...|\n",
      "|                   3|                    9.0|                   3|     115300|136950|(218,[0,1,2,3,54,...|\n",
      "|                   5|                   25.0|                   5|     114000|122500|(218,[0,1,2,3,18,...|\n",
      "|                   2|                    4.0|                   2|      49920| 55120|(218,[0,1,2,3,78,...|\n",
      "|                   3|                    9.0|                   3|     104000|104000|(218,[0,1,2,3,19,...|\n",
      "|                   4|                   16.0|                   4|     124238|145319|(218,[0,1,2,3,92,...|\n",
      "|                   3|                    9.0|                   3|      60000| 80000|(218,[0,1,2,3,61,...|\n",
      "+--------------------+-----------------------+--------------------+-----------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "=== FEATURE STATISTICS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 23:12:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------------+--------------------+------------------+------------------+\n",
      "|summary|MIN_YEARS_EXPERIENCE|MIN_YEARS_EXPERIENCE_SQ|MAX_YEARS_EXPERIENCE|       SALARY_FROM|            SALARY|\n",
      "+-------+--------------------+-----------------------+--------------------+------------------+------------------+\n",
      "|  count|                3596|                   3596|                3596|              3596|              3596|\n",
      "|   mean|  3.6384872080088986|       18.9972191323693|  3.6384872080088986| 91714.32619577309| 107798.5881535039|\n",
      "| stddev|   2.400048294044211|     24.190746240439758|   2.400048294044211|32683.349277662266|36636.119374840724|\n",
      "|    min|                   0|                    0.0|                   0|             14000|             31640|\n",
      "|    max|                  12|                  144.0|                  12|            324000|            338750|\n",
      "+-------+--------------------+-----------------------+--------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "=== CATEGORICAL VARIABLE COUNTS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSA_NAME unique values: 211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE_TYPE_NAME unique values: 3\n",
      "\n",
      "✓ Data preprocessing pipeline completed successfully!\n",
      "✓ Ready for model training with 'features_poly' as input and 'SALARY' as target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Split data into training and testing sets\n",
    "# Using 70-30 split for better evaluation capability\n",
    "# Justification:\n",
    "# - 70% training: Provides sufficient data for model to learn patterns\n",
    "# - 30% testing: Larger test set gives more reliable performance metrics\n",
    "# - Good balance for datasets with moderate size (thousands of records)\n",
    "# - Seed=42 ensures reproducibility across runs\n",
    "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"\\n=== DATA SPLIT SUMMARY ===\")\n",
    "print(f\"Training set count: {train_data.count()} ({train_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "print(f\"Testing set count: {test_data.count()} ({test_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "print(f\"Random seed: 42 (for reproducibility)\")\n",
    "print(\"\\nSplit Justification:\")\n",
    "print(\"• 70-30 split provides robust model evaluation\")\n",
    "print(\"• Larger test set (30%) improves confidence in performance metrics\")\n",
    "print(\"• Balanced approach for moderate-sized datasets\")\n",
    "print(\"• Alternative splits: 80-20 for large datasets, 60-40 for small datasets\")\n",
    "\n",
    "# Step 5: Show final structure\n",
    "print(\"\\n=== FINAL DATAFRAME STRUCTURE ===\")\n",
    "df_transformed.printSchema()\n",
    "\n",
    "print(\"\\n=== SAMPLE OF FINAL DATA ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                     'SALARY', 'features_poly').show(10)\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\n=== FEATURE STATISTICS ===\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'SALARY').describe().show()\n",
    "\n",
    "# Optional: Show unique values in categorical columns\n",
    "print(\"\\n=== CATEGORICAL VARIABLE COUNTS ===\")\n",
    "print(\"MSA_NAME unique values:\", df_clean.select('MSA_NAME').distinct().count())\n",
    "print(\"REMOTE_TYPE_NAME unique values:\", df_clean.select('REMOTE_TYPE_NAME').distinct().count())\n",
    "\n",
    "# Save transformed data for future use (optional)\n",
    "# df_transformed.write.parquet(\"transformed_salary_data.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"\\n✓ Data preprocessing pipeline completed successfully!\")\n",
    "print(\"✓ Ready for model training with 'features_poly' as input and 'SALARY' as target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42835dcf",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6df48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 23:13:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REMOTE_TYPE_NAME VALUE COUNTS AFTER COMBINING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|REMOTE_TYPE_NAME|count|\n",
      "+----------------+-----+\n",
      "|          Onsite|57741|\n",
      "|          Remote|12497|\n",
      "|          Hybrid| 2260|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame count: 72498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame count: 3596\n",
      "\n",
      "Cleaned DataFrame Schema:\n",
      "root\n",
      " |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n",
      " |-- SALARY_FROM: integer (nullable = true)\n",
      " |-- MSA_NAME: string (nullable = true)\n",
      " |-- REMOTE_TYPE_NAME: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      "\n",
      "\n",
      "DataFrame with squared feature:\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|SALARY_FROM|            MSA_NAME|REMOTE_TYPE_NAME|SALARY|MIN_YEARS_EXPERIENCE_SQ|\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "|                   2|                   2|      79500|New York-Newark-J...|          Onsite| 92962|                    4.0|\n",
      "|                   2|                   2|      75026|         Jackson, MS|          Onsite| 75026|                    4.0|\n",
      "|                   1|                   1|      60923|       Rochester, NY|          Remote| 60923|                    1.0|\n",
      "|                   2|                   2|     113400|Phoenix-Mesa-Chan...|          Onsite|131100|                    4.0|\n",
      "|                   3|                   3|     115300|Augusta-Watervill...|          Remote|136950|                    9.0|\n",
      "+--------------------+--------------------+-----------+--------------------+----------------+------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed DataFrame with all features:\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "|MIN_YEARS_EXPERIENCE|MIN_YEARS_EXPERIENCE_SQ|MAX_YEARS_EXPERIENCE|SALARY_FROM|MSA_NAME                             |REMOTE_TYPE_NAME|SALARY|features                                       |features_poly                                        |\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "|2                   |4.0                    |2                   |79500      |New York-Newark-Jersey City, NY-NJ-PA|Onsite          |92962 |(217,[0,1,2,3,214],[2.0,2.0,79500.0,1.0,1.0])  |(218,[0,1,2,3,4,215],[2.0,4.0,2.0,79500.0,1.0,1.0])  |\n",
      "|2                   |4.0                    |2                   |75026      |Jackson, MS                          |Onsite          |75026 |(217,[0,1,2,20,214],[2.0,2.0,75026.0,1.0,1.0]) |(218,[0,1,2,3,21,215],[2.0,4.0,2.0,75026.0,1.0,1.0]) |\n",
      "|1                   |1.0                    |1                   |60923      |Rochester, NY                        |Remote          |60923 |(217,[0,1,2,70,215],[1.0,1.0,60923.0,1.0,1.0]) |(218,[0,1,2,3,71,216],[1.0,1.0,1.0,60923.0,1.0,1.0]) |\n",
      "|2                   |4.0                    |2                   |113400     |Phoenix-Mesa-Chandler, AZ            |Onsite          |131100|(217,[0,1,2,14,214],[2.0,2.0,113400.0,1.0,1.0])|(218,[0,1,2,3,15,215],[2.0,4.0,2.0,113400.0,1.0,1.0])|\n",
      "|3                   |9.0                    |3                   |115300     |Augusta-Waterville, ME               |Remote          |136950|(217,[0,1,2,53,215],[3.0,3.0,115300.0,1.0,1.0])|(218,[0,1,2,3,54,216],[3.0,9.0,3.0,115300.0,1.0,1.0])|\n",
      "+--------------------+-----------------------+--------------------+-----------+-------------------------------------+----------------+------+-----------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== DATA SPLIT SUMMARY ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 2574 (71.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set count: 1022 (28.4%)\n",
      "\n",
      "================================================================================\n",
      "TRAINING LINEAR REGRESSION MODEL\n",
      "================================================================================\n",
      "\n",
      "⚠️  IDENTIFYING THE KEY ISSUE:\n",
      "The 'features' column includes SALARY_FROM, which creates DATA LEAKAGE!\n",
      "SALARY_FROM is part of the same salary range as our target (SALARY).\n",
      "This violates ML principles and makes the model unrealistic.\n",
      "\n",
      "✓ Created 'features_clean' column WITHOUT SALARY_FROM\n",
      "  Features included: ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
      "\n",
      "Training Linear Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 23:14:10 WARN Instrumentation: [220cae14] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/08 23:14:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/10/08 23:14:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/10/08 23:14:16 WARN Instrumentation: [220cae14] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model training completed!\n",
      "\n",
      "================================================================================\n",
      "MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "=== SAMPLE PREDICTIONS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------------------+--------------------+--------------------+----------------+\n",
      "|SALARY|        prediction|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|            MSA_NAME|REMOTE_TYPE_NAME|\n",
      "+------+------------------+--------------------+--------------------+--------------------+----------------+\n",
      "| 49547|54123.724107785674|                   0|                   0|Riverside-San Ber...|          Onsite|\n",
      "| 41600| 81030.78622835217|                   0|                   0|    Jacksonville, FL|          Onsite|\n",
      "| 66500| 77147.76402066693|                   0|                   0|Houston-The Woodl...|          Onsite|\n",
      "| 48880| 70845.72970745854|                   0|                   0|Denver-Aurora-Lak...|          Onsite|\n",
      "| 50960|  79838.8928748018|                   0|                   0|Chicago-Napervill...|          Onsite|\n",
      "| 61328| 75431.71877777357|                   0|                   0|Dallas-Fort Worth...|          Onsite|\n",
      "| 48922| 71006.56657273862|                   0|                   0|    Raleigh-Cary, NC|          Onsite|\n",
      "| 62400| 81505.58810108715|                   0|                   0|Boston-Cambridge-...|          Remote|\n",
      "| 62400| 84839.10132449259|                   0|                   0|Chicago-Napervill...|          Remote|\n",
      "| 62400| 77952.33839705357|                   0|                   0|Los Angeles-Long ...|          Remote|\n",
      "+------+------------------+--------------------+--------------------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "================================================================================\n",
      "MODEL COEFFICIENTS AND STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Intercept: $77,166.48\n",
      "R² (R-squared): 0.4205\n",
      "RMSE (Root Mean Squared Error): $28,714.10\n",
      "MAE (Mean Absolute Error): $21,077.77\n",
      "\n",
      "================================================================================\n",
      "CALCULATING COEFFICIENT STATISTICS MANUALLY\n",
      "================================================================================\n",
      "\n",
      "Extracting feature matrix and target values from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2,574 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2574, 216)\n",
      "Label vector shape: (2574,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residual Standard Error: $30,006.80\n",
      "Degrees of Freedom: 2357\n",
      "❌ Error calculating statistics: Singular matrix\n",
      "   This may happen with singular matrices or perfect multicollinearity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COEFFICIENT ANALYSIS TABLE ===\n",
      "             Feature   Coefficient\n",
      "MIN_YEARS_EXPERIENCE   4346.702719\n",
      "MAX_YEARS_EXPERIENCE   4346.702719\n",
      "               MSA_0  12170.474476\n",
      "               MSA_1   3875.848099\n",
      "               MSA_2  -2216.381832\n",
      "               MSA_3    263.206998\n",
      "               MSA_4  15502.361770\n",
      "               MSA_5   4670.381095\n",
      "               MSA_6   1336.867872\n",
      "               MSA_7  -4322.782072\n",
      "               MSA_8  -8193.486727\n",
      "               MSA_9    145.396541\n",
      "              MSA_10   6137.504086\n",
      "              MSA_11   4558.735547\n",
      "              MSA_12  10003.789801\n",
      "              MSA_13   1349.100445\n",
      "              MSA_14  34760.900322\n",
      "              MSA_15   1979.252241\n",
      "              MSA_16  -2376.652197\n",
      "              MSA_17 -21038.614191\n",
      "              MSA_18   9384.031296\n",
      "              MSA_19  -4119.615087\n",
      "              MSA_20   1783.074739\n",
      "              MSA_21 -11785.320427\n",
      "              MSA_22  -4338.026653\n",
      "              MSA_23  -9262.264065\n",
      "              MSA_24  -4073.126130\n",
      "              MSA_25 -12340.412635\n",
      "              MSA_26   1584.477699\n",
      "              MSA_27   2884.435757\n",
      "              MSA_28  -2188.945114\n",
      "              MSA_29   8376.130855\n",
      "              MSA_30  -7811.367847\n",
      "              MSA_31  -4161.945207\n",
      "              MSA_32   6902.219816\n",
      "              MSA_33  -8380.752993\n",
      "              MSA_34  -6881.457229\n",
      "              MSA_35 -13700.804258\n",
      "              MSA_36   5862.274449\n",
      "              MSA_37  -2084.520353\n",
      "              MSA_38  -7105.347401\n",
      "              MSA_39  -4797.847376\n",
      "              MSA_40 -11661.242929\n",
      "              MSA_41  -1139.490607\n",
      "              MSA_42   1785.840194\n",
      "              MSA_43  -6453.594774\n",
      "              MSA_44    683.854911\n",
      "              MSA_45 -11992.952396\n",
      "              MSA_46  -6706.521349\n",
      "              MSA_47   4781.065801\n",
      "              MSA_48 -12529.475245\n",
      "              MSA_49 -13915.823555\n",
      "              MSA_50  -3502.451611\n",
      "              MSA_51  -1349.346059\n",
      "              MSA_52  -4280.771864\n",
      "              MSA_53   3297.434770\n",
      "              MSA_54  -9538.294799\n",
      "              MSA_55 -11381.382182\n",
      "              MSA_56  -6429.185482\n",
      "              MSA_57  -7614.609856\n",
      "              MSA_58  11206.071331\n",
      "              MSA_59    128.055612\n",
      "              MSA_60  -7933.670013\n",
      "              MSA_61  19539.456487\n",
      "              MSA_62    291.933041\n",
      "              MSA_63  -6449.462449\n",
      "              MSA_64  -8630.425726\n",
      "              MSA_65  -2884.297705\n",
      "              MSA_66  -3864.411093\n",
      "              MSA_67 -27288.238879\n",
      "              MSA_68 -15865.162822\n",
      "              MSA_69  23065.374483\n",
      "              MSA_70  -3705.283731\n",
      "              MSA_71  -4042.050758\n",
      "              MSA_72  -9149.273541\n",
      "              MSA_73  -7529.443117\n",
      "              MSA_74 -32393.939812\n",
      "              MSA_75  -5137.380171\n",
      "              MSA_76 -17597.297984\n",
      "              MSA_77 -21044.787672\n",
      "              MSA_78  -7241.611940\n",
      "              MSA_79 -16438.221273\n",
      "              MSA_80   2055.599598\n",
      "              MSA_81  -1166.795379\n",
      "              MSA_82 -12699.280211\n",
      "              MSA_83  -2874.216205\n",
      "              MSA_84    383.284618\n",
      "              MSA_85 -19909.218749\n",
      "              MSA_86 -18033.009072\n",
      "              MSA_87  -5990.384286\n",
      "              MSA_88   2874.293438\n",
      "              MSA_89 -22478.953483\n",
      "              MSA_90 -18276.793008\n",
      "              MSA_91  11076.221747\n",
      "              MSA_92  -6997.400874\n",
      "              MSA_93 -21363.233222\n",
      "              MSA_94  -4282.239122\n",
      "              MSA_95    620.130524\n",
      "              MSA_96 -17852.953126\n",
      "              MSA_97  -8570.481769\n",
      "              MSA_98  -1991.494153\n",
      "              MSA_99 -19945.332768\n",
      "             MSA_100  -1275.908462\n",
      "             MSA_101 -26983.488531\n",
      "             MSA_102  -9558.662530\n",
      "             MSA_103  13522.816148\n",
      "             MSA_104 -18730.909770\n",
      "             MSA_105 -22523.678934\n",
      "             MSA_106  -5628.260678\n",
      "             MSA_107 -11694.517063\n",
      "             MSA_108 -22954.842653\n",
      "             MSA_109 -28775.538372\n",
      "             MSA_110  52983.872969\n",
      "             MSA_111 -14988.722247\n",
      "             MSA_112 -39604.318889\n",
      "             MSA_113 -26570.910368\n",
      "             MSA_114  -6333.413390\n",
      "             MSA_115   9147.981012\n",
      "             MSA_116 -28217.126120\n",
      "             MSA_117 -41741.911510\n",
      "             MSA_118 105048.002628\n",
      "             MSA_119 -10926.666008\n",
      "             MSA_120  -7773.316493\n",
      "             MSA_121 -15833.113330\n",
      "             MSA_122  -5523.388196\n",
      "             MSA_123 -24034.058601\n",
      "             MSA_124   6801.169252\n",
      "             MSA_125  -2393.946244\n",
      "             MSA_126      0.000000\n",
      "             MSA_127 -12095.424632\n",
      "             MSA_128   6851.061423\n",
      "             MSA_129 -10568.939870\n",
      "             MSA_130 -14715.762237\n",
      "             MSA_131  -8655.131632\n",
      "             MSA_132 -21248.940674\n",
      "             MSA_133   6851.061423\n",
      "             MSA_134 -16358.613369\n",
      "             MSA_135 -36135.649058\n",
      "             MSA_136      0.000000\n",
      "             MSA_137 -19658.928696\n",
      "             MSA_138 -13448.832273\n",
      "             MSA_139 -19610.832736\n",
      "             MSA_140   6251.277024\n",
      "             MSA_141 -40355.534563\n",
      "             MSA_142  -8862.124674\n",
      "             MSA_143   6494.468954\n",
      "             MSA_144  30611.390166\n",
      "             MSA_145 -13656.909396\n",
      "             MSA_146 -39828.911345\n",
      "             MSA_147 -31748.725818\n",
      "             MSA_148      0.000000\n",
      "             MSA_149 -19264.928666\n",
      "             MSA_150      0.000000\n",
      "             MSA_151   9577.647115\n",
      "             MSA_152   6851.061423\n",
      "             MSA_153 -14715.762237\n",
      "             MSA_154      0.000000\n",
      "             MSA_155  -4968.977150\n",
      "             MSA_156  32164.463877\n",
      "             MSA_157      0.000000\n",
      "             MSA_158  13557.870026\n",
      "             MSA_159 -17768.717800\n",
      "             MSA_160 -24442.132811\n",
      "             MSA_161  16564.462712\n",
      "             MSA_162      0.000000\n",
      "             MSA_163   8751.277205\n",
      "             MSA_164  17331.500460\n",
      "             MSA_165 -34942.133595\n",
      "             MSA_166      0.000000\n",
      "             MSA_167 -25761.910295\n",
      "             MSA_168      0.000000\n",
      "             MSA_169      0.000000\n",
      "             MSA_170 -10611.316702\n",
      "             MSA_171   6851.061423\n",
      "             MSA_172      0.000000\n",
      "             MSA_173 -19027.317330\n",
      "             MSA_174 -20865.762696\n",
      "             MSA_175      0.000000\n",
      "             MSA_176 -46499.319381\n",
      "             MSA_177    751.276608\n",
      "             MSA_178    834.684152\n",
      "             MSA_179   8456.528522\n",
      "             MSA_180 -17248.724736\n",
      "             MSA_181      0.000000\n",
      "             MSA_182  56264.465676\n",
      "             MSA_183  16138.092833\n",
      "             MSA_184 -13248.724437\n",
      "             MSA_185  37184.241638\n",
      "             MSA_186 -15055.317034\n",
      "             MSA_187      0.000000\n",
      "             MSA_188  61831.503782\n",
      "             MSA_189 -17361.909668\n",
      "             MSA_190      0.000000\n",
      "             MSA_191      0.000000\n",
      "             MSA_192  22549.937112\n",
      "             MSA_193  38544.686968\n",
      "             MSA_194 -14587.792944\n",
      "             MSA_195  -2609.168871\n",
      "             MSA_196      0.000000\n",
      "             MSA_197 -24570.317744\n",
      "             MSA_198  -4715.761490\n",
      "             MSA_199      0.000000\n",
      "             MSA_200      0.000000\n",
      "             MSA_201  10331.499938\n",
      "             MSA_202  -6776.908878\n",
      "             MSA_203      0.000000\n",
      "             MSA_204  63264.466199\n",
      "             MSA_205 -34614.318494\n",
      "             MSA_206 -70795.989038\n",
      "             MSA_207  -9155.532234\n",
      "             MSA_208  10231.061676\n",
      "             MSA_209      0.000000\n",
      "            REMOTE_0  21651.240478\n",
      "            REMOTE_1  -1997.969903\n",
      "\n",
      "================================================================================\n",
      "MODEL INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "📊 COEFFICIENTS INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIN_YEARS_EXPERIENCE:\n",
      "  • Coefficient: $4,346.70\n",
      "  • Interpretation: For each additional year of experience,\n",
      "    salary increases by $4,346.70 (all else equal)\n",
      "\n",
      "MAX_YEARS_EXPERIENCE:\n",
      "  • Coefficient: $4,346.70\n",
      "  • Interpretation: For each additional year of experience,\n",
      "    salary increases by $4,346.70 (all else equal)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📈 MODEL PERFORMANCE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. R² (R-squared) = 0.4205\n",
      "   • Interpretation: The model explains 42.05% of the variance in salary\n",
      "   • Assessment: Weak but meaningful explanatory power\n",
      "\n",
      "2. RMSE (Root Mean Squared Error) = $28,714.10\n",
      "   • Interpretation: On average, predictions deviate by $28,714.10\n",
      "   • Assessment: Predictions are typically off by ~$28,714\n",
      "\n",
      "3. MAE (Mean Absolute Error) = $21,077.77\n",
      "   • Interpretation: The average absolute prediction error is $21,077.77\n",
      "   • MAE/RMSE Ratio: 0.734\n",
      "   • Large errors present (outliers) since MAE << RMSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 TEST SET PERFORMANCE:\n",
      "--------------------------------------------------------------------------------\n",
      "Test R²: 0.3617\n",
      "Test RMSE: $26,948.87\n",
      "Test MAE: $21,366.08\n",
      "\n",
      "📊 TRAINING vs TEST COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "Training R²: 0.4205 | Test R²: 0.3617 | Difference: 0.0588\n",
      "✓ Good generalization - acceptable overfitting\n",
      "\n",
      "================================================================================\n",
      "✓ MODEL TRAINING AND EVALUATION COMPLETED!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, pow as spark_pow, when, trim\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Initialize Spark Session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"SalaryPrediction\").getOrCreate()\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "# Step 1: Combine remote type values before cleaning\n",
    "df_processed = df.withColumn('REMOTE_TYPE_NAME',\n",
    "    when((col('REMOTE_TYPE_NAME').isNull()) | \n",
    "         (trim(col('REMOTE_TYPE_NAME')) == '[None]') |\n",
    "         (trim(col('REMOTE_TYPE_NAME')) == 'Not Remote' ) |\n",
    "         (trim(col('REMOTE_TYPE_NAME')) == 'Onsite'), 'Onsite')\n",
    "    .when((col('REMOTE_TYPE_NAME') == 'Hybrid Remote'), 'Hybrid')\n",
    "    .otherwise(col('REMOTE_TYPE_NAME'))\n",
    ")\n",
    "\n",
    "print(\"=== REMOTE_TYPE_NAME VALUE COUNTS AFTER COMBINING ===\")\n",
    "df_processed.groupBy('REMOTE_TYPE_NAME').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# Step 2: Drop rows with missing values in target and key features\n",
    "selected_columns = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                   'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY']\n",
    "\n",
    "df_clean = df_processed.select(selected_columns).dropna()\n",
    "\n",
    "print(\"Original DataFrame count:\", df.count())\n",
    "print(\"Cleaned DataFrame count:\", df_clean.count())\n",
    "print(\"\\nCleaned DataFrame Schema:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Step 2: Create squared feature for MIN_YEARS_EXPERIENCE\n",
    "df_clean = df_clean.withColumn('MIN_YEARS_EXPERIENCE_SQ', \n",
    "                               spark_pow(col('MIN_YEARS_EXPERIENCE'), 2))\n",
    "\n",
    "print(\"\\nDataFrame with squared feature:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# Step 3: Create Pipeline for encoding and feature assembly\n",
    "\n",
    "# StringIndexer for categorical variables\n",
    "msa_indexer = StringIndexer(inputCol='MSA_NAME', \n",
    "                            outputCol='MSA_NAME_INDEX',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "remote_indexer = StringIndexer(inputCol='REMOTE_TYPE_NAME', \n",
    "                               outputCol='REMOTE_TYPE_NAME_INDEX',\n",
    "                               handleInvalid='keep')\n",
    "\n",
    "# OneHotEncoder for categorical variables\n",
    "msa_encoder = OneHotEncoder(inputCol='MSA_NAME_INDEX', \n",
    "                           outputCol='MSA_NAME_VEC',\n",
    "                           dropLast=True)\n",
    "\n",
    "remote_encoder = OneHotEncoder(inputCol='REMOTE_TYPE_NAME_INDEX', \n",
    "                              outputCol='REMOTE_TYPE_NAME_VEC',\n",
    "                              dropLast=True)\n",
    "\n",
    "# VectorAssembler for basic features\n",
    "feature_cols = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "               'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, \n",
    "                            outputCol='features',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "# VectorAssembler for polynomial features\n",
    "poly_feature_cols = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                    'MAX_YEARS_EXPERIENCE', 'SALARY_FROM',\n",
    "                    'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "poly_assembler = VectorAssembler(inputCols=poly_feature_cols, \n",
    "                                outputCol='features_poly',\n",
    "                                handleInvalid='keep')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    msa_indexer,\n",
    "    remote_indexer,\n",
    "    msa_encoder,\n",
    "    remote_encoder,\n",
    "    assembler,\n",
    "    poly_assembler\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformed DataFrame with all features:\")\n",
    "df_transformed.select('MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', \n",
    "                     'MAX_YEARS_EXPERIENCE', 'SALARY_FROM', \n",
    "                     'MSA_NAME', 'REMOTE_TYPE_NAME', 'SALARY',\n",
    "                     'features', 'features_poly').show(5, truncate=False)\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"\\n=== DATA SPLIT SUMMARY ===\")\n",
    "print(f\"Training set count: {train_data.count()} ({train_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "print(f\"Testing set count: {test_data.count()} ({test_data.count()/df_transformed.count()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN LINEAR REGRESSION MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING LINEAR REGRESSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL ISSUE RESOLUTION: \n",
    "# The 'features' column includes SALARY_FROM which is highly correlated with SALARY\n",
    "# This creates MULTICOLLINEARITY and DATA LEAKAGE issues:\n",
    "# 1. SALARY_FROM is derived from the same job posting as SALARY (target variable)\n",
    "# 2. Including it violates the independence assumption\n",
    "# 3. It artificially inflates R² and makes the model unusable for real predictions\n",
    "# \n",
    "# SOLUTION: Create a new feature vector WITHOUT SALARY_FROM\n",
    "\n",
    "print(\"\\n⚠️  IDENTIFYING THE KEY ISSUE:\")\n",
    "print(\"The 'features' column includes SALARY_FROM, which creates DATA LEAKAGE!\")\n",
    "print(\"SALARY_FROM is part of the same salary range as our target (SALARY).\")\n",
    "print(\"This violates ML principles and makes the model unrealistic.\\n\")\n",
    "\n",
    "# Create a new assembler WITHOUT SALARY_FROM\n",
    "feature_cols_clean = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE',\n",
    "                      'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler_clean = VectorAssembler(inputCols=feature_cols_clean, \n",
    "                                  outputCol='features_clean',\n",
    "                                  handleInvalid='keep')\n",
    "\n",
    "# Transform data with clean features\n",
    "df_train = assembler_clean.transform(train_data)\n",
    "df_test = assembler_clean.transform(test_data)\n",
    "\n",
    "print(\"✓ Created 'features_clean' column WITHOUT SALARY_FROM\")\n",
    "print(f\"  Features included: {feature_cols_clean}\\n\")\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol='features_clean',\n",
    "    labelCol='SALARY',\n",
    "    maxIter=100,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Linear Regression model...\")\n",
    "lr_model = lr.fit(df_train)\n",
    "print(\"✓ Model training completed!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: MAKE PREDICTIONS AND EVALUATE\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = lr_model.transform(df_test)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"=== SAMPLE PREDICTIONS ===\")\n",
    "predictions.select('SALARY', 'prediction', 'MIN_YEARS_EXPERIENCE', \n",
    "                   'MAX_YEARS_EXPERIENCE', 'MSA_NAME', 'REMOTE_TYPE_NAME').show(10)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: EXTRACT MODEL COEFFICIENTS AND CALCULATE STATISTICS MANUALLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COEFFICIENTS AND STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get model summary\n",
    "summary = lr_model.summary\n",
    "\n",
    "# Extract basic metrics\n",
    "intercept = lr_model.intercept\n",
    "coefficients = lr_model.coefficients\n",
    "r2 = summary.r2\n",
    "rmse = summary.rootMeanSquaredError\n",
    "mae = summary.meanAbsoluteError\n",
    "\n",
    "print(f\"Intercept: ${intercept:,.2f}\")\n",
    "print(f\"R² (R-squared): {r2:.4f}\")\n",
    "print(f\"RMSE (Root Mean Squared Error): ${rmse:,.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error): ${mae:,.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MANUAL CALCULATION OF COEFFICIENT STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING COEFFICIENT STATISTICS MANUALLY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Extracting feature matrix and target values from training data...\")\n",
    "\n",
    "# Collect training data for manual statistics calculation\n",
    "# WARNING: Only do this if dataset is not too large (< 100K rows recommended)\n",
    "train_count = df_train.count()\n",
    "print(f\"Training set size: {train_count:,} rows\")\n",
    "\n",
    "if train_count > 100000:\n",
    "    print(\"⚠️  Warning: Large dataset. Manual statistics calculation may be slow.\")\n",
    "    print(\"   Consider using a sample for coefficient statistics.\\n\")\n",
    "\n",
    "# Extract features and labels\n",
    "train_features = np.array(df_train.select('features_clean').rdd.map(lambda row: row[0].toArray()).collect())\n",
    "train_labels = np.array(df_train.select('SALARY').rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "print(f\"Feature matrix shape: {train_features.shape}\")\n",
    "print(f\"Label vector shape: {train_labels.shape}\")\n",
    "\n",
    "# Get predictions on training data for residuals\n",
    "train_predictions = lr_model.transform(df_train)\n",
    "train_pred_values = np.array(train_predictions.select('prediction').rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = train_labels - train_pred_values\n",
    "n = len(train_labels)\n",
    "k = train_features.shape[1]  # number of features\n",
    "df_residual = n - k - 1  # degrees of freedom\n",
    "\n",
    "# Calculate residual standard error\n",
    "rse = np.sqrt(np.sum(residuals**2) / df_residual)\n",
    "\n",
    "print(f\"\\nResidual Standard Error: ${rse:,.2f}\")\n",
    "print(f\"Degrees of Freedom: {df_residual}\")\n",
    "\n",
    "# Calculate variance-covariance matrix\n",
    "# Var(β) = σ² * (X'X)^(-1)\n",
    "try:\n",
    "    X = train_features\n",
    "    XtX = np.dot(X.T, X)\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    \n",
    "    # Variance-covariance matrix\n",
    "    var_covar_matrix = (rse**2) * XtX_inv\n",
    "    \n",
    "    # Standard errors are square roots of diagonal elements\n",
    "    std_errors = np.sqrt(np.diag(var_covar_matrix))\n",
    "    \n",
    "    # Calculate t-values\n",
    "    coef_array = np.array(coefficients.toArray())\n",
    "    t_values = coef_array / std_errors\n",
    "    \n",
    "    # Calculate p-values (two-tailed test)\n",
    "    p_values = 2 * (1 - scipy_stats.t.cdf(np.abs(t_values), df_residual))\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    t_critical = scipy_stats.t.ppf(0.975, df_residual)  # 97.5th percentile for two-tailed\n",
    "    ci_lower = coef_array - t_critical * std_errors\n",
    "    ci_upper = coef_array + t_critical * std_errors\n",
    "    \n",
    "    stats_available = True\n",
    "    print(\"✓ Coefficient statistics calculated successfully!\\n\")\n",
    "    \n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"❌ Error calculating statistics: {e}\")\n",
    "    print(\"   This may happen with singular matrices or perfect multicollinearity.\\n\")\n",
    "    stats_available = False\n",
    "    std_errors = [None] * len(coefficients)\n",
    "    t_values = [None] * len(coefficients)\n",
    "    p_values = [None] * len(coefficients)\n",
    "    ci_lower = [None] * len(coefficients)\n",
    "    ci_upper = [None] * len(coefficients)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE COEFFICIENT TABLE\n",
    "# ============================================================================\n",
    "\n",
    "# Create feature names for interpretation\n",
    "num_msa_categories = df_clean.select('MSA_NAME').distinct().count() - 1\n",
    "num_remote_categories = df_clean.select('REMOTE_TYPE_NAME').distinct().count() - 1\n",
    "\n",
    "feature_names = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\n",
    "feature_names += [f'MSA_{i}' for i in range(num_msa_categories)]\n",
    "feature_names += [f'REMOTE_{i}' for i in range(num_remote_categories)]\n",
    "\n",
    "# Create DataFrame for coefficient analysis\n",
    "coef_data = []\n",
    "for i, (name, coef) in enumerate(zip(feature_names, coefficients)):\n",
    "    row_data = {\n",
    "        'Feature': name,\n",
    "        'Coefficient': float(coef)\n",
    "    }\n",
    "    \n",
    "    if stats_available:\n",
    "        row_data.update({\n",
    "            'Std_Error': float(std_errors[i]),\n",
    "            'T_Value': float(t_values[i]),\n",
    "            'P_Value': float(p_values[i]),\n",
    "            'CI_Lower': float(ci_lower[i]),\n",
    "            'CI_Upper': float(ci_upper[i]),\n",
    "            'Significant': '***' if p_values[i] < 0.001 else '**' if p_values[i] < 0.01 else '*' if p_values[i] < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    coef_data.append(row_data)\n",
    "\n",
    "# Convert to Pandas for better display\n",
    "coef_df = pd.DataFrame(coef_data)\n",
    "\n",
    "print(\"\\n=== COEFFICIENT ANALYSIS TABLE ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: INTERPRET RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL INTERPRETATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"📊 COEFFICIENTS INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if stats_available:\n",
    "    for i, row in coef_df.iterrows():\n",
    "        if i < 2:  # Only interpret the main numerical features\n",
    "            name = row['Feature']\n",
    "            coef = row['Coefficient']\n",
    "            p_val = row['P_Value']\n",
    "            sig = row['Significant']\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  • Coefficient: ${coef:,.2f} {sig}\")\n",
    "            print(f\"  • Interpretation: For each additional year of experience,\")\n",
    "            print(f\"    salary {'increases' if coef > 0 else 'decreases'} by ${abs(coef):,.2f} (all else equal)\")\n",
    "            print(f\"  • Statistical Significance: {'Significant' if sig != 'No' else 'Not significant'} (p={p_val:.4f})\")\n",
    "            print(f\"  • 95% CI: [${row['CI_Lower']:,.2f}, ${row['CI_Upper']:,.2f}]\")\n",
    "            if p_val < 0.05:\n",
    "                print(f\"  • Conclusion: This effect is statistically significant at the 5% level\")\n",
    "            else:\n",
    "                print(f\"  • Conclusion: This effect is NOT statistically significant\")\n",
    "else:\n",
    "    for i, row in coef_df.iterrows():\n",
    "        if i < 2:\n",
    "            name = row['Feature']\n",
    "            coef = row['Coefficient']\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  • Coefficient: ${coef:,.2f}\")\n",
    "            print(f\"  • Interpretation: For each additional year of experience,\")\n",
    "            print(f\"    salary {'increases' if coef > 0 else 'decreases'} by ${abs(coef):,.2f} (all else equal)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n📈 MODEL PERFORMANCE METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n1. R² (R-squared) = {r2:.4f}\")\n",
    "print(f\"   • Interpretation: The model explains {r2*100:.2f}% of the variance in salary\")\n",
    "if r2 > 0.7:\n",
    "    print(f\"   • Assessment: Strong explanatory power\")\n",
    "elif r2 > 0.5:\n",
    "    print(f\"   • Assessment: Moderate explanatory power\")\n",
    "elif r2 > 0.3:\n",
    "    print(f\"   • Assessment: Weak but meaningful explanatory power\")\n",
    "else:\n",
    "    print(f\"   • Assessment: Poor explanatory power - consider adding more features\")\n",
    "\n",
    "print(f\"\\n2. RMSE (Root Mean Squared Error) = ${rmse:,.2f}\")\n",
    "print(f\"   • Interpretation: On average, predictions deviate by ${rmse:,.2f}\")\n",
    "print(f\"   • Assessment: Predictions are typically off by ~${rmse:,.0f}\")\n",
    "\n",
    "print(f\"\\n3. MAE (Mean Absolute Error) = ${mae:,.2f}\")\n",
    "print(f\"   • Interpretation: The average absolute prediction error is ${mae:,.2f}\")\n",
    "ratio = mae / rmse if rmse > 0 else 0\n",
    "print(f\"   • MAE/RMSE Ratio: {ratio:.3f}\")\n",
    "if ratio < 0.8:\n",
    "    print(f\"   • Large errors present (outliers) since MAE << RMSE\")\n",
    "elif ratio < 0.9:\n",
    "    print(f\"   • Some large errors present\")\n",
    "else:\n",
    "    print(f\"   • Errors are relatively uniform\")\n",
    "\n",
    "# Calculate additional evaluators\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "test_r2 = evaluator_r2.evaluate(predictions)\n",
    "test_rmse = evaluator_rmse.evaluate(predictions)\n",
    "test_mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n🎯 TEST SET PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Test RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"Test MAE: ${test_mae:,.2f}\")\n",
    "\n",
    "# Compare training vs test performance\n",
    "print(\"\\n📊 TRAINING vs TEST COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training R²: {r2:.4f} | Test R²: {test_r2:.4f} | Difference: {abs(r2-test_r2):.4f}\")\n",
    "if abs(r2 - test_r2) < 0.05:\n",
    "    print(\"✓ Excellent generalization - minimal overfitting\")\n",
    "elif abs(r2 - test_r2) < 0.10:\n",
    "    print(\"✓ Good generalization - acceptable overfitting\")\n",
    "elif abs(r2 - test_r2) < 0.15:\n",
    "    print(\"⚠ Moderate overfitting detected - consider regularization\")\n",
    "else:\n",
    "    print(\"❌ Significant overfitting - model may not generalize well\")\n",
    "    print(\"   Consider: reducing features, adding regularization, or collecting more data\")\n",
    "\n",
    "if stats_available:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"\\n🔍 STATISTICAL INSIGHTS:\")\n",
    "    print(\"-\" * 80)\n",
    "    sig_features = coef_df[coef_df['Significant'] != 'No'] if 'Significant' in coef_df.columns else pd.DataFrame()\n",
    "    if len(sig_features) > 0:\n",
    "        print(f\"Number of significant features (p < 0.05): {len(sig_features)}\")\n",
    "        print(f\"Total features: {len(coef_df)}\")\n",
    "        print(f\"Percentage significant: {len(sig_features)/len(coef_df)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nAdjusted R²: {1 - (1-r2)*(n-1)/(n-k-1):.4f}\")\n",
    "    print(f\"  • Accounts for number of predictors\")\n",
    "    print(f\"  • Better metric for comparing models with different numbers of features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ MODEL TRAINING AND EVALUATION COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2b679",
   "metadata": {},
   "source": [
    "# Generalized Linear Regression Summary\n",
    "The following statistical summary reveals that...\n",
    "- Experience matters significantly: each additional year of minimum or maximum experience requirement adds approximately $4,347 to the predicted salary, holding other factors constant. \n",
    "- Location is also a significant factor, with dramatic geographic variation—jobs in Omaha pay over $105,000 more than the baseline (York-Hanover, PA), while positions in Weirton-Steubenville pay $70,796 less, creating a range of $175,844 across metropolitan areas. Tech hubs like San Jose ($34,761 premium), San Francisco ($15,502 premium), and Austin ($10,004 premium) command substantial premiums, while many mid-sized and smaller cities show negative coefficients. \n",
    "- Remote work arrangements also significantly impact compensation: onsite positions pay $21,651 more than hybrid roles, while fully remote positions pay $1,998 less than hybrid, suggesting employers may offer location-based compensation adjustments or that hybrid roles command a premium for flexibility without full remote work.\n",
    "- There are several location coefficients of exactly $0, meaning those MSAs have identical salary expectations to the baseline after controlling for experience and work type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f3042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED GENERALIZED LINEAR REGRESSION MODEL INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "✓ All required variables found. Proceeding with interpretation...\n",
      "\n",
      "================================================================================\n",
      "PART 1: FEATURE NAME MAPPING\n",
      "================================================================================\n",
      "\n",
      "Number of MSA categories: 211\n",
      "Number of Remote Type categories: 3\n",
      "\n",
      "MSA Categories (Total: 211):\n",
      "  0: New York-Newark-Jersey City, NY-NJ-PA\n",
      "  1: Washington-Arlington-Alexandria, DC-VA-MD-WV\n",
      "  2: Los Angeles-Long Beach-Anaheim, CA\n",
      "  3: Dallas-Fort Worth-Arlington, TX\n",
      "  4: San Francisco-Oakland-Berkeley, CA\n",
      "  5: Chicago-Naperville-Elgin, IL-IN-WI\n",
      "  6: Boston-Cambridge-Newton, MA-NH\n",
      "  7: Denver-Aurora-Lakewood, CO\n",
      "  8: Philadelphia-Camden-Wilmington, PA-NJ-DE-MD\n",
      "  9: Tampa-St. Petersburg-Clearwater, FL\n",
      "  10: Seattle-Tacoma-Bellevue, WA\n",
      "  11: Phoenix-Mesa-Chandler, AZ\n",
      "  12: Austin-Round Rock-Georgetown, TX\n",
      "  13: Atlanta-Sandy Springs-Alpharetta, GA\n",
      "  14: San Jose-Sunnyvale-Santa Clara, CA\n",
      "  15: Houston-The Woodlands-Sugar Land, TX\n",
      "  16: Baltimore-Columbia-Towson, MD\n",
      "  17: Jackson, MS\n",
      "  18: Columbus, OH\n",
      "  19: Miami-Fort Lauderdale-Pompano Beach, FL\n",
      "  20: Providence-Warwick, RI-MA\n",
      "  21: Minneapolis-St. Paul-Bloomington, MN-WI\n",
      "  22: Buffalo-Cheektowaga, NY\n",
      "  23: Indianapolis-Carmel-Anderson, IN\n",
      "  24: Richmond, VA\n",
      "  25: Detroit-Warren-Dearborn, MI\n",
      "  26: Portland-Vancouver-Hillsboro, OR-WA\n",
      "  27: Springfield, IL\n",
      "  28: Charlotte-Concord-Gastonia, NC-SC\n",
      "  29: Fayetteville-Springdale-Rogers, AR\n",
      "  30: St. Louis, MO-IL\n",
      "  31: Raleigh-Cary, NC\n",
      "  32: Hartford-East Hartford-Middletown, CT\n",
      "  33: Kansas City, MO-KS\n",
      "  34: Trenton-Princeton, NJ\n",
      "  35: Nashville-Davidson--Murfreesboro--Franklin, TN\n",
      "  36: Jacksonville, FL\n",
      "  37: Sacramento-Roseville-Folsom, CA\n",
      "  38: Boise City, ID\n",
      "  39: Juneau, AK\n",
      "  40: Salt Lake City, UT\n",
      "  41: Tallahassee, FL\n",
      "  42: Little Rock-North Little Rock-Conway, AR\n",
      "  43: Urban Honolulu, HI\n",
      "  44: Dover, DE\n",
      "  45: Cleveland-Elyria, OH\n",
      "  46: Pierre, SD\n",
      "  47: Columbia, SC\n",
      "  48: Concord, NH\n",
      "  49: San Diego-Chula Vista-Carlsbad, CA\n",
      "  50: Augusta-Waterville, ME\n",
      "  51: Lansing-East Lansing, MI\n",
      "  52: Madison, WI\n",
      "  53: Salem, OR\n",
      "  54: Topeka, KS\n",
      "  55: Cincinnati, OH-KY-IN\n",
      "  56: Jefferson City, MO\n",
      "  57: Oklahoma City, OK\n",
      "  58: Olympia-Lacey-Tumwater, WA\n",
      "  59: Des Moines-West Des Moines, IA\n",
      "  60: Lincoln, NE\n",
      "  61: Pensacola-Ferry Pass-Brent, FL\n",
      "  62: Barre, VT\n",
      "  63: Baton Rouge, LA\n",
      "  64: Carson City, NV\n",
      "  65: Harrisburg-Carlisle, PA\n",
      "  66: Pittsburgh, PA\n",
      "  67: Rochester, NY\n",
      "  68: Binghamton, NY\n",
      "  69: Bridgeport-Stamford-Norwalk, CT\n",
      "  70: Frankfort, KY\n",
      "  71: Helena, MT\n",
      "  72: Albany-Schenectady-Troy, NY\n",
      "  73: Bismarck, ND\n",
      "  74: Virginia Beach-Norfolk-Newport News, VA-NC\n",
      "  75: Charleston, WV\n",
      "  76: Memphis, TN-MS-AR\n",
      "  77: Riverside-San Bernardino-Ontario, CA\n",
      "  78: Santa Fe, NM\n",
      "  79: Cheyenne, WY\n",
      "  80: Montgomery, AL\n",
      "  81: Coos Bay, OR\n",
      "  82: San Antonio-New Braunfels, TX\n",
      "  83: Greenville-Anderson, SC\n",
      "  84: Huntsville, AL\n",
      "  85: Louisville/Jefferson County, KY-IN\n",
      "  86: Orlando-Kissimmee-Sanford, FL\n",
      "  87: Winston-Salem, NC\n",
      "  88: Dayton-Kettering, OH\n",
      "  89: Las Vegas-Henderson-Paradise, NV\n",
      "  90: Sioux Falls, SD\n",
      "  91: Akron, OH\n",
      "  92: Albuquerque, NM\n",
      "  93: Charleston-North Charleston, SC\n",
      "  94: Durham-Chapel Hill, NC\n",
      "  95: Fort Collins, CO\n",
      "  96: Grand Rapids-Kentwood, MI\n",
      "  97: Greensboro-High Point, NC\n",
      "  98: Milwaukee-Waukesha, WI\n",
      "  99: Ann Arbor, MI\n",
      "  100: Colorado Springs, CO\n",
      "  101: Davenport-Moline-Rock Island, IA-IL\n",
      "  102: Oxnard-Thousand Oaks-Ventura, CA\n",
      "  103: Toledo, OH\n",
      "  104: Worcester, MA-CT\n",
      "  105: Fargo, ND-MN\n",
      "  106: Greeley, CO\n",
      "  107: Knoxville, TN\n",
      "  108: Lexington-Fayette, KY\n",
      "  109: Syracuse, NY\n",
      "  110: Augusta-Richmond County, GA-SC\n",
      "  111: Birmingham-Hoover, AL\n",
      "  112: Bremerton-Silverdale-Port Orchard, WA\n",
      "  113: Chattanooga, TN-GA\n",
      "  114: Green Bay, WI\n",
      "  115: Johnson City, TN\n",
      "  116: Manhattan, KS\n",
      "  117: Mount Pleasant, TX\n",
      "  118: Omaha-Council Bluffs, NE-IA\n",
      "  119: Owensboro, KY\n",
      "  120: Palm Bay-Melbourne-Titusville, FL\n",
      "  121: Rochester, MN\n",
      "  122: Santa Maria-Santa Barbara, CA\n",
      "  123: Spokane-Spokane Valley, WA\n",
      "  124: Springfield, MA\n",
      "  125: Utica-Rome, NY\n",
      "  126: Allentown-Bethlehem-Easton, PA-NJ\n",
      "  127: Anchorage, AK\n",
      "  128: Asheville, NC\n",
      "  129: Athens-Clarke County, GA\n",
      "  130: Bloomington, IL\n",
      "  131: Boulder, CO\n",
      "  132: Brevard, NC\n",
      "  133: Burlington, NC\n",
      "  134: El Paso, TX\n",
      "  135: Flint, MI\n",
      "  136: Fresno, CA\n",
      "  137: Ithaca, NY\n",
      "  138: Lakeland-Winter Haven, FL\n",
      "  139: Lancaster, PA\n",
      "  140: Michigan City-La Porte, IN\n",
      "  141: New Haven-Milford, CT\n",
      "  142: Ogden-Clearfield, UT\n",
      "  143: Portland-South Portland, ME\n",
      "  144: Poughkeepsie-Newburgh-Middletown, NY\n",
      "  145: Reading, PA\n",
      "  146: Santa Rosa-Petaluma, CA\n",
      "  147: Tulsa, OK\n",
      "  148: Warner Robins, GA\n",
      "  149: Wichita, KS\n",
      "  150: Appleton, WI\n",
      "  151: Athens, TX\n",
      "  152: Auburn, NY\n",
      "  153: Bakersfield, CA\n",
      "  154: Beckley, WV\n",
      "  155: Billings, MT\n",
      "  156: Burlington-South Burlington, VT\n",
      "  157: Cedar Rapids, IA\n",
      "  158: Cleveland, MS\n",
      "  159: College Station-Bryan, TX\n",
      "  160: Columbia, MO\n",
      "  161: Corning, NY\n",
      "  162: Corpus Christi, TX\n",
      "  163: Corvallis, OR\n",
      "  164: Crestview-Fort Walton Beach-Destin, FL\n",
      "  165: Decatur, IL\n",
      "  166: Defiance, OH\n",
      "  167: Elkhart-Goshen, IN\n",
      "  168: Erie, PA\n",
      "  169: Fort Wayne, IN\n",
      "  170: Gainesville, FL\n",
      "  171: Gainesville, GA\n",
      "  172: Glenwood Springs, CO\n",
      "  173: Gulfport-Biloxi, MS\n",
      "  174: Harrisonburg, VA\n",
      "  175: Holland, MI\n",
      "  176: Huntington-Ashland, WV-KY-OH\n",
      "  177: Jackson, MI\n",
      "  178: Kennewick-Richland, WA\n",
      "  179: Lafayette, LA\n",
      "  180: Lebanon, NH-VT\n",
      "  181: Manchester-Nashua, NH\n",
      "  182: Midland, MI\n",
      "  183: Modesto, CA\n",
      "  184: Napa, CA\n",
      "  185: New Orleans-Metairie, LA\n",
      "  186: North Port-Sarasota-Bradenton, FL\n",
      "  187: Oshkosh-Neenah, WI\n",
      "  188: Panama City, FL\n",
      "  189: Peoria, IL\n",
      "  190: Provo-Orem, UT\n",
      "  191: Pueblo, CO\n",
      "  192: Racine, WI\n",
      "  193: Redding, CA\n",
      "  194: Richmond-Berea, KY\n",
      "  195: Rocky Mount, NC\n",
      "  196: San Luis Obispo-Paso Robles, CA\n",
      "  197: Santa Cruz-Watsonville, CA\n",
      "  198: Savannah, GA\n",
      "  199: Scranton--Wilkes-Barre, PA\n",
      "  200: Springfield, MO\n",
      "  201: State College, PA\n",
      "  202: Stockton, CA\n",
      "  203: Tiffin, OH\n",
      "  204: Tucson, AZ\n",
      "  205: Ukiah, CA\n",
      "  206: Weirton-Steubenville, WV-OH\n",
      "  207: Wheeling, WV-OH\n",
      "  208: Wilmington, NC\n",
      "  209: Wilmington, OH\n",
      "  210: York-Hanover, PA\n",
      "\n",
      "⭐ Baseline MSA (reference category): York-Hanover, PA\n",
      "   All other MSA coefficients are relative to York-Hanover, PA\n",
      "\n",
      "Remote Type Categories (Total: 3):\n",
      "  0: Onsite\n",
      "  1: Remote\n",
      "  2: Hybrid\n",
      "\n",
      "⭐ Baseline Remote Type (reference category): Hybrid\n",
      "   All other Remote Type coefficients are relative to Hybrid\n",
      "\n",
      "Total features in model: 214\n",
      "\n",
      "Feature list:\n",
      "  1. MIN_YEARS_EXPERIENCE\n",
      "  2. MAX_YEARS_EXPERIENCE\n",
      "  3. MSA: New York-Newark-Jersey City, NY-NJ-PA\n",
      "  4. MSA: Washington-Arlington-Alexandria, DC-VA-MD-WV\n",
      "  5. MSA: Los Angeles-Long Beach-Anaheim, CA\n",
      "  6. MSA: Dallas-Fort Worth-Arlington, TX\n",
      "  7. MSA: San Francisco-Oakland-Berkeley, CA\n",
      "  8. MSA: Chicago-Naperville-Elgin, IL-IN-WI\n",
      "  9. MSA: Boston-Cambridge-Newton, MA-NH\n",
      "  10. MSA: Denver-Aurora-Lakewood, CO\n",
      "  11. MSA: Philadelphia-Camden-Wilmington, PA-NJ-DE-MD\n",
      "  12. MSA: Tampa-St. Petersburg-Clearwater, FL\n",
      "  13. MSA: Seattle-Tacoma-Bellevue, WA\n",
      "  14. MSA: Phoenix-Mesa-Chandler, AZ\n",
      "  15. MSA: Austin-Round Rock-Georgetown, TX\n",
      "  16. MSA: Atlanta-Sandy Springs-Alpharetta, GA\n",
      "  17. MSA: San Jose-Sunnyvale-Santa Clara, CA\n",
      "  18. MSA: Houston-The Woodlands-Sugar Land, TX\n",
      "  19. MSA: Baltimore-Columbia-Towson, MD\n",
      "  20. MSA: Jackson, MS\n",
      "  21. MSA: Columbus, OH\n",
      "  22. MSA: Miami-Fort Lauderdale-Pompano Beach, FL\n",
      "  23. MSA: Providence-Warwick, RI-MA\n",
      "  24. MSA: Minneapolis-St. Paul-Bloomington, MN-WI\n",
      "  25. MSA: Buffalo-Cheektowaga, NY\n",
      "  26. MSA: Indianapolis-Carmel-Anderson, IN\n",
      "  27. MSA: Richmond, VA\n",
      "  28. MSA: Detroit-Warren-Dearborn, MI\n",
      "  29. MSA: Portland-Vancouver-Hillsboro, OR-WA\n",
      "  30. MSA: Springfield, IL\n",
      "  31. MSA: Charlotte-Concord-Gastonia, NC-SC\n",
      "  32. MSA: Fayetteville-Springdale-Rogers, AR\n",
      "  33. MSA: St. Louis, MO-IL\n",
      "  34. MSA: Raleigh-Cary, NC\n",
      "  35. MSA: Hartford-East Hartford-Middletown, CT\n",
      "  36. MSA: Kansas City, MO-KS\n",
      "  37. MSA: Trenton-Princeton, NJ\n",
      "  38. MSA: Nashville-Davidson--Murfreesboro--Franklin, TN\n",
      "  39. MSA: Jacksonville, FL\n",
      "  40. MSA: Sacramento-Roseville-Folsom, CA\n",
      "  41. MSA: Boise City, ID\n",
      "  42. MSA: Juneau, AK\n",
      "  43. MSA: Salt Lake City, UT\n",
      "  44. MSA: Tallahassee, FL\n",
      "  45. MSA: Little Rock-North Little Rock-Conway, AR\n",
      "  46. MSA: Urban Honolulu, HI\n",
      "  47. MSA: Dover, DE\n",
      "  48. MSA: Cleveland-Elyria, OH\n",
      "  49. MSA: Pierre, SD\n",
      "  50. MSA: Columbia, SC\n",
      "  51. MSA: Concord, NH\n",
      "  52. MSA: San Diego-Chula Vista-Carlsbad, CA\n",
      "  53. MSA: Augusta-Waterville, ME\n",
      "  54. MSA: Lansing-East Lansing, MI\n",
      "  55. MSA: Madison, WI\n",
      "  56. MSA: Salem, OR\n",
      "  57. MSA: Topeka, KS\n",
      "  58. MSA: Cincinnati, OH-KY-IN\n",
      "  59. MSA: Jefferson City, MO\n",
      "  60. MSA: Oklahoma City, OK\n",
      "  61. MSA: Olympia-Lacey-Tumwater, WA\n",
      "  62. MSA: Des Moines-West Des Moines, IA\n",
      "  63. MSA: Lincoln, NE\n",
      "  64. MSA: Pensacola-Ferry Pass-Brent, FL\n",
      "  65. MSA: Barre, VT\n",
      "  66. MSA: Baton Rouge, LA\n",
      "  67. MSA: Carson City, NV\n",
      "  68. MSA: Harrisburg-Carlisle, PA\n",
      "  69. MSA: Pittsburgh, PA\n",
      "  70. MSA: Rochester, NY\n",
      "  71. MSA: Binghamton, NY\n",
      "  72. MSA: Bridgeport-Stamford-Norwalk, CT\n",
      "  73. MSA: Frankfort, KY\n",
      "  74. MSA: Helena, MT\n",
      "  75. MSA: Albany-Schenectady-Troy, NY\n",
      "  76. MSA: Bismarck, ND\n",
      "  77. MSA: Virginia Beach-Norfolk-Newport News, VA-NC\n",
      "  78. MSA: Charleston, WV\n",
      "  79. MSA: Memphis, TN-MS-AR\n",
      "  80. MSA: Riverside-San Bernardino-Ontario, CA\n",
      "  81. MSA: Santa Fe, NM\n",
      "  82. MSA: Cheyenne, WY\n",
      "  83. MSA: Montgomery, AL\n",
      "  84. MSA: Coos Bay, OR\n",
      "  85. MSA: San Antonio-New Braunfels, TX\n",
      "  86. MSA: Greenville-Anderson, SC\n",
      "  87. MSA: Huntsville, AL\n",
      "  88. MSA: Louisville/Jefferson County, KY-IN\n",
      "  89. MSA: Orlando-Kissimmee-Sanford, FL\n",
      "  90. MSA: Winston-Salem, NC\n",
      "  91. MSA: Dayton-Kettering, OH\n",
      "  92. MSA: Las Vegas-Henderson-Paradise, NV\n",
      "  93. MSA: Sioux Falls, SD\n",
      "  94. MSA: Akron, OH\n",
      "  95. MSA: Albuquerque, NM\n",
      "  96. MSA: Charleston-North Charleston, SC\n",
      "  97. MSA: Durham-Chapel Hill, NC\n",
      "  98. MSA: Fort Collins, CO\n",
      "  99. MSA: Grand Rapids-Kentwood, MI\n",
      "  100. MSA: Greensboro-High Point, NC\n",
      "  101. MSA: Milwaukee-Waukesha, WI\n",
      "  102. MSA: Ann Arbor, MI\n",
      "  103. MSA: Colorado Springs, CO\n",
      "  104. MSA: Davenport-Moline-Rock Island, IA-IL\n",
      "  105. MSA: Oxnard-Thousand Oaks-Ventura, CA\n",
      "  106. MSA: Toledo, OH\n",
      "  107. MSA: Worcester, MA-CT\n",
      "  108. MSA: Fargo, ND-MN\n",
      "  109. MSA: Greeley, CO\n",
      "  110. MSA: Knoxville, TN\n",
      "  111. MSA: Lexington-Fayette, KY\n",
      "  112. MSA: Syracuse, NY\n",
      "  113. MSA: Augusta-Richmond County, GA-SC\n",
      "  114. MSA: Birmingham-Hoover, AL\n",
      "  115. MSA: Bremerton-Silverdale-Port Orchard, WA\n",
      "  116. MSA: Chattanooga, TN-GA\n",
      "  117. MSA: Green Bay, WI\n",
      "  118. MSA: Johnson City, TN\n",
      "  119. MSA: Manhattan, KS\n",
      "  120. MSA: Mount Pleasant, TX\n",
      "  121. MSA: Omaha-Council Bluffs, NE-IA\n",
      "  122. MSA: Owensboro, KY\n",
      "  123. MSA: Palm Bay-Melbourne-Titusville, FL\n",
      "  124. MSA: Rochester, MN\n",
      "  125. MSA: Santa Maria-Santa Barbara, CA\n",
      "  126. MSA: Spokane-Spokane Valley, WA\n",
      "  127. MSA: Springfield, MA\n",
      "  128. MSA: Utica-Rome, NY\n",
      "  129. MSA: Allentown-Bethlehem-Easton, PA-NJ\n",
      "  130. MSA: Anchorage, AK\n",
      "  131. MSA: Asheville, NC\n",
      "  132. MSA: Athens-Clarke County, GA\n",
      "  133. MSA: Bloomington, IL\n",
      "  134. MSA: Boulder, CO\n",
      "  135. MSA: Brevard, NC\n",
      "  136. MSA: Burlington, NC\n",
      "  137. MSA: El Paso, TX\n",
      "  138. MSA: Flint, MI\n",
      "  139. MSA: Fresno, CA\n",
      "  140. MSA: Ithaca, NY\n",
      "  141. MSA: Lakeland-Winter Haven, FL\n",
      "  142. MSA: Lancaster, PA\n",
      "  143. MSA: Michigan City-La Porte, IN\n",
      "  144. MSA: New Haven-Milford, CT\n",
      "  145. MSA: Ogden-Clearfield, UT\n",
      "  146. MSA: Portland-South Portland, ME\n",
      "  147. MSA: Poughkeepsie-Newburgh-Middletown, NY\n",
      "  148. MSA: Reading, PA\n",
      "  149. MSA: Santa Rosa-Petaluma, CA\n",
      "  150. MSA: Tulsa, OK\n",
      "  151. MSA: Warner Robins, GA\n",
      "  152. MSA: Wichita, KS\n",
      "  153. MSA: Appleton, WI\n",
      "  154. MSA: Athens, TX\n",
      "  155. MSA: Auburn, NY\n",
      "  156. MSA: Bakersfield, CA\n",
      "  157. MSA: Beckley, WV\n",
      "  158. MSA: Billings, MT\n",
      "  159. MSA: Burlington-South Burlington, VT\n",
      "  160. MSA: Cedar Rapids, IA\n",
      "  161. MSA: Cleveland, MS\n",
      "  162. MSA: College Station-Bryan, TX\n",
      "  163. MSA: Columbia, MO\n",
      "  164. MSA: Corning, NY\n",
      "  165. MSA: Corpus Christi, TX\n",
      "  166. MSA: Corvallis, OR\n",
      "  167. MSA: Crestview-Fort Walton Beach-Destin, FL\n",
      "  168. MSA: Decatur, IL\n",
      "  169. MSA: Defiance, OH\n",
      "  170. MSA: Elkhart-Goshen, IN\n",
      "  171. MSA: Erie, PA\n",
      "  172. MSA: Fort Wayne, IN\n",
      "  173. MSA: Gainesville, FL\n",
      "  174. MSA: Gainesville, GA\n",
      "  175. MSA: Glenwood Springs, CO\n",
      "  176. MSA: Gulfport-Biloxi, MS\n",
      "  177. MSA: Harrisonburg, VA\n",
      "  178. MSA: Holland, MI\n",
      "  179. MSA: Huntington-Ashland, WV-KY-OH\n",
      "  180. MSA: Jackson, MI\n",
      "  181. MSA: Kennewick-Richland, WA\n",
      "  182. MSA: Lafayette, LA\n",
      "  183. MSA: Lebanon, NH-VT\n",
      "  184. MSA: Manchester-Nashua, NH\n",
      "  185. MSA: Midland, MI\n",
      "  186. MSA: Modesto, CA\n",
      "  187. MSA: Napa, CA\n",
      "  188. MSA: New Orleans-Metairie, LA\n",
      "  189. MSA: North Port-Sarasota-Bradenton, FL\n",
      "  190. MSA: Oshkosh-Neenah, WI\n",
      "  191. MSA: Panama City, FL\n",
      "  192. MSA: Peoria, IL\n",
      "  193. MSA: Provo-Orem, UT\n",
      "  194. MSA: Pueblo, CO\n",
      "  195. MSA: Racine, WI\n",
      "  196. MSA: Redding, CA\n",
      "  197. MSA: Richmond-Berea, KY\n",
      "  198. MSA: Rocky Mount, NC\n",
      "  199. MSA: San Luis Obispo-Paso Robles, CA\n",
      "  200. MSA: Santa Cruz-Watsonville, CA\n",
      "  201. MSA: Savannah, GA\n",
      "  202. MSA: Scranton--Wilkes-Barre, PA\n",
      "  203. MSA: Springfield, MO\n",
      "  204. MSA: State College, PA\n",
      "  205. MSA: Stockton, CA\n",
      "  206. MSA: Tiffin, OH\n",
      "  207. MSA: Tucson, AZ\n",
      "  208. MSA: Ukiah, CA\n",
      "  209. MSA: Weirton-Steubenville, WV-OH\n",
      "  210. MSA: Wheeling, WV-OH\n",
      "  211. MSA: Wilmington, NC\n",
      "  212. MSA: Wilmington, OH\n",
      "  213. Remote: Onsite\n",
      "  214. Remote: Remote\n",
      "\n",
      "================================================================================\n",
      "PART 2: COMPREHENSIVE COEFFICIENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FULL COEFFICIENT TABLE WITH DETAILED FEATURE NAMES\n",
      "================================================================================\n",
      "\n",
      "Significance codes: '***' p<0.001, '**' p<0.01, '*' p<0.05, '.' p<0.10\n",
      "\n",
      "                                            Feature   Coefficient Coef_Formatted\n",
      "                               MIN_YEARS_EXPERIENCE   4346.702719      $4,346.70\n",
      "                               MAX_YEARS_EXPERIENCE   4346.702719      $4,346.70\n",
      "         MSA: New York-Newark-Jersey City, NY-NJ-PA  12170.474476     $12,170.47\n",
      "  MSA: Washington-Arlington-Alexandria, DC-VA-MD-WV   3875.848099      $3,875.85\n",
      "            MSA: Los Angeles-Long Beach-Anaheim, CA  -2216.381832     $-2,216.38\n",
      "               MSA: Dallas-Fort Worth-Arlington, TX    263.206998        $263.21\n",
      "            MSA: San Francisco-Oakland-Berkeley, CA  15502.361770     $15,502.36\n",
      "            MSA: Chicago-Naperville-Elgin, IL-IN-WI   4670.381095      $4,670.38\n",
      "                MSA: Boston-Cambridge-Newton, MA-NH   1336.867872      $1,336.87\n",
      "                    MSA: Denver-Aurora-Lakewood, CO  -4322.782072     $-4,322.78\n",
      "   MSA: Philadelphia-Camden-Wilmington, PA-NJ-DE-MD  -8193.486727     $-8,193.49\n",
      "           MSA: Tampa-St. Petersburg-Clearwater, FL    145.396541        $145.40\n",
      "                   MSA: Seattle-Tacoma-Bellevue, WA   6137.504086      $6,137.50\n",
      "                     MSA: Phoenix-Mesa-Chandler, AZ   4558.735547      $4,558.74\n",
      "              MSA: Austin-Round Rock-Georgetown, TX  10003.789801     $10,003.79\n",
      "          MSA: Atlanta-Sandy Springs-Alpharetta, GA   1349.100445      $1,349.10\n",
      "            MSA: San Jose-Sunnyvale-Santa Clara, CA  34760.900322     $34,760.90\n",
      "          MSA: Houston-The Woodlands-Sugar Land, TX   1979.252241      $1,979.25\n",
      "                 MSA: Baltimore-Columbia-Towson, MD  -2376.652197     $-2,376.65\n",
      "                                   MSA: Jackson, MS -21038.614191    $-21,038.61\n",
      "                                  MSA: Columbus, OH   9384.031296      $9,384.03\n",
      "       MSA: Miami-Fort Lauderdale-Pompano Beach, FL  -4119.615087     $-4,119.62\n",
      "                     MSA: Providence-Warwick, RI-MA   1783.074739      $1,783.07\n",
      "       MSA: Minneapolis-St. Paul-Bloomington, MN-WI -11785.320427    $-11,785.32\n",
      "                       MSA: Buffalo-Cheektowaga, NY  -4338.026653     $-4,338.03\n",
      "              MSA: Indianapolis-Carmel-Anderson, IN  -9262.264065     $-9,262.26\n",
      "                                  MSA: Richmond, VA  -4073.126130     $-4,073.13\n",
      "                   MSA: Detroit-Warren-Dearborn, MI -12340.412635    $-12,340.41\n",
      "           MSA: Portland-Vancouver-Hillsboro, OR-WA   1584.477699      $1,584.48\n",
      "                               MSA: Springfield, IL   2884.435757      $2,884.44\n",
      "             MSA: Charlotte-Concord-Gastonia, NC-SC  -2188.945114     $-2,188.95\n",
      "            MSA: Fayetteville-Springdale-Rogers, AR   8376.130855      $8,376.13\n",
      "                              MSA: St. Louis, MO-IL  -7811.367847     $-7,811.37\n",
      "                              MSA: Raleigh-Cary, NC  -4161.945207     $-4,161.95\n",
      "         MSA: Hartford-East Hartford-Middletown, CT   6902.219816      $6,902.22\n",
      "                            MSA: Kansas City, MO-KS  -8380.752993     $-8,380.75\n",
      "                         MSA: Trenton-Princeton, NJ  -6881.457229     $-6,881.46\n",
      "MSA: Nashville-Davidson--Murfreesboro--Franklin, TN -13700.804258    $-13,700.80\n",
      "                              MSA: Jacksonville, FL   5862.274449      $5,862.27\n",
      "               MSA: Sacramento-Roseville-Folsom, CA  -2084.520353     $-2,084.52\n",
      "                                MSA: Boise City, ID  -7105.347401     $-7,105.35\n",
      "                                    MSA: Juneau, AK  -4797.847376     $-4,797.85\n",
      "                            MSA: Salt Lake City, UT -11661.242929    $-11,661.24\n",
      "                               MSA: Tallahassee, FL  -1139.490607     $-1,139.49\n",
      "      MSA: Little Rock-North Little Rock-Conway, AR   1785.840194      $1,785.84\n",
      "                            MSA: Urban Honolulu, HI  -6453.594774     $-6,453.59\n",
      "                                     MSA: Dover, DE    683.854911        $683.85\n",
      "                          MSA: Cleveland-Elyria, OH -11992.952396    $-11,992.95\n",
      "                                    MSA: Pierre, SD  -6706.521349     $-6,706.52\n",
      "                                  MSA: Columbia, SC   4781.065801      $4,781.07\n",
      "                                   MSA: Concord, NH -12529.475245    $-12,529.48\n",
      "            MSA: San Diego-Chula Vista-Carlsbad, CA -13915.823555    $-13,915.82\n",
      "                        MSA: Augusta-Waterville, ME  -3502.451611     $-3,502.45\n",
      "                      MSA: Lansing-East Lansing, MI  -1349.346059     $-1,349.35\n",
      "                                   MSA: Madison, WI  -4280.771864     $-4,280.77\n",
      "                                     MSA: Salem, OR   3297.434770      $3,297.43\n",
      "                                    MSA: Topeka, KS  -9538.294799     $-9,538.29\n",
      "                          MSA: Cincinnati, OH-KY-IN -11381.382182    $-11,381.38\n",
      "                            MSA: Jefferson City, MO  -6429.185482     $-6,429.19\n",
      "                             MSA: Oklahoma City, OK  -7614.609856     $-7,614.61\n",
      "                    MSA: Olympia-Lacey-Tumwater, WA  11206.071331     $11,206.07\n",
      "                MSA: Des Moines-West Des Moines, IA    128.055612        $128.06\n",
      "                                   MSA: Lincoln, NE  -7933.670013     $-7,933.67\n",
      "                MSA: Pensacola-Ferry Pass-Brent, FL  19539.456487     $19,539.46\n",
      "                                     MSA: Barre, VT    291.933041        $291.93\n",
      "                               MSA: Baton Rouge, LA  -6449.462449     $-6,449.46\n",
      "                               MSA: Carson City, NV  -8630.425726     $-8,630.43\n",
      "                       MSA: Harrisburg-Carlisle, PA  -2884.297705     $-2,884.30\n",
      "                                MSA: Pittsburgh, PA  -3864.411093     $-3,864.41\n",
      "                                 MSA: Rochester, NY -27288.238879    $-27,288.24\n",
      "                                MSA: Binghamton, NY -15865.162822    $-15,865.16\n",
      "               MSA: Bridgeport-Stamford-Norwalk, CT  23065.374483     $23,065.37\n",
      "                                 MSA: Frankfort, KY  -3705.283731     $-3,705.28\n",
      "                                    MSA: Helena, MT  -4042.050758     $-4,042.05\n",
      "                   MSA: Albany-Schenectady-Troy, NY  -9149.273541     $-9,149.27\n",
      "                                  MSA: Bismarck, ND  -7529.443117     $-7,529.44\n",
      "    MSA: Virginia Beach-Norfolk-Newport News, VA-NC -32393.939812    $-32,393.94\n",
      "                                MSA: Charleston, WV  -5137.380171     $-5,137.38\n",
      "                             MSA: Memphis, TN-MS-AR -17597.297984    $-17,597.30\n",
      "          MSA: Riverside-San Bernardino-Ontario, CA -21044.787672    $-21,044.79\n",
      "                                  MSA: Santa Fe, NM  -7241.611940     $-7,241.61\n",
      "                                  MSA: Cheyenne, WY -16438.221273    $-16,438.22\n",
      "                                MSA: Montgomery, AL   2055.599598      $2,055.60\n",
      "                                  MSA: Coos Bay, OR  -1166.795379     $-1,166.80\n",
      "                 MSA: San Antonio-New Braunfels, TX -12699.280211    $-12,699.28\n",
      "                       MSA: Greenville-Anderson, SC  -2874.216205     $-2,874.22\n",
      "                                MSA: Huntsville, AL    383.284618        $383.28\n",
      "            MSA: Louisville/Jefferson County, KY-IN -19909.218749    $-19,909.22\n",
      "                 MSA: Orlando-Kissimmee-Sanford, FL -18033.009072    $-18,033.01\n",
      "                             MSA: Winston-Salem, NC  -5990.384286     $-5,990.38\n",
      "                          MSA: Dayton-Kettering, OH   2874.293438      $2,874.29\n",
      "              MSA: Las Vegas-Henderson-Paradise, NV -22478.953483    $-22,478.95\n",
      "                               MSA: Sioux Falls, SD -18276.793008    $-18,276.79\n",
      "                                     MSA: Akron, OH  11076.221747     $11,076.22\n",
      "                               MSA: Albuquerque, NM  -6997.400874     $-6,997.40\n",
      "               MSA: Charleston-North Charleston, SC -21363.233222    $-21,363.23\n",
      "                        MSA: Durham-Chapel Hill, NC  -4282.239122     $-4,282.24\n",
      "                              MSA: Fort Collins, CO    620.130524        $620.13\n",
      "                     MSA: Grand Rapids-Kentwood, MI -17852.953126    $-17,852.95\n",
      "                     MSA: Greensboro-High Point, NC  -8570.481769     $-8,570.48\n",
      "                        MSA: Milwaukee-Waukesha, WI  -1991.494153     $-1,991.49\n",
      "                                 MSA: Ann Arbor, MI -19945.332768    $-19,945.33\n",
      "                          MSA: Colorado Springs, CO  -1275.908462     $-1,275.91\n",
      "           MSA: Davenport-Moline-Rock Island, IA-IL -26983.488531    $-26,983.49\n",
      "              MSA: Oxnard-Thousand Oaks-Ventura, CA  -9558.662530     $-9,558.66\n",
      "                                    MSA: Toledo, OH  13522.816148     $13,522.82\n",
      "                              MSA: Worcester, MA-CT -18730.909770    $-18,730.91\n",
      "                                  MSA: Fargo, ND-MN -22523.678934    $-22,523.68\n",
      "                                   MSA: Greeley, CO  -5628.260678     $-5,628.26\n",
      "                                 MSA: Knoxville, TN -11694.517063    $-11,694.52\n",
      "                         MSA: Lexington-Fayette, KY -22954.842653    $-22,954.84\n",
      "                                  MSA: Syracuse, NY -28775.538372    $-28,775.54\n",
      "                MSA: Augusta-Richmond County, GA-SC  52983.872969     $52,983.87\n",
      "                         MSA: Birmingham-Hoover, AL -14988.722247    $-14,988.72\n",
      "         MSA: Bremerton-Silverdale-Port Orchard, WA -39604.318889    $-39,604.32\n",
      "                            MSA: Chattanooga, TN-GA -26570.910368    $-26,570.91\n",
      "                                 MSA: Green Bay, WI  -6333.413390     $-6,333.41\n",
      "                              MSA: Johnson City, TN   9147.981012      $9,147.98\n",
      "                                 MSA: Manhattan, KS -28217.126120    $-28,217.13\n",
      "                            MSA: Mount Pleasant, TX -41741.911510    $-41,741.91\n",
      "                   MSA: Omaha-Council Bluffs, NE-IA 105048.002628    $105,048.00\n",
      "                                 MSA: Owensboro, KY -10926.666008    $-10,926.67\n",
      "             MSA: Palm Bay-Melbourne-Titusville, FL  -7773.316493     $-7,773.32\n",
      "                                 MSA: Rochester, MN -15833.113330    $-15,833.11\n",
      "                 MSA: Santa Maria-Santa Barbara, CA  -5523.388196     $-5,523.39\n",
      "                    MSA: Spokane-Spokane Valley, WA -24034.058601    $-24,034.06\n",
      "                               MSA: Springfield, MA   6801.169252      $6,801.17\n",
      "                                MSA: Utica-Rome, NY  -2393.946244     $-2,393.95\n",
      "             MSA: Allentown-Bethlehem-Easton, PA-NJ      0.000000          $0.00\n",
      "                                 MSA: Anchorage, AK -12095.424632    $-12,095.42\n",
      "                                 MSA: Asheville, NC   6851.061423      $6,851.06\n",
      "                      MSA: Athens-Clarke County, GA -10568.939870    $-10,568.94\n",
      "                               MSA: Bloomington, IL -14715.762237    $-14,715.76\n",
      "                                   MSA: Boulder, CO  -8655.131632     $-8,655.13\n",
      "                                   MSA: Brevard, NC -21248.940674    $-21,248.94\n",
      "                                MSA: Burlington, NC   6851.061423      $6,851.06\n",
      "                                   MSA: El Paso, TX -16358.613369    $-16,358.61\n",
      "                                     MSA: Flint, MI -36135.649058    $-36,135.65\n",
      "                                    MSA: Fresno, CA      0.000000          $0.00\n",
      "                                    MSA: Ithaca, NY -19658.928696    $-19,658.93\n",
      "                     MSA: Lakeland-Winter Haven, FL -13448.832273    $-13,448.83\n",
      "                                 MSA: Lancaster, PA -19610.832736    $-19,610.83\n",
      "                    MSA: Michigan City-La Porte, IN   6251.277024      $6,251.28\n",
      "                         MSA: New Haven-Milford, CT -40355.534563    $-40,355.53\n",
      "                          MSA: Ogden-Clearfield, UT  -8862.124674     $-8,862.12\n",
      "                   MSA: Portland-South Portland, ME   6494.468954      $6,494.47\n",
      "          MSA: Poughkeepsie-Newburgh-Middletown, NY  30611.390166     $30,611.39\n",
      "                                   MSA: Reading, PA -13656.909396    $-13,656.91\n",
      "                       MSA: Santa Rosa-Petaluma, CA -39828.911345    $-39,828.91\n",
      "                                     MSA: Tulsa, OK -31748.725818    $-31,748.73\n",
      "                             MSA: Warner Robins, GA      0.000000          $0.00\n",
      "                                   MSA: Wichita, KS -19264.928666    $-19,264.93\n",
      "                                  MSA: Appleton, WI      0.000000          $0.00\n",
      "                                    MSA: Athens, TX   9577.647115      $9,577.65\n",
      "                                    MSA: Auburn, NY   6851.061423      $6,851.06\n",
      "                               MSA: Bakersfield, CA -14715.762237    $-14,715.76\n",
      "                                   MSA: Beckley, WV      0.000000          $0.00\n",
      "                                  MSA: Billings, MT  -4968.977150     $-4,968.98\n",
      "               MSA: Burlington-South Burlington, VT  32164.463877     $32,164.46\n",
      "                              MSA: Cedar Rapids, IA      0.000000          $0.00\n",
      "                                 MSA: Cleveland, MS  13557.870026     $13,557.87\n",
      "                     MSA: College Station-Bryan, TX -17768.717800    $-17,768.72\n",
      "                                  MSA: Columbia, MO -24442.132811    $-24,442.13\n",
      "                                   MSA: Corning, NY  16564.462712     $16,564.46\n",
      "                            MSA: Corpus Christi, TX      0.000000          $0.00\n",
      "                                 MSA: Corvallis, OR   8751.277205      $8,751.28\n",
      "        MSA: Crestview-Fort Walton Beach-Destin, FL  17331.500460     $17,331.50\n",
      "                                   MSA: Decatur, IL -34942.133595    $-34,942.13\n",
      "                                  MSA: Defiance, OH      0.000000          $0.00\n",
      "                            MSA: Elkhart-Goshen, IN -25761.910295    $-25,761.91\n",
      "                                      MSA: Erie, PA      0.000000          $0.00\n",
      "                                MSA: Fort Wayne, IN      0.000000          $0.00\n",
      "                               MSA: Gainesville, FL -10611.316702    $-10,611.32\n",
      "                               MSA: Gainesville, GA   6851.061423      $6,851.06\n",
      "                          MSA: Glenwood Springs, CO      0.000000          $0.00\n",
      "                           MSA: Gulfport-Biloxi, MS -19027.317330    $-19,027.32\n",
      "                              MSA: Harrisonburg, VA -20865.762696    $-20,865.76\n",
      "                                   MSA: Holland, MI      0.000000          $0.00\n",
      "                  MSA: Huntington-Ashland, WV-KY-OH -46499.319381    $-46,499.32\n",
      "                                   MSA: Jackson, MI    751.276608        $751.28\n",
      "                        MSA: Kennewick-Richland, WA    834.684152        $834.68\n",
      "                                 MSA: Lafayette, LA   8456.528522      $8,456.53\n",
      "                                MSA: Lebanon, NH-VT -17248.724736    $-17,248.72\n",
      "                         MSA: Manchester-Nashua, NH      0.000000          $0.00\n",
      "                                   MSA: Midland, MI  56264.465676     $56,264.47\n",
      "                                   MSA: Modesto, CA  16138.092833     $16,138.09\n",
      "                                      MSA: Napa, CA -13248.724437    $-13,248.72\n",
      "                      MSA: New Orleans-Metairie, LA  37184.241638     $37,184.24\n",
      "             MSA: North Port-Sarasota-Bradenton, FL -15055.317034    $-15,055.32\n",
      "                            MSA: Oshkosh-Neenah, WI      0.000000          $0.00\n",
      "                               MSA: Panama City, FL  61831.503782     $61,831.50\n",
      "                                    MSA: Peoria, IL -17361.909668    $-17,361.91\n",
      "                                MSA: Provo-Orem, UT      0.000000          $0.00\n",
      "                                    MSA: Pueblo, CO      0.000000          $0.00\n",
      "                                    MSA: Racine, WI  22549.937112     $22,549.94\n",
      "                                   MSA: Redding, CA  38544.686968     $38,544.69\n",
      "                            MSA: Richmond-Berea, KY -14587.792944    $-14,587.79\n",
      "                               MSA: Rocky Mount, NC  -2609.168871     $-2,609.17\n",
      "               MSA: San Luis Obispo-Paso Robles, CA      0.000000          $0.00\n",
      "                    MSA: Santa Cruz-Watsonville, CA -24570.317744    $-24,570.32\n",
      "                                  MSA: Savannah, GA  -4715.761490     $-4,715.76\n",
      "                    MSA: Scranton--Wilkes-Barre, PA      0.000000          $0.00\n",
      "                               MSA: Springfield, MO      0.000000          $0.00\n",
      "                             MSA: State College, PA  10331.499938     $10,331.50\n",
      "                                  MSA: Stockton, CA  -6776.908878     $-6,776.91\n",
      "                                    MSA: Tiffin, OH      0.000000          $0.00\n",
      "                                    MSA: Tucson, AZ  63264.466199     $63,264.47\n",
      "                                     MSA: Ukiah, CA -34614.318494    $-34,614.32\n",
      "                   MSA: Weirton-Steubenville, WV-OH -70795.989038    $-70,795.99\n",
      "                               MSA: Wheeling, WV-OH  -9155.532234     $-9,155.53\n",
      "                                MSA: Wilmington, NC  10231.061676     $10,231.06\n",
      "                                MSA: Wilmington, OH      0.000000          $0.00\n",
      "                                     Remote: Onsite  21651.240478     $21,651.24\n",
      "                                     Remote: Remote  -1997.969903     $-1,997.97\n",
      "\n",
      "================================================================================\n",
      "PART 3: INTERPRETATION OF NUMERICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Feature: MIN_YEARS_EXPERIENCE\n",
      "======================================================================\n",
      "Coefficient: $4,346.70\n",
      "\n",
      "📊 INTERPRETATION:\n",
      "• For each additional year in min years experience,\n",
      "  the predicted salary INCREASES by $4,346.70, holding all other factors constant.\n",
      "\n",
      "======================================================================\n",
      "Feature: MAX_YEARS_EXPERIENCE\n",
      "======================================================================\n",
      "Coefficient: $4,346.70\n",
      "\n",
      "📊 INTERPRETATION:\n",
      "• For each additional year in max years experience,\n",
      "  the predicted salary INCREASES by $4,346.70, holding all other factors constant.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PART 4: INTERPRETATION OF MSA (LOCATION) EFFECTS\n",
      "================================================================================\n",
      "\n",
      "⭐ Baseline/Reference Category: York-Hanover, PA\n",
      "   All coefficients below compare each MSA to York-Hanover, PA\n",
      "\n",
      "======================================================================\n",
      "MSA SALARY EFFECTS (sorted by salary impact)\n",
      "======================================================================\n",
      "\n",
      "Location: Omaha-Council Bluffs, NE-IA\n",
      "  Coefficient: $105,048.00\n",
      "  💰 Jobs in Omaha-Council Bluffs, NE-IA pay $105,048.00 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Tucson, AZ\n",
      "  Coefficient: $63,264.47\n",
      "  💰 Jobs in Tucson, AZ pay $63,264.47 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Panama City, FL\n",
      "  Coefficient: $61,831.50\n",
      "  💰 Jobs in Panama City, FL pay $61,831.50 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Midland, MI\n",
      "  Coefficient: $56,264.47\n",
      "  💰 Jobs in Midland, MI pay $56,264.47 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Augusta-Richmond County, GA-SC\n",
      "  Coefficient: $52,983.87\n",
      "  💰 Jobs in Augusta-Richmond County, GA-SC pay $52,983.87 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Redding, CA\n",
      "  Coefficient: $38,544.69\n",
      "  💰 Jobs in Redding, CA pay $38,544.69 MORE than York-Hanover, PA\n",
      "\n",
      "Location: New Orleans-Metairie, LA\n",
      "  Coefficient: $37,184.24\n",
      "  💰 Jobs in New Orleans-Metairie, LA pay $37,184.24 MORE than York-Hanover, PA\n",
      "\n",
      "Location: San Jose-Sunnyvale-Santa Clara, CA\n",
      "  Coefficient: $34,760.90\n",
      "  💰 Jobs in San Jose-Sunnyvale-Santa Clara, CA pay $34,760.90 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Burlington-South Burlington, VT\n",
      "  Coefficient: $32,164.46\n",
      "  💰 Jobs in Burlington-South Burlington, VT pay $32,164.46 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Poughkeepsie-Newburgh-Middletown, NY\n",
      "  Coefficient: $30,611.39\n",
      "  💰 Jobs in Poughkeepsie-Newburgh-Middletown, NY pay $30,611.39 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Bridgeport-Stamford-Norwalk, CT\n",
      "  Coefficient: $23,065.37\n",
      "  💰 Jobs in Bridgeport-Stamford-Norwalk, CT pay $23,065.37 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Racine, WI\n",
      "  Coefficient: $22,549.94\n",
      "  💰 Jobs in Racine, WI pay $22,549.94 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Pensacola-Ferry Pass-Brent, FL\n",
      "  Coefficient: $19,539.46\n",
      "  💰 Jobs in Pensacola-Ferry Pass-Brent, FL pay $19,539.46 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Crestview-Fort Walton Beach-Destin, FL\n",
      "  Coefficient: $17,331.50\n",
      "  💰 Jobs in Crestview-Fort Walton Beach-Destin, FL pay $17,331.50 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Corning, NY\n",
      "  Coefficient: $16,564.46\n",
      "  💰 Jobs in Corning, NY pay $16,564.46 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Modesto, CA\n",
      "  Coefficient: $16,138.09\n",
      "  💰 Jobs in Modesto, CA pay $16,138.09 MORE than York-Hanover, PA\n",
      "\n",
      "Location: San Francisco-Oakland-Berkeley, CA\n",
      "  Coefficient: $15,502.36\n",
      "  💰 Jobs in San Francisco-Oakland-Berkeley, CA pay $15,502.36 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Cleveland, MS\n",
      "  Coefficient: $13,557.87\n",
      "  💰 Jobs in Cleveland, MS pay $13,557.87 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Toledo, OH\n",
      "  Coefficient: $13,522.82\n",
      "  💰 Jobs in Toledo, OH pay $13,522.82 MORE than York-Hanover, PA\n",
      "\n",
      "Location: New York-Newark-Jersey City, NY-NJ-PA\n",
      "  Coefficient: $12,170.47\n",
      "  💰 Jobs in New York-Newark-Jersey City, NY-NJ-PA pay $12,170.47 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Olympia-Lacey-Tumwater, WA\n",
      "  Coefficient: $11,206.07\n",
      "  💰 Jobs in Olympia-Lacey-Tumwater, WA pay $11,206.07 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Akron, OH\n",
      "  Coefficient: $11,076.22\n",
      "  💰 Jobs in Akron, OH pay $11,076.22 MORE than York-Hanover, PA\n",
      "\n",
      "Location: State College, PA\n",
      "  Coefficient: $10,331.50\n",
      "  💰 Jobs in State College, PA pay $10,331.50 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Wilmington, NC\n",
      "  Coefficient: $10,231.06\n",
      "  💰 Jobs in Wilmington, NC pay $10,231.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Austin-Round Rock-Georgetown, TX\n",
      "  Coefficient: $10,003.79\n",
      "  💰 Jobs in Austin-Round Rock-Georgetown, TX pay $10,003.79 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Athens, TX\n",
      "  Coefficient: $9,577.65\n",
      "  💰 Jobs in Athens, TX pay $9,577.65 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Columbus, OH\n",
      "  Coefficient: $9,384.03\n",
      "  💰 Jobs in Columbus, OH pay $9,384.03 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Johnson City, TN\n",
      "  Coefficient: $9,147.98\n",
      "  💰 Jobs in Johnson City, TN pay $9,147.98 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Corvallis, OR\n",
      "  Coefficient: $8,751.28\n",
      "  💰 Jobs in Corvallis, OR pay $8,751.28 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Lafayette, LA\n",
      "  Coefficient: $8,456.53\n",
      "  💰 Jobs in Lafayette, LA pay $8,456.53 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Fayetteville-Springdale-Rogers, AR\n",
      "  Coefficient: $8,376.13\n",
      "  💰 Jobs in Fayetteville-Springdale-Rogers, AR pay $8,376.13 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Hartford-East Hartford-Middletown, CT\n",
      "  Coefficient: $6,902.22\n",
      "  💰 Jobs in Hartford-East Hartford-Middletown, CT pay $6,902.22 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Burlington, NC\n",
      "  Coefficient: $6,851.06\n",
      "  💰 Jobs in Burlington, NC pay $6,851.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Asheville, NC\n",
      "  Coefficient: $6,851.06\n",
      "  💰 Jobs in Asheville, NC pay $6,851.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Auburn, NY\n",
      "  Coefficient: $6,851.06\n",
      "  💰 Jobs in Auburn, NY pay $6,851.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Gainesville, GA\n",
      "  Coefficient: $6,851.06\n",
      "  💰 Jobs in Gainesville, GA pay $6,851.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Springfield, MA\n",
      "  Coefficient: $6,801.17\n",
      "  💰 Jobs in Springfield, MA pay $6,801.17 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Portland-South Portland, ME\n",
      "  Coefficient: $6,494.47\n",
      "  💰 Jobs in Portland-South Portland, ME pay $6,494.47 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Michigan City-La Porte, IN\n",
      "  Coefficient: $6,251.28\n",
      "  💰 Jobs in Michigan City-La Porte, IN pay $6,251.28 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Seattle-Tacoma-Bellevue, WA\n",
      "  Coefficient: $6,137.50\n",
      "  💰 Jobs in Seattle-Tacoma-Bellevue, WA pay $6,137.50 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Jacksonville, FL\n",
      "  Coefficient: $5,862.27\n",
      "  💰 Jobs in Jacksonville, FL pay $5,862.27 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Columbia, SC\n",
      "  Coefficient: $4,781.07\n",
      "  💰 Jobs in Columbia, SC pay $4,781.07 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Chicago-Naperville-Elgin, IL-IN-WI\n",
      "  Coefficient: $4,670.38\n",
      "  💰 Jobs in Chicago-Naperville-Elgin, IL-IN-WI pay $4,670.38 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Phoenix-Mesa-Chandler, AZ\n",
      "  Coefficient: $4,558.74\n",
      "  💰 Jobs in Phoenix-Mesa-Chandler, AZ pay $4,558.74 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Washington-Arlington-Alexandria, DC-VA-MD-WV\n",
      "  Coefficient: $3,875.85\n",
      "  💰 Jobs in Washington-Arlington-Alexandria, DC-VA-MD-WV pay $3,875.85 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Salem, OR\n",
      "  Coefficient: $3,297.43\n",
      "  💰 Jobs in Salem, OR pay $3,297.43 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Springfield, IL\n",
      "  Coefficient: $2,884.44\n",
      "  💰 Jobs in Springfield, IL pay $2,884.44 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Dayton-Kettering, OH\n",
      "  Coefficient: $2,874.29\n",
      "  💰 Jobs in Dayton-Kettering, OH pay $2,874.29 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Montgomery, AL\n",
      "  Coefficient: $2,055.60\n",
      "  💰 Jobs in Montgomery, AL pay $2,055.60 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Houston-The Woodlands-Sugar Land, TX\n",
      "  Coefficient: $1,979.25\n",
      "  💰 Jobs in Houston-The Woodlands-Sugar Land, TX pay $1,979.25 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Little Rock-North Little Rock-Conway, AR\n",
      "  Coefficient: $1,785.84\n",
      "  💰 Jobs in Little Rock-North Little Rock-Conway, AR pay $1,785.84 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Providence-Warwick, RI-MA\n",
      "  Coefficient: $1,783.07\n",
      "  💰 Jobs in Providence-Warwick, RI-MA pay $1,783.07 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Portland-Vancouver-Hillsboro, OR-WA\n",
      "  Coefficient: $1,584.48\n",
      "  💰 Jobs in Portland-Vancouver-Hillsboro, OR-WA pay $1,584.48 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Atlanta-Sandy Springs-Alpharetta, GA\n",
      "  Coefficient: $1,349.10\n",
      "  💰 Jobs in Atlanta-Sandy Springs-Alpharetta, GA pay $1,349.10 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Boston-Cambridge-Newton, MA-NH\n",
      "  Coefficient: $1,336.87\n",
      "  💰 Jobs in Boston-Cambridge-Newton, MA-NH pay $1,336.87 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Kennewick-Richland, WA\n",
      "  Coefficient: $834.68\n",
      "  💰 Jobs in Kennewick-Richland, WA pay $834.68 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Jackson, MI\n",
      "  Coefficient: $751.28\n",
      "  💰 Jobs in Jackson, MI pay $751.28 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Dover, DE\n",
      "  Coefficient: $683.85\n",
      "  💰 Jobs in Dover, DE pay $683.85 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Fort Collins, CO\n",
      "  Coefficient: $620.13\n",
      "  💰 Jobs in Fort Collins, CO pay $620.13 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Huntsville, AL\n",
      "  Coefficient: $383.28\n",
      "  💰 Jobs in Huntsville, AL pay $383.28 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Barre, VT\n",
      "  Coefficient: $291.93\n",
      "  💰 Jobs in Barre, VT pay $291.93 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Dallas-Fort Worth-Arlington, TX\n",
      "  Coefficient: $263.21\n",
      "  💰 Jobs in Dallas-Fort Worth-Arlington, TX pay $263.21 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Tampa-St. Petersburg-Clearwater, FL\n",
      "  Coefficient: $145.40\n",
      "  💰 Jobs in Tampa-St. Petersburg-Clearwater, FL pay $145.40 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Des Moines-West Des Moines, IA\n",
      "  Coefficient: $128.06\n",
      "  💰 Jobs in Des Moines-West Des Moines, IA pay $128.06 MORE than York-Hanover, PA\n",
      "\n",
      "Location: Manchester-Nashua, NH\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Manchester-Nashua, NH pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Provo-Orem, UT\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Provo-Orem, UT pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Allentown-Bethlehem-Easton, PA-NJ\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Allentown-Bethlehem-Easton, PA-NJ pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Fresno, CA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Fresno, CA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Erie, PA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Erie, PA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Fort Wayne, IN\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Fort Wayne, IN pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Pueblo, CO\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Pueblo, CO pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Glenwood Springs, CO\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Glenwood Springs, CO pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Defiance, OH\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Defiance, OH pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: San Luis Obispo-Paso Robles, CA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in San Luis Obispo-Paso Robles, CA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Holland, MI\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Holland, MI pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Corpus Christi, TX\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Corpus Christi, TX pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Warner Robins, GA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Warner Robins, GA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Appleton, WI\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Appleton, WI pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Cedar Rapids, IA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Cedar Rapids, IA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Beckley, WV\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Beckley, WV pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Oshkosh-Neenah, WI\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Oshkosh-Neenah, WI pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Springfield, MO\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Springfield, MO pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Wilmington, OH\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Wilmington, OH pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Scranton--Wilkes-Barre, PA\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Scranton--Wilkes-Barre, PA pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Tiffin, OH\n",
      "  Coefficient: $0.00\n",
      "  💵 Jobs in Tiffin, OH pay $0.00 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Tallahassee, FL\n",
      "  Coefficient: $-1,139.49\n",
      "  💵 Jobs in Tallahassee, FL pay $1,139.49 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Coos Bay, OR\n",
      "  Coefficient: $-1,166.80\n",
      "  💵 Jobs in Coos Bay, OR pay $1,166.80 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Colorado Springs, CO\n",
      "  Coefficient: $-1,275.91\n",
      "  💵 Jobs in Colorado Springs, CO pay $1,275.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lansing-East Lansing, MI\n",
      "  Coefficient: $-1,349.35\n",
      "  💵 Jobs in Lansing-East Lansing, MI pay $1,349.35 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Milwaukee-Waukesha, WI\n",
      "  Coefficient: $-1,991.49\n",
      "  💵 Jobs in Milwaukee-Waukesha, WI pay $1,991.49 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Sacramento-Roseville-Folsom, CA\n",
      "  Coefficient: $-2,084.52\n",
      "  💵 Jobs in Sacramento-Roseville-Folsom, CA pay $2,084.52 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Charlotte-Concord-Gastonia, NC-SC\n",
      "  Coefficient: $-2,188.95\n",
      "  💵 Jobs in Charlotte-Concord-Gastonia, NC-SC pay $2,188.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Los Angeles-Long Beach-Anaheim, CA\n",
      "  Coefficient: $-2,216.38\n",
      "  💵 Jobs in Los Angeles-Long Beach-Anaheim, CA pay $2,216.38 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Baltimore-Columbia-Towson, MD\n",
      "  Coefficient: $-2,376.65\n",
      "  💵 Jobs in Baltimore-Columbia-Towson, MD pay $2,376.65 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Utica-Rome, NY\n",
      "  Coefficient: $-2,393.95\n",
      "  💵 Jobs in Utica-Rome, NY pay $2,393.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Rocky Mount, NC\n",
      "  Coefficient: $-2,609.17\n",
      "  💵 Jobs in Rocky Mount, NC pay $2,609.17 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Greenville-Anderson, SC\n",
      "  Coefficient: $-2,874.22\n",
      "  💵 Jobs in Greenville-Anderson, SC pay $2,874.22 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Harrisburg-Carlisle, PA\n",
      "  Coefficient: $-2,884.30\n",
      "  💵 Jobs in Harrisburg-Carlisle, PA pay $2,884.30 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Augusta-Waterville, ME\n",
      "  Coefficient: $-3,502.45\n",
      "  💵 Jobs in Augusta-Waterville, ME pay $3,502.45 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Frankfort, KY\n",
      "  Coefficient: $-3,705.28\n",
      "  💵 Jobs in Frankfort, KY pay $3,705.28 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Pittsburgh, PA\n",
      "  Coefficient: $-3,864.41\n",
      "  💵 Jobs in Pittsburgh, PA pay $3,864.41 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Helena, MT\n",
      "  Coefficient: $-4,042.05\n",
      "  💵 Jobs in Helena, MT pay $4,042.05 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Richmond, VA\n",
      "  Coefficient: $-4,073.13\n",
      "  💵 Jobs in Richmond, VA pay $4,073.13 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Miami-Fort Lauderdale-Pompano Beach, FL\n",
      "  Coefficient: $-4,119.62\n",
      "  💵 Jobs in Miami-Fort Lauderdale-Pompano Beach, FL pay $4,119.62 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Raleigh-Cary, NC\n",
      "  Coefficient: $-4,161.95\n",
      "  💵 Jobs in Raleigh-Cary, NC pay $4,161.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Madison, WI\n",
      "  Coefficient: $-4,280.77\n",
      "  💵 Jobs in Madison, WI pay $4,280.77 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Durham-Chapel Hill, NC\n",
      "  Coefficient: $-4,282.24\n",
      "  💵 Jobs in Durham-Chapel Hill, NC pay $4,282.24 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Denver-Aurora-Lakewood, CO\n",
      "  Coefficient: $-4,322.78\n",
      "  💵 Jobs in Denver-Aurora-Lakewood, CO pay $4,322.78 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Buffalo-Cheektowaga, NY\n",
      "  Coefficient: $-4,338.03\n",
      "  💵 Jobs in Buffalo-Cheektowaga, NY pay $4,338.03 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Savannah, GA\n",
      "  Coefficient: $-4,715.76\n",
      "  💵 Jobs in Savannah, GA pay $4,715.76 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Juneau, AK\n",
      "  Coefficient: $-4,797.85\n",
      "  💵 Jobs in Juneau, AK pay $4,797.85 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Billings, MT\n",
      "  Coefficient: $-4,968.98\n",
      "  💵 Jobs in Billings, MT pay $4,968.98 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Charleston, WV\n",
      "  Coefficient: $-5,137.38\n",
      "  💵 Jobs in Charleston, WV pay $5,137.38 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Santa Maria-Santa Barbara, CA\n",
      "  Coefficient: $-5,523.39\n",
      "  💵 Jobs in Santa Maria-Santa Barbara, CA pay $5,523.39 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Greeley, CO\n",
      "  Coefficient: $-5,628.26\n",
      "  💵 Jobs in Greeley, CO pay $5,628.26 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Winston-Salem, NC\n",
      "  Coefficient: $-5,990.38\n",
      "  💵 Jobs in Winston-Salem, NC pay $5,990.38 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Green Bay, WI\n",
      "  Coefficient: $-6,333.41\n",
      "  💵 Jobs in Green Bay, WI pay $6,333.41 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Jefferson City, MO\n",
      "  Coefficient: $-6,429.19\n",
      "  💵 Jobs in Jefferson City, MO pay $6,429.19 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Baton Rouge, LA\n",
      "  Coefficient: $-6,449.46\n",
      "  💵 Jobs in Baton Rouge, LA pay $6,449.46 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Urban Honolulu, HI\n",
      "  Coefficient: $-6,453.59\n",
      "  💵 Jobs in Urban Honolulu, HI pay $6,453.59 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Pierre, SD\n",
      "  Coefficient: $-6,706.52\n",
      "  💵 Jobs in Pierre, SD pay $6,706.52 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Stockton, CA\n",
      "  Coefficient: $-6,776.91\n",
      "  💵 Jobs in Stockton, CA pay $6,776.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Trenton-Princeton, NJ\n",
      "  Coefficient: $-6,881.46\n",
      "  💵 Jobs in Trenton-Princeton, NJ pay $6,881.46 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Albuquerque, NM\n",
      "  Coefficient: $-6,997.40\n",
      "  💵 Jobs in Albuquerque, NM pay $6,997.40 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Boise City, ID\n",
      "  Coefficient: $-7,105.35\n",
      "  💵 Jobs in Boise City, ID pay $7,105.35 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Santa Fe, NM\n",
      "  Coefficient: $-7,241.61\n",
      "  💵 Jobs in Santa Fe, NM pay $7,241.61 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Bismarck, ND\n",
      "  Coefficient: $-7,529.44\n",
      "  💵 Jobs in Bismarck, ND pay $7,529.44 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Oklahoma City, OK\n",
      "  Coefficient: $-7,614.61\n",
      "  💵 Jobs in Oklahoma City, OK pay $7,614.61 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Palm Bay-Melbourne-Titusville, FL\n",
      "  Coefficient: $-7,773.32\n",
      "  💵 Jobs in Palm Bay-Melbourne-Titusville, FL pay $7,773.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: St. Louis, MO-IL\n",
      "  Coefficient: $-7,811.37\n",
      "  💵 Jobs in St. Louis, MO-IL pay $7,811.37 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lincoln, NE\n",
      "  Coefficient: $-7,933.67\n",
      "  💵 Jobs in Lincoln, NE pay $7,933.67 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Philadelphia-Camden-Wilmington, PA-NJ-DE-MD\n",
      "  Coefficient: $-8,193.49\n",
      "  💵 Jobs in Philadelphia-Camden-Wilmington, PA-NJ-DE-MD pay $8,193.49 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Kansas City, MO-KS\n",
      "  Coefficient: $-8,380.75\n",
      "  💵 Jobs in Kansas City, MO-KS pay $8,380.75 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Greensboro-High Point, NC\n",
      "  Coefficient: $-8,570.48\n",
      "  💵 Jobs in Greensboro-High Point, NC pay $8,570.48 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Carson City, NV\n",
      "  Coefficient: $-8,630.43\n",
      "  💵 Jobs in Carson City, NV pay $8,630.43 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Boulder, CO\n",
      "  Coefficient: $-8,655.13\n",
      "  💵 Jobs in Boulder, CO pay $8,655.13 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Ogden-Clearfield, UT\n",
      "  Coefficient: $-8,862.12\n",
      "  💵 Jobs in Ogden-Clearfield, UT pay $8,862.12 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Albany-Schenectady-Troy, NY\n",
      "  Coefficient: $-9,149.27\n",
      "  💵 Jobs in Albany-Schenectady-Troy, NY pay $9,149.27 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Wheeling, WV-OH\n",
      "  Coefficient: $-9,155.53\n",
      "  💵 Jobs in Wheeling, WV-OH pay $9,155.53 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Indianapolis-Carmel-Anderson, IN\n",
      "  Coefficient: $-9,262.26\n",
      "  💵 Jobs in Indianapolis-Carmel-Anderson, IN pay $9,262.26 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Topeka, KS\n",
      "  Coefficient: $-9,538.29\n",
      "  💵 Jobs in Topeka, KS pay $9,538.29 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Oxnard-Thousand Oaks-Ventura, CA\n",
      "  Coefficient: $-9,558.66\n",
      "  💵 Jobs in Oxnard-Thousand Oaks-Ventura, CA pay $9,558.66 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Athens-Clarke County, GA\n",
      "  Coefficient: $-10,568.94\n",
      "  💵 Jobs in Athens-Clarke County, GA pay $10,568.94 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Gainesville, FL\n",
      "  Coefficient: $-10,611.32\n",
      "  💵 Jobs in Gainesville, FL pay $10,611.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Owensboro, KY\n",
      "  Coefficient: $-10,926.67\n",
      "  💵 Jobs in Owensboro, KY pay $10,926.67 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Cincinnati, OH-KY-IN\n",
      "  Coefficient: $-11,381.38\n",
      "  💵 Jobs in Cincinnati, OH-KY-IN pay $11,381.38 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Salt Lake City, UT\n",
      "  Coefficient: $-11,661.24\n",
      "  💵 Jobs in Salt Lake City, UT pay $11,661.24 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Knoxville, TN\n",
      "  Coefficient: $-11,694.52\n",
      "  💵 Jobs in Knoxville, TN pay $11,694.52 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Minneapolis-St. Paul-Bloomington, MN-WI\n",
      "  Coefficient: $-11,785.32\n",
      "  💵 Jobs in Minneapolis-St. Paul-Bloomington, MN-WI pay $11,785.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Cleveland-Elyria, OH\n",
      "  Coefficient: $-11,992.95\n",
      "  💵 Jobs in Cleveland-Elyria, OH pay $11,992.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Anchorage, AK\n",
      "  Coefficient: $-12,095.42\n",
      "  💵 Jobs in Anchorage, AK pay $12,095.42 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Detroit-Warren-Dearborn, MI\n",
      "  Coefficient: $-12,340.41\n",
      "  💵 Jobs in Detroit-Warren-Dearborn, MI pay $12,340.41 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Concord, NH\n",
      "  Coefficient: $-12,529.48\n",
      "  💵 Jobs in Concord, NH pay $12,529.48 LESS than York-Hanover, PA\n",
      "\n",
      "Location: San Antonio-New Braunfels, TX\n",
      "  Coefficient: $-12,699.28\n",
      "  💵 Jobs in San Antonio-New Braunfels, TX pay $12,699.28 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Napa, CA\n",
      "  Coefficient: $-13,248.72\n",
      "  💵 Jobs in Napa, CA pay $13,248.72 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lakeland-Winter Haven, FL\n",
      "  Coefficient: $-13,448.83\n",
      "  💵 Jobs in Lakeland-Winter Haven, FL pay $13,448.83 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Reading, PA\n",
      "  Coefficient: $-13,656.91\n",
      "  💵 Jobs in Reading, PA pay $13,656.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Nashville-Davidson--Murfreesboro--Franklin, TN\n",
      "  Coefficient: $-13,700.80\n",
      "  💵 Jobs in Nashville-Davidson--Murfreesboro--Franklin, TN pay $13,700.80 LESS than York-Hanover, PA\n",
      "\n",
      "Location: San Diego-Chula Vista-Carlsbad, CA\n",
      "  Coefficient: $-13,915.82\n",
      "  💵 Jobs in San Diego-Chula Vista-Carlsbad, CA pay $13,915.82 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Richmond-Berea, KY\n",
      "  Coefficient: $-14,587.79\n",
      "  💵 Jobs in Richmond-Berea, KY pay $14,587.79 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Bakersfield, CA\n",
      "  Coefficient: $-14,715.76\n",
      "  💵 Jobs in Bakersfield, CA pay $14,715.76 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Bloomington, IL\n",
      "  Coefficient: $-14,715.76\n",
      "  💵 Jobs in Bloomington, IL pay $14,715.76 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Birmingham-Hoover, AL\n",
      "  Coefficient: $-14,988.72\n",
      "  💵 Jobs in Birmingham-Hoover, AL pay $14,988.72 LESS than York-Hanover, PA\n",
      "\n",
      "Location: North Port-Sarasota-Bradenton, FL\n",
      "  Coefficient: $-15,055.32\n",
      "  💵 Jobs in North Port-Sarasota-Bradenton, FL pay $15,055.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Rochester, MN\n",
      "  Coefficient: $-15,833.11\n",
      "  💵 Jobs in Rochester, MN pay $15,833.11 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Binghamton, NY\n",
      "  Coefficient: $-15,865.16\n",
      "  💵 Jobs in Binghamton, NY pay $15,865.16 LESS than York-Hanover, PA\n",
      "\n",
      "Location: El Paso, TX\n",
      "  Coefficient: $-16,358.61\n",
      "  💵 Jobs in El Paso, TX pay $16,358.61 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Cheyenne, WY\n",
      "  Coefficient: $-16,438.22\n",
      "  💵 Jobs in Cheyenne, WY pay $16,438.22 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lebanon, NH-VT\n",
      "  Coefficient: $-17,248.72\n",
      "  💵 Jobs in Lebanon, NH-VT pay $17,248.72 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Peoria, IL\n",
      "  Coefficient: $-17,361.91\n",
      "  💵 Jobs in Peoria, IL pay $17,361.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Memphis, TN-MS-AR\n",
      "  Coefficient: $-17,597.30\n",
      "  💵 Jobs in Memphis, TN-MS-AR pay $17,597.30 LESS than York-Hanover, PA\n",
      "\n",
      "Location: College Station-Bryan, TX\n",
      "  Coefficient: $-17,768.72\n",
      "  💵 Jobs in College Station-Bryan, TX pay $17,768.72 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Grand Rapids-Kentwood, MI\n",
      "  Coefficient: $-17,852.95\n",
      "  💵 Jobs in Grand Rapids-Kentwood, MI pay $17,852.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Orlando-Kissimmee-Sanford, FL\n",
      "  Coefficient: $-18,033.01\n",
      "  💵 Jobs in Orlando-Kissimmee-Sanford, FL pay $18,033.01 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Sioux Falls, SD\n",
      "  Coefficient: $-18,276.79\n",
      "  💵 Jobs in Sioux Falls, SD pay $18,276.79 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Worcester, MA-CT\n",
      "  Coefficient: $-18,730.91\n",
      "  💵 Jobs in Worcester, MA-CT pay $18,730.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Gulfport-Biloxi, MS\n",
      "  Coefficient: $-19,027.32\n",
      "  💵 Jobs in Gulfport-Biloxi, MS pay $19,027.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Wichita, KS\n",
      "  Coefficient: $-19,264.93\n",
      "  💵 Jobs in Wichita, KS pay $19,264.93 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lancaster, PA\n",
      "  Coefficient: $-19,610.83\n",
      "  💵 Jobs in Lancaster, PA pay $19,610.83 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Ithaca, NY\n",
      "  Coefficient: $-19,658.93\n",
      "  💵 Jobs in Ithaca, NY pay $19,658.93 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Louisville/Jefferson County, KY-IN\n",
      "  Coefficient: $-19,909.22\n",
      "  💵 Jobs in Louisville/Jefferson County, KY-IN pay $19,909.22 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Ann Arbor, MI\n",
      "  Coefficient: $-19,945.33\n",
      "  💵 Jobs in Ann Arbor, MI pay $19,945.33 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Harrisonburg, VA\n",
      "  Coefficient: $-20,865.76\n",
      "  💵 Jobs in Harrisonburg, VA pay $20,865.76 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Jackson, MS\n",
      "  Coefficient: $-21,038.61\n",
      "  💵 Jobs in Jackson, MS pay $21,038.61 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Riverside-San Bernardino-Ontario, CA\n",
      "  Coefficient: $-21,044.79\n",
      "  💵 Jobs in Riverside-San Bernardino-Ontario, CA pay $21,044.79 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Brevard, NC\n",
      "  Coefficient: $-21,248.94\n",
      "  💵 Jobs in Brevard, NC pay $21,248.94 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Charleston-North Charleston, SC\n",
      "  Coefficient: $-21,363.23\n",
      "  💵 Jobs in Charleston-North Charleston, SC pay $21,363.23 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Las Vegas-Henderson-Paradise, NV\n",
      "  Coefficient: $-22,478.95\n",
      "  💵 Jobs in Las Vegas-Henderson-Paradise, NV pay $22,478.95 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Fargo, ND-MN\n",
      "  Coefficient: $-22,523.68\n",
      "  💵 Jobs in Fargo, ND-MN pay $22,523.68 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Lexington-Fayette, KY\n",
      "  Coefficient: $-22,954.84\n",
      "  💵 Jobs in Lexington-Fayette, KY pay $22,954.84 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Spokane-Spokane Valley, WA\n",
      "  Coefficient: $-24,034.06\n",
      "  💵 Jobs in Spokane-Spokane Valley, WA pay $24,034.06 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Columbia, MO\n",
      "  Coefficient: $-24,442.13\n",
      "  💵 Jobs in Columbia, MO pay $24,442.13 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Santa Cruz-Watsonville, CA\n",
      "  Coefficient: $-24,570.32\n",
      "  💵 Jobs in Santa Cruz-Watsonville, CA pay $24,570.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Elkhart-Goshen, IN\n",
      "  Coefficient: $-25,761.91\n",
      "  💵 Jobs in Elkhart-Goshen, IN pay $25,761.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Chattanooga, TN-GA\n",
      "  Coefficient: $-26,570.91\n",
      "  💵 Jobs in Chattanooga, TN-GA pay $26,570.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Davenport-Moline-Rock Island, IA-IL\n",
      "  Coefficient: $-26,983.49\n",
      "  💵 Jobs in Davenport-Moline-Rock Island, IA-IL pay $26,983.49 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Rochester, NY\n",
      "  Coefficient: $-27,288.24\n",
      "  💵 Jobs in Rochester, NY pay $27,288.24 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Manhattan, KS\n",
      "  Coefficient: $-28,217.13\n",
      "  💵 Jobs in Manhattan, KS pay $28,217.13 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Syracuse, NY\n",
      "  Coefficient: $-28,775.54\n",
      "  💵 Jobs in Syracuse, NY pay $28,775.54 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Tulsa, OK\n",
      "  Coefficient: $-31,748.73\n",
      "  💵 Jobs in Tulsa, OK pay $31,748.73 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Virginia Beach-Norfolk-Newport News, VA-NC\n",
      "  Coefficient: $-32,393.94\n",
      "  💵 Jobs in Virginia Beach-Norfolk-Newport News, VA-NC pay $32,393.94 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Ukiah, CA\n",
      "  Coefficient: $-34,614.32\n",
      "  💵 Jobs in Ukiah, CA pay $34,614.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Decatur, IL\n",
      "  Coefficient: $-34,942.13\n",
      "  💵 Jobs in Decatur, IL pay $34,942.13 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Flint, MI\n",
      "  Coefficient: $-36,135.65\n",
      "  💵 Jobs in Flint, MI pay $36,135.65 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Bremerton-Silverdale-Port Orchard, WA\n",
      "  Coefficient: $-39,604.32\n",
      "  💵 Jobs in Bremerton-Silverdale-Port Orchard, WA pay $39,604.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Santa Rosa-Petaluma, CA\n",
      "  Coefficient: $-39,828.91\n",
      "  💵 Jobs in Santa Rosa-Petaluma, CA pay $39,828.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: New Haven-Milford, CT\n",
      "  Coefficient: $-40,355.53\n",
      "  💵 Jobs in New Haven-Milford, CT pay $40,355.53 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Mount Pleasant, TX\n",
      "  Coefficient: $-41,741.91\n",
      "  💵 Jobs in Mount Pleasant, TX pay $41,741.91 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Huntington-Ashland, WV-KY-OH\n",
      "  Coefficient: $-46,499.32\n",
      "  💵 Jobs in Huntington-Ashland, WV-KY-OH pay $46,499.32 LESS than York-Hanover, PA\n",
      "\n",
      "Location: Weirton-Steubenville, WV-OH\n",
      "  Coefficient: $-70,795.99\n",
      "  💵 Jobs in Weirton-Steubenville, WV-OH pay $70,795.99 LESS than York-Hanover, PA\n",
      "\n",
      "======================================================================\n",
      "KEY FINDINGS:\n",
      "======================================================================\n",
      "\n",
      "🏆 HIGHEST PAYING LOCATION (relative to York-Hanover, PA):\n",
      "   Omaha-Council Bluffs, NE-IA\n",
      "   Premium: $105,048.00\n",
      "\n",
      "📉 LOWEST PAYING LOCATION (relative to York-Hanover, PA):\n",
      "   Weirton-Steubenville, WV-OH\n",
      "   Difference: $-70,795.99\n",
      "\n",
      "📊 LOCATION SALARY SPREAD:\n",
      "   Difference between highest and lowest paying locations: $175,843.99\n",
      "\n",
      "================================================================================\n",
      "PART 5: INTERPRETATION OF REMOTE WORK TYPE EFFECTS\n",
      "================================================================================\n",
      "\n",
      "⭐ Baseline/Reference Category: Hybrid\n",
      "   All coefficients below compare each work type to Hybrid\n",
      "\n",
      "======================================================================\n",
      "REMOTE WORK TYPE SALARY EFFECTS (sorted by salary impact)\n",
      "======================================================================\n",
      "\n",
      "Work Type: Onsite\n",
      "  Coefficient: $21,651.24\n",
      "  💰 Onsite positions pay $21,651.24 MORE than Hybrid\n",
      "\n",
      "Work Type: Remote\n",
      "  Coefficient: $-1,997.97\n",
      "  💵 Remote positions pay $1,997.97 LESS than Hybrid\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PART 6: OVERALL MODEL INSIGHTS AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "💡 BUSINESS INSIGHTS:\n",
      "======================================================================\n",
      "\n",
      "1. EXPERIENCE FACTORS:\n",
      "\n",
      "2. LOCATION FACTORS:\n",
      "   • Location matters! Different MSAs show varying salary levels\n",
      "   • Baseline location: York-Hanover, PA\n",
      "   • Location premium ranges from $-70,795.99 to $105,048.00\n",
      "\n",
      "3. REMOTE WORK FACTORS:\n",
      "   • Baseline work arrangement: Hybrid\n",
      "   • Remote work type affects salary differently\n",
      "   • Premium/discount ranges from $-1,997.97 to $21,651.24\n",
      "\n",
      "======================================================================\n",
      "⚠️  IMPORTANT NOTES:\n",
      "======================================================================\n",
      "\n",
      "1. INTERPRETATION OF CATEGORICAL VARIABLES:\n",
      "   • One-hot encoding with dropLast=True creates reference categories\n",
      "   • MSA baseline: York-Hanover, PA\n",
      "   • Remote Type baseline: Hybrid\n",
      "   • All coefficients are relative to these baselines\n",
      "\n",
      "2. COEFFICIENT INTERPRETATION:\n",
      "   • Positive coefficient = higher salary than baseline\n",
      "   • Negative coefficient = lower salary than baseline\n",
      "   • Magnitude shows the dollar amount difference\n",
      "\n",
      "3. STATISTICAL SIGNIFICANCE:\n",
      "   • P-value < 0.05 means the effect is unlikely due to chance\n",
      "   • Confidence intervals show the range of plausible values\n",
      "   • T-values measure how many standard errors the coefficient is from zero\n",
      "\n",
      "4. DATA LEAKAGE RESOLUTION:\n",
      "   • SALARY_FROM was excluded from features to prevent data leakage\n",
      "   • This ensures the model uses only information available before knowing salary\n",
      "   • Results now reflect realistic prediction scenarios\n",
      "\n",
      "================================================================================\n",
      "✓ DETAILED INTERPRETATION COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED INTERPRETATION OF GLM COEFFICIENTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED GENERALIZED LINEAR REGRESSION MODEL INTERPRETATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check if required variables exist\n",
    "required_vars = ['lr_model', 'coefficients', 'pipeline_model', 'df_clean']\n",
    "missing_vars = [var for var in required_vars if var not in dir()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ ERROR: Missing required variables: {missing_vars}\")\n",
    "    print(\"\\n⚠️  This code must be run AFTER the model training code.\")\n",
    "    print(\"   Please run the Linear Regression training code first, then run this interpretation code.\")\n",
    "    print(\"\\n   Required variables from training:\")\n",
    "    print(\"   - lr_model (trained model)\")\n",
    "    print(\"   - coefficients (model coefficients)\")\n",
    "    print(\"   - pipeline_model (fitted pipeline)\")\n",
    "    print(\"   - df_clean (cleaned dataframe)\")\n",
    "    print(\"   - stats_available (whether stats were calculated)\")\n",
    "    print(\"   - std_errors, t_values, p_values, ci_lower, ci_upper (if stats_available)\")\n",
    "    raise ValueError(\"Missing required variables from model training\")\n",
    "\n",
    "print(\"✓ All required variables found. Proceeding with interpretation...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: EXTRACT AND MAP FEATURE NAMES\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PART 1: FEATURE NAME MAPPING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get the actual categorical mappings from the StringIndexer models\n",
    "msa_model = pipeline_model.stages[0]  # msa_indexer\n",
    "remote_model = pipeline_model.stages[1]  # remote_indexer\n",
    "\n",
    "# Get the labels (original category names) in order of their indices\n",
    "msa_labels = msa_model.labels\n",
    "remote_labels = remote_model.labels\n",
    "\n",
    "print(f\"Number of MSA categories: {len(msa_labels)}\")\n",
    "print(f\"Number of Remote Type categories: {len(remote_labels)}\")\n",
    "\n",
    "# Create detailed feature names with actual category labels\n",
    "# OneHotEncoder with dropLast=True means we have n-1 features for n categories\n",
    "# The dropped category becomes the reference/baseline category\n",
    "\n",
    "feature_names_detailed = []\n",
    "\n",
    "# Numerical features\n",
    "feature_names_detailed.append('MIN_YEARS_EXPERIENCE')\n",
    "feature_names_detailed.append('MAX_YEARS_EXPERIENCE')\n",
    "\n",
    "# MSA (Metropolitan Statistical Area) - one-hot encoded\n",
    "# dropLast=True means the last category is the baseline/reference\n",
    "print(f\"\\nMSA Categories (Total: {len(msa_labels)}):\")\n",
    "for i, label in enumerate(msa_labels):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "baseline_msa = msa_labels[-1]  # Last one is dropped (baseline)\n",
    "print(f\"\\n⭐ Baseline MSA (reference category): {baseline_msa}\")\n",
    "print(f\"   All other MSA coefficients are relative to {baseline_msa}\\n\")\n",
    "\n",
    "for i in range(len(msa_labels) - 1):  # All except the last one\n",
    "    feature_names_detailed.append(f'MSA: {msa_labels[i]}')\n",
    "\n",
    "# Remote Type - one-hot encoded\n",
    "print(f\"Remote Type Categories (Total: {len(remote_labels)}):\")\n",
    "for i, label in enumerate(remote_labels):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "baseline_remote = remote_labels[-1]  # Last one is dropped (baseline)\n",
    "print(f\"\\n⭐ Baseline Remote Type (reference category): {baseline_remote}\")\n",
    "print(f\"   All other Remote Type coefficients are relative to {baseline_remote}\\n\")\n",
    "\n",
    "for i in range(len(remote_labels) - 1):  # All except the last one\n",
    "    feature_names_detailed.append(f'Remote: {remote_labels[i]}')\n",
    "\n",
    "print(f\"Total features in model: {len(feature_names_detailed)}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, name in enumerate(feature_names_detailed):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: CREATE COMPREHENSIVE COEFFICIENT TABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: COMPREHENSIVE COEFFICIENT ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Recreate the coefficient DataFrame with detailed names\n",
    "coef_data_detailed = []\n",
    "for i, name in enumerate(feature_names_detailed):\n",
    "    coef = float(coefficients[i])\n",
    "    row_data = {\n",
    "        'Feature': name,\n",
    "        'Coefficient': coef,\n",
    "        'Coef_Formatted': f'${coef:,.2f}'\n",
    "    }\n",
    "    \n",
    "    if stats_available:\n",
    "        se = float(std_errors[i])\n",
    "        t = float(t_values[i])\n",
    "        p = float(p_values[i])\n",
    "        ci_low = float(ci_lower[i])\n",
    "        ci_high = float(ci_upper[i])\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p < 0.001:\n",
    "            sig_level = '***'\n",
    "            sig_text = 'Highly Significant'\n",
    "        elif p < 0.01:\n",
    "            sig_level = '**'\n",
    "            sig_text = 'Very Significant'\n",
    "        elif p < 0.05:\n",
    "            sig_level = '*'\n",
    "            sig_text = 'Significant'\n",
    "        elif p < 0.10:\n",
    "            sig_level = '.'\n",
    "            sig_text = 'Marginally Significant'\n",
    "        else:\n",
    "            sig_level = ''\n",
    "            sig_text = 'Not Significant'\n",
    "        \n",
    "        row_data.update({\n",
    "            'Std_Error': se,\n",
    "            'T_Value': t,\n",
    "            'P_Value': p,\n",
    "            'CI_95_Lower': ci_low,\n",
    "            'CI_95_Upper': ci_high,\n",
    "            'Sig_Level': sig_level,\n",
    "            'Significance': sig_text\n",
    "        })\n",
    "    \n",
    "    coef_data_detailed.append(row_data)\n",
    "\n",
    "coef_df_detailed = pd.DataFrame(coef_data_detailed)\n",
    "\n",
    "# Display the full table\n",
    "print(\"=\"*80)\n",
    "print(\"FULL COEFFICIENT TABLE WITH DETAILED FEATURE NAMES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSignificance codes: '***' p<0.001, '**' p<0.01, '*' p<0.05, '.' p<0.10\\n\")\n",
    "print(coef_df_detailed.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: INTERPRET NUMERICAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: INTERPRETATION OF NUMERICAL FEATURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "numerical_features = coef_df_detailed[coef_df_detailed['Feature'].isin(['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE'])]\n",
    "\n",
    "for idx, row in numerical_features.iterrows():\n",
    "    feature = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Coefficient: ${coef:,.2f}\")\n",
    "    \n",
    "    if stats_available:\n",
    "        print(f\"Standard Error: ${row['Std_Error']:,.2f}\")\n",
    "        print(f\"T-Value: {row['T_Value']:.3f}\")\n",
    "        print(f\"P-Value: {row['P_Value']:.4f} {row['Sig_Level']}\")\n",
    "        print(f\"95% Confidence Interval: [${row['CI_95_Lower']:,.2f}, ${row['CI_95_Upper']:,.2f}]\")\n",
    "        print(f\"Significance: {row['Significance']}\")\n",
    "    \n",
    "    print(f\"\\n📊 INTERPRETATION:\")\n",
    "    if coef > 0:\n",
    "        print(f\"• For each additional year in {feature.replace('_', ' ').lower()},\")\n",
    "        print(f\"  the predicted salary INCREASES by ${abs(coef):,.2f}, holding all other factors constant.\")\n",
    "    else:\n",
    "        print(f\"• For each additional year in {feature.replace('_', ' ').lower()},\")\n",
    "        print(f\"  the predicted salary DECREASES by ${abs(coef):,.2f}, holding all other factors constant.\")\n",
    "    \n",
    "    if stats_available:\n",
    "        if row['P_Value'] < 0.05:\n",
    "            print(f\"• This effect is STATISTICALLY SIGNIFICANT (p = {row['P_Value']:.4f})\")\n",
    "            print(f\"• We can be 95% confident the true effect is between ${row['CI_95_Lower']:,.2f} and ${row['CI_95_Upper']:,.2f}\")\n",
    "        else:\n",
    "            print(f\"• This effect is NOT statistically significant (p = {row['P_Value']:.4f})\")\n",
    "            print(f\"• We cannot confidently say this feature affects salary\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: INTERPRET CATEGORICAL FEATURES (MSA)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: INTERPRETATION OF MSA (LOCATION) EFFECTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n⭐ Baseline/Reference Category: {baseline_msa}\")\n",
    "print(f\"   All coefficients below compare each MSA to {baseline_msa}\\n\")\n",
    "\n",
    "msa_features = coef_df_detailed[coef_df_detailed['Feature'].str.startswith('MSA:')]\n",
    "\n",
    "# Sort by coefficient value to see which locations pay most/least\n",
    "msa_features_sorted = msa_features.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MSA SALARY EFFECTS (sorted by salary impact)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for idx, row in msa_features_sorted.iterrows():\n",
    "    location = row['Feature'].replace('MSA: ', '')\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"  Coefficient: ${coef:,.2f}\")\n",
    "    \n",
    "    if stats_available:\n",
    "        print(f\"  P-Value: {row['P_Value']:.4f} {row['Sig_Level']} ({row['Significance']})\")\n",
    "    \n",
    "    if coef > 0:\n",
    "        print(f\"  💰 Jobs in {location} pay ${abs(coef):,.2f} MORE than {baseline_msa}\")\n",
    "    else:\n",
    "        print(f\"  💵 Jobs in {location} pay ${abs(coef):,.2f} LESS than {baseline_msa}\")\n",
    "    \n",
    "    if stats_available:\n",
    "        if row['P_Value'] < 0.05:\n",
    "            print(f\"  ✓ This difference IS statistically significant\")\n",
    "        else:\n",
    "            print(f\"  ✗ This difference is NOT statistically significant\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Identify top and bottom paying locations\n",
    "if len(msa_features_sorted) > 0:\n",
    "    top_location = msa_features_sorted.iloc[0]\n",
    "    bottom_location = msa_features_sorted.iloc[-1]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"KEY FINDINGS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n🏆 HIGHEST PAYING LOCATION (relative to {baseline_msa}):\")\n",
    "    print(f\"   {top_location['Feature'].replace('MSA: ', '')}\")\n",
    "    print(f\"   Premium: ${top_location['Coefficient']:,.2f}\")\n",
    "    if stats_available and top_location['P_Value'] < 0.05:\n",
    "        print(f\"   ✓ Statistically significant (p = {top_location['P_Value']:.4f})\")\n",
    "    \n",
    "    print(f\"\\n📉 LOWEST PAYING LOCATION (relative to {baseline_msa}):\")\n",
    "    print(f\"   {bottom_location['Feature'].replace('MSA: ', '')}\")\n",
    "    print(f\"   Difference: ${bottom_location['Coefficient']:,.2f}\")\n",
    "    if stats_available and bottom_location['P_Value'] < 0.05:\n",
    "        print(f\"   ✓ Statistically significant (p = {bottom_location['P_Value']:.4f})\")\n",
    "    \n",
    "    location_spread = top_location['Coefficient'] - bottom_location['Coefficient']\n",
    "    print(f\"\\n📊 LOCATION SALARY SPREAD:\")\n",
    "    print(f\"   Difference between highest and lowest paying locations: ${location_spread:,.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: INTERPRET CATEGORICAL FEATURES (REMOTE TYPE)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: INTERPRETATION OF REMOTE WORK TYPE EFFECTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n⭐ Baseline/Reference Category: {baseline_remote}\")\n",
    "print(f\"   All coefficients below compare each work type to {baseline_remote}\\n\")\n",
    "\n",
    "remote_features = coef_df_detailed[coef_df_detailed['Feature'].str.startswith('Remote:')]\n",
    "\n",
    "# Sort by coefficient value\n",
    "remote_features_sorted = remote_features.sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REMOTE WORK TYPE SALARY EFFECTS (sorted by salary impact)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for idx, row in remote_features_sorted.iterrows():\n",
    "    work_type = row['Feature'].replace('Remote: ', '')\n",
    "    coef = row['Coefficient']\n",
    "    \n",
    "    print(f\"Work Type: {work_type}\")\n",
    "    print(f\"  Coefficient: ${coef:,.2f}\")\n",
    "    \n",
    "    if stats_available:\n",
    "        print(f\"  P-Value: {row['P_Value']:.4f} {row['Sig_Level']} ({row['Significance']})\")\n",
    "    \n",
    "    if coef > 0:\n",
    "        print(f\"  💰 {work_type} positions pay ${abs(coef):,.2f} MORE than {baseline_remote}\")\n",
    "    else:\n",
    "        print(f\"  💵 {work_type} positions pay ${abs(coef):,.2f} LESS than {baseline_remote}\")\n",
    "    \n",
    "    if stats_available:\n",
    "        if row['P_Value'] < 0.05:\n",
    "            print(f\"  ✓ This difference IS statistically significant\")\n",
    "        else:\n",
    "            print(f\"  ✗ This difference is NOT statistically significant\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: OVERALL MODEL INSIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: OVERALL MODEL INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if stats_available:\n",
    "    sig_features = coef_df_detailed[coef_df_detailed['P_Value'] < 0.05]\n",
    "    highly_sig_features = coef_df_detailed[coef_df_detailed['P_Value'] < 0.001]\n",
    "    \n",
    "    print(\"📈 STATISTICAL SUMMARY:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total features in model: {len(coef_df_detailed)}\")\n",
    "    print(f\"Significant features (p < 0.05): {len(sig_features)} ({len(sig_features)/len(coef_df_detailed)*100:.1f}%)\")\n",
    "    print(f\"Highly significant features (p < 0.001): {len(highly_sig_features)} ({len(highly_sig_features)/len(coef_df_detailed)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n🎯 MOST INFLUENTIAL FEATURES:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get features with largest absolute coefficients that are significant\n",
    "    sig_features_abs = sig_features.copy()\n",
    "    sig_features_abs['Abs_Coefficient'] = sig_features_abs['Coefficient'].abs()\n",
    "    top_features = sig_features_abs.nlargest(5, 'Abs_Coefficient')\n",
    "    \n",
    "    print(\"\\nTop 5 most impactful significant features:\")\n",
    "    for i, (idx, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"\\n{i}. {row['Feature']}\")\n",
    "        print(f\"   Impact: ${row['Coefficient']:,.2f}\")\n",
    "        print(f\"   Significance: {row['Significance']} (p = {row['P_Value']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"💡 BUSINESS INSIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. EXPERIENCE FACTORS:\")\n",
    "if 'MIN_YEARS_EXPERIENCE' in coef_df_detailed['Feature'].values:\n",
    "    min_exp_coef = coef_df_detailed[coef_df_detailed['Feature'] == 'MIN_YEARS_EXPERIENCE']['Coefficient'].values[0]\n",
    "    if stats_available:\n",
    "        min_exp_p = coef_df_detailed[coef_df_detailed['Feature'] == 'MIN_YEARS_EXPERIENCE']['P_Value'].values[0]\n",
    "        if min_exp_p < 0.05:\n",
    "            print(f\"   • Minimum experience requirement significantly affects salary\")\n",
    "            print(f\"   • Each additional year adds ~${min_exp_coef:,.2f} to salary\")\n",
    "\n",
    "if 'MAX_YEARS_EXPERIENCE' in coef_df_detailed['Feature'].values:\n",
    "    max_exp_coef = coef_df_detailed[coef_df_detailed['Feature'] == 'MAX_YEARS_EXPERIENCE']['Coefficient'].values[0]\n",
    "    if stats_available:\n",
    "        max_exp_p = coef_df_detailed[coef_df_detailed['Feature'] == 'MAX_YEARS_EXPERIENCE']['P_Value'].values[0]\n",
    "        if max_exp_p < 0.05:\n",
    "            print(f\"   • Maximum experience requirement significantly affects salary\")\n",
    "            print(f\"   • Each additional year adds ~${max_exp_coef:,.2f} to salary\")\n",
    "\n",
    "print(\"\\n2. LOCATION FACTORS:\")\n",
    "print(f\"   • Location matters! Different MSAs show varying salary levels\")\n",
    "print(f\"   • Baseline location: {baseline_msa}\")\n",
    "if len(msa_features_sorted) > 0:\n",
    "    print(f\"   • Location premium ranges from ${msa_features_sorted['Coefficient'].min():,.2f} to ${msa_features_sorted['Coefficient'].max():,.2f}\")\n",
    "\n",
    "print(\"\\n3. REMOTE WORK FACTORS:\")\n",
    "print(f\"   • Baseline work arrangement: {baseline_remote}\")\n",
    "if len(remote_features_sorted) > 0:\n",
    "    print(f\"   • Remote work type affects salary differently\")\n",
    "    print(f\"   • Premium/discount ranges from ${remote_features_sorted['Coefficient'].min():,.2f} to ${remote_features_sorted['Coefficient'].max():,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"⚠️  IMPORTANT NOTES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. INTERPRETATION OF CATEGORICAL VARIABLES:\")\n",
    "print(\"   • One-hot encoding with dropLast=True creates reference categories\")\n",
    "print(f\"   • MSA baseline: {baseline_msa}\")\n",
    "print(f\"   • Remote Type baseline: {baseline_remote}\")\n",
    "print(\"   • All coefficients are relative to these baselines\")\n",
    "\n",
    "print(\"\\n2. COEFFICIENT INTERPRETATION:\")\n",
    "print(\"   • Positive coefficient = higher salary than baseline\")\n",
    "print(\"   • Negative coefficient = lower salary than baseline\")\n",
    "print(\"   • Magnitude shows the dollar amount difference\")\n",
    "\n",
    "print(\"\\n3. STATISTICAL SIGNIFICANCE:\")\n",
    "print(\"   • P-value < 0.05 means the effect is unlikely due to chance\")\n",
    "print(\"   • Confidence intervals show the range of plausible values\")\n",
    "print(\"   • T-values measure how many standard errors the coefficient is from zero\")\n",
    "\n",
    "print(\"\\n4. DATA LEAKAGE RESOLUTION:\")\n",
    "print(\"   • SALARY_FROM was excluded from features to prevent data leakage\")\n",
    "print(\"   • This ensures the model uses only information available before knowing salary\")\n",
    "print(\"   • Results now reflect realistic prediction scenarios\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ DETAILED INTERPRETATION COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa601667",
   "metadata": {},
   "source": [
    "# Polynoimal Linear Regression & Summary\n",
    "The following coefficients reveal that...\n",
    "- MIN_YEARS_EXPERIENCE_SQ coefficient = -$487.46 (negative and significant): This indicates that each additional year of experience adds progressively less value to salary. For example, going from 0 to 1 years adds more salary than going from 10 to 11 years\n",
    "-- MIN_YEARS_EXPERIENCE = +$6,677.47: the first year of experience adds $6,677.47\n",
    "- MAX years experience has the same coefficients, suggesting the same effects or possibly duplication in the data\n",
    "Location coefficients:\n",
    "- San Jose = +$33,059: Highest tech hub premium relative to baseline\n",
    "- San Francisco = +$15,768: Strong tech market premium\n",
    "- New York = +$12,077: Major metro premium\n",
    "- Austin = +$10,464: Emerging tech hub\n",
    "- No meaningul insights for work arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f473e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POLYNOMIAL LINEAR REGRESSION MODEL\n",
      "================================================================================\n",
      "\n",
      "⚠️  IDENTIFYING THE KEY ISSUES:\n",
      "================================================================================\n",
      "ISSUE 1: DATA LEAKAGE\n",
      "  • 'features_poly' includes SALARY_FROM\n",
      "  • SALARY_FROM is derived from the same job posting as SALARY (target)\n",
      "  • This creates unrealistic model performance\n",
      "\n",
      "ISSUE 2: MULTICOLLINEARITY\n",
      "  • Including both MIN_YEARS_EXPERIENCE and MIN_YEARS_EXPERIENCE_SQ\n",
      "  • High correlation between a variable and its square\n",
      "  • Can cause unstable coefficient estimates\n",
      "\n",
      "SOLUTION:\n",
      "  ✓ Remove SALARY_FROM to prevent data leakage\n",
      "  ✓ Keep polynomial term for legitimate non-linear relationship modeling\n",
      "  ✓ Multicollinearity with polynomial terms is ACCEPTABLE when modeling\n",
      "    non-linear relationships (this is standard practice)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 'features_poly_clean' column WITHOUT SALARY_FROM\n",
      "  Features included: ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', 'MAX_YEARS_EXPERIENCE', 'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
      "\n",
      "Training Polynomial Linear Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 23:15:18 WARN Instrumentation: [f6bb1389] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/08 23:15:23 WARN Instrumentation: [f6bb1389] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Polynomial model training completed!\n",
      "\n",
      "================================================================================\n",
      "POLYNOMIAL MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "=== SAMPLE PREDICTIONS (POLYNOMIAL MODEL) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "|SALARY|       prediction|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|            MSA_NAME|REMOTE_TYPE_NAME|\n",
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "| 49547|48293.28752570613|                   0|                   0|Riverside-San Ber...|          Onsite|\n",
      "| 41600|70945.63330280867|                   0|                   0|    Jacksonville, FL|          Onsite|\n",
      "| 66500| 69672.3751949212|                   0|                   0|Houston-The Woodl...|          Onsite|\n",
      "| 48880|63514.95910589581|                   0|                   0|Denver-Aurora-Lak...|          Onsite|\n",
      "| 50960|71940.14443706104|                   0|                   0|Chicago-Napervill...|          Onsite|\n",
      "| 61328|68048.23462625974|                   0|                   0|Dallas-Fort Worth...|          Onsite|\n",
      "| 48922|64574.17597306847|                   0|                   0|    Raleigh-Cary, NC|          Onsite|\n",
      "| 62400| 73590.6087689534|                   0|                   0|Boston-Cambridge-...|          Remote|\n",
      "| 62400|77046.61084601682|                   0|                   0|Chicago-Napervill...|          Remote|\n",
      "| 62400|70186.41182621836|                   0|                   0|Los Angeles-Long ...|          Remote|\n",
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "================================================================================\n",
      "POLYNOMIAL MODEL COEFFICIENTS AND STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Intercept: $69,523.64\n",
      "R² (R-squared): 0.4290\n",
      "RMSE (Root Mean Squared Error): $28,503.40\n",
      "MAE (Mean Absolute Error): $21,028.48\n",
      "\n",
      "================================================================================\n",
      "CALCULATING POLYNOMIAL MODEL COEFFICIENT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Extracting feature matrix and target values from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2,574 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2574, 217)\n",
      "Label vector shape: (2574,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residual Standard Error: $29,792.93\n",
      "Degrees of Freedom: 2356\n",
      "❌ Error calculating statistics: Singular matrix\n",
      "   This may happen with singular matrices or perfect multicollinearity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== POLYNOMIAL MODEL COEFFICIENT ANALYSIS TABLE (TOP 20 FEATURES) ===\n",
      "                Feature  Coefficient\n",
      "   MIN_YEARS_EXPERIENCE  6677.465980\n",
      "MIN_YEARS_EXPERIENCE_SQ  -487.464719\n",
      "   MAX_YEARS_EXPERIENCE  6677.465980\n",
      "                  MSA_0 12077.196478\n",
      "                  MSA_1  5748.537947\n",
      "                  MSA_2 -2379.658351\n",
      "                  MSA_3   588.630858\n",
      "                  MSA_4 15768.413900\n",
      "                  MSA_5  4480.540669\n",
      "                  MSA_6  1024.538592\n",
      "                  MSA_7 -3944.644662\n",
      "                  MSA_8 -8198.378398\n",
      "                  MSA_9 -1001.453955\n",
      "                 MSA_10  5793.802321\n",
      "                 MSA_11  4157.992914\n",
      "                 MSA_12 10463.939075\n",
      "                 MSA_13  1796.482977\n",
      "                 MSA_14 33058.888969\n",
      "                 MSA_15  2212.771427\n",
      "                 MSA_16 -1550.457717\n",
      "\n",
      "Note: Statistical tests not available\n",
      "\n",
      "================================================================================\n",
      "POLYNOMIAL MODEL INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "📊 POLYNOMIAL FEATURES INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIN_YEARS_EXPERIENCE:\n",
      "  • Coefficient: $6,677.47\n",
      "  • Linear effect on salary\n",
      "\n",
      "MIN_YEARS_EXPERIENCE_SQ:\n",
      "  • Coefficient: $-487.46\n",
      "  • This is the SQUARED term - captures non-linear relationship\n",
      "\n",
      "MAX_YEARS_EXPERIENCE:\n",
      "  • Coefficient: $6,677.47\n",
      "  • Linear effect on salary\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📈 POLYNOMIAL MODEL PERFORMANCE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. R² (R-squared) = 0.4290\n",
      "   • Interpretation: The polynomial model explains 42.90% of variance in salary\n",
      "   • Comparison to linear model: R² = 0.4205 (linear) vs 0.4290 (polynomial)\n",
      "   • Improvement: +0.85 percentage points\n",
      "   • Assessment: Minimal improvement - polynomial may not be necessary\n",
      "\n",
      "2. RMSE (Root Mean Squared Error) = $28,503.40\n",
      "   • Interpretation: Average prediction error is $28,503.40\n",
      "   • Comparison to linear model: $28,714.10 (linear) vs $28,503.40 (polynomial)\n",
      "   • Improvement: 0.73% reduction in error\n",
      "\n",
      "3. MAE (Mean Absolute Error) = $21,028.48\n",
      "   • Interpretation: Average absolute prediction error is $21,028.48\n",
      "   • Comparison to linear model: $21,077.77 (linear) vs $21,028.48 (polynomial)\n",
      "   • Improvement: 0.23% reduction in error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 TEST SET PERFORMANCE (POLYNOMIAL MODEL):\n",
      "--------------------------------------------------------------------------------\n",
      "Test R²: 0.3723\n",
      "Test RMSE: $26,723.22\n",
      "Test MAE: $21,256.43\n",
      "\n",
      "📊 TRAINING vs TEST COMPARISON (POLYNOMIAL MODEL):\n",
      "--------------------------------------------------------------------------------\n",
      "Training R²: 0.4290 | Test R²: 0.3723 | Difference: 0.0567\n",
      "✓ Good generalization - acceptable overfitting\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: LINEAR vs POLYNOMIAL\n",
      "================================================================================\n",
      "\n",
      "         Metric Linear Model Polynomial Model\n",
      "  R² (Training)       0.4205           0.4290\n",
      "      R² (Test)       0.3617           0.3723\n",
      "RMSE (Training)   $28,714.10       $28,503.40\n",
      "    RMSE (Test)   $26,948.87       $26,723.22\n",
      " MAE (Training)   $21,077.77       $21,028.48\n",
      "     MAE (Test)   $21,366.08       $21,256.43\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "💡 KEY INSIGHTS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. POLYNOMIAL TERM EFFECT:\n",
      "   • MIN_YEARS_EXPERIENCE_SQ coefficient: $-487.46\n",
      "   • Negative squared term indicates DIMINISHING returns to experience\n",
      "   • Each additional year of experience adds LESS value than the previous year\n",
      "\n",
      "2. MODEL SELECTION:\n",
      "   ≈ MODELS PERFORM SIMILARLY\n",
      "   • Consider LINEAR MODEL for simplicity\n",
      "   • Polynomial adds complexity without substantial benefit\n",
      "\n",
      "3. DATA LEAKAGE RESOLUTION:\n",
      "   ✓ SALARY_FROM excluded from both models\n",
      "   ✓ Models use only pre-salary information\n",
      "   ✓ Results reflect realistic prediction scenarios\n",
      "\n",
      "================================================================================\n",
      "✓ POLYNOMIAL MODEL TRAINING AND EVALUATION COMPLETED!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# POLYNOMIAL LINEAR REGRESSION MODEL TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POLYNOMIAL LINEAR REGRESSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL ISSUE RESOLUTION FOR POLYNOMIAL FEATURES:\n",
    "# The 'features_poly' column includes BOTH MIN_YEARS_EXPERIENCE_SQ AND SALARY_FROM\n",
    "# \n",
    "# TWO MAJOR PROBLEMS:\n",
    "# 1. DATA LEAKAGE: SALARY_FROM is part of the salary range (same as target SALARY)\n",
    "#    - This violates ML independence assumptions\n",
    "#    - Creates unrealistic model that can't be used for real predictions\n",
    "# \n",
    "# 2. PERFECT MULTICOLLINEARITY: MIN_YEARS_EXPERIENCE and MIN_YEARS_EXPERIENCE_SQ\n",
    "#    - These are perfectly correlated (one is just the square of the other)\n",
    "#    - Can cause numerical instability in coefficient estimation\n",
    "#    - Makes interpretation difficult\n",
    "#\n",
    "# SOLUTION: Create new polynomial features WITHOUT SALARY_FROM\n",
    "\n",
    "print(\"\\n⚠️  IDENTIFYING THE KEY ISSUES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"ISSUE 1: DATA LEAKAGE\")\n",
    "print(\"  • 'features_poly' includes SALARY_FROM\")\n",
    "print(\"  • SALARY_FROM is derived from the same job posting as SALARY (target)\")\n",
    "print(\"  • This creates unrealistic model performance\\n\")\n",
    "\n",
    "print(\"ISSUE 2: MULTICOLLINEARITY\")\n",
    "print(\"  • Including both MIN_YEARS_EXPERIENCE and MIN_YEARS_EXPERIENCE_SQ\")\n",
    "print(\"  • High correlation between a variable and its square\")\n",
    "print(\"  • Can cause unstable coefficient estimates\\n\")\n",
    "\n",
    "print(\"SOLUTION:\")\n",
    "print(\"  ✓ Remove SALARY_FROM to prevent data leakage\")\n",
    "print(\"  ✓ Keep polynomial term for legitimate non-linear relationship modeling\")\n",
    "print(\"  ✓ Multicollinearity with polynomial terms is ACCEPTABLE when modeling\")\n",
    "print(\"    non-linear relationships (this is standard practice)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create polynomial features WITHOUT SALARY_FROM\n",
    "poly_feature_cols_clean = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ',\n",
    "                           'MAX_YEARS_EXPERIENCE',\n",
    "                           'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "poly_assembler_clean = VectorAssembler(inputCols=poly_feature_cols_clean, \n",
    "                                       outputCol='features_poly_clean',\n",
    "                                       handleInvalid='keep')\n",
    "\n",
    "# Transform data with clean polynomial features\n",
    "df_train_poly = poly_assembler_clean.transform(train_data)\n",
    "df_test_poly = poly_assembler_clean.transform(test_data)\n",
    "\n",
    "print(\"✓ Created 'features_poly_clean' column WITHOUT SALARY_FROM\")\n",
    "print(f\"  Features included: {poly_feature_cols_clean}\\n\")\n",
    "\n",
    "# Initialize Polynomial Linear Regression model\n",
    "lr_poly = LinearRegression(\n",
    "    featuresCol='features_poly_clean',\n",
    "    labelCol='SALARY',\n",
    "    maxIter=100,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Polynomial Linear Regression model...\")\n",
    "lr_poly_model = lr_poly.fit(df_train_poly)\n",
    "print(\"✓ Polynomial model training completed!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"POLYNOMIAL MODEL EVALUATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions_poly = lr_poly_model.transform(df_test_poly)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"=== SAMPLE PREDICTIONS (POLYNOMIAL MODEL) ===\")\n",
    "predictions_poly.select('SALARY', 'prediction', 'MIN_YEARS_EXPERIENCE', \n",
    "                        'MAX_YEARS_EXPERIENCE', 'MSA_NAME', 'REMOTE_TYPE_NAME').show(10)\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT MODEL COEFFICIENTS AND CALCULATE STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POLYNOMIAL MODEL COEFFICIENTS AND STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get model summary\n",
    "summary_poly = lr_poly_model.summary\n",
    "\n",
    "# Extract basic metrics\n",
    "intercept_poly = lr_poly_model.intercept\n",
    "coefficients_poly = lr_poly_model.coefficients\n",
    "r2_poly = summary_poly.r2\n",
    "rmse_poly = summary_poly.rootMeanSquaredError\n",
    "mae_poly = summary_poly.meanAbsoluteError\n",
    "\n",
    "print(f\"Intercept: ${intercept_poly:,.2f}\")\n",
    "print(f\"R² (R-squared): {r2_poly:.4f}\")\n",
    "print(f\"RMSE (Root Mean Squared Error): ${rmse_poly:,.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error): ${mae_poly:,.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MANUAL CALCULATION OF COEFFICIENT STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING POLYNOMIAL MODEL COEFFICIENT STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Extracting feature matrix and target values from training data...\")\n",
    "\n",
    "# Collect training data for manual statistics calculation\n",
    "train_count_poly = df_train_poly.count()\n",
    "print(f\"Training set size: {train_count_poly:,} rows\")\n",
    "\n",
    "if train_count_poly > 100000:\n",
    "    print(\"⚠️  Warning: Large dataset. Manual statistics calculation may be slow.\")\n",
    "    print(\"   Consider using a sample for coefficient statistics.\\n\")\n",
    "\n",
    "# Extract features and labels\n",
    "train_features_poly = np.array(df_train_poly.select('features_poly_clean').rdd.map(lambda row: row[0].toArray()).collect())\n",
    "train_labels_poly = np.array(df_train_poly.select('SALARY').rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "print(f\"Feature matrix shape: {train_features_poly.shape}\")\n",
    "print(f\"Label vector shape: {train_labels_poly.shape}\")\n",
    "\n",
    "# Get predictions on training data for residuals\n",
    "train_predictions_poly = lr_poly_model.transform(df_train_poly)\n",
    "train_pred_values_poly = np.array(train_predictions_poly.select('prediction').rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_poly = train_labels_poly - train_pred_values_poly\n",
    "n_poly = len(train_labels_poly)\n",
    "k_poly = train_features_poly.shape[1]  # number of features\n",
    "df_residual_poly = n_poly - k_poly - 1  # degrees of freedom\n",
    "\n",
    "# Calculate residual standard error\n",
    "rse_poly = np.sqrt(np.sum(residuals_poly**2) / df_residual_poly)\n",
    "\n",
    "print(f\"\\nResidual Standard Error: ${rse_poly:,.2f}\")\n",
    "print(f\"Degrees of Freedom: {df_residual_poly}\")\n",
    "\n",
    "# Calculate variance-covariance matrix\n",
    "try:\n",
    "    X_poly = train_features_poly\n",
    "    XtX_poly = np.dot(X_poly.T, X_poly)\n",
    "    XtX_inv_poly = np.linalg.inv(XtX_poly)\n",
    "    \n",
    "    # Variance-covariance matrix\n",
    "    var_covar_matrix_poly = (rse_poly**2) * XtX_inv_poly\n",
    "    \n",
    "    # Standard errors are square roots of diagonal elements\n",
    "    std_errors_poly = np.sqrt(np.diag(var_covar_matrix_poly))\n",
    "    \n",
    "    # Calculate t-values\n",
    "    coef_array_poly = np.array(coefficients_poly.toArray())\n",
    "    t_values_poly = coef_array_poly / std_errors_poly\n",
    "    \n",
    "    # Calculate p-values (two-tailed test)\n",
    "    p_values_poly = 2 * (1 - scipy_stats.t.cdf(np.abs(t_values_poly), df_residual_poly))\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    t_critical_poly = scipy_stats.t.ppf(0.975, df_residual_poly)\n",
    "    ci_lower_poly = coef_array_poly - t_critical_poly * std_errors_poly\n",
    "    ci_upper_poly = coef_array_poly + t_critical_poly * std_errors_poly\n",
    "    \n",
    "    stats_available_poly = True\n",
    "    print(\"✓ Coefficient statistics calculated successfully!\\n\")\n",
    "    \n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"❌ Error calculating statistics: {e}\")\n",
    "    print(\"   This may happen with singular matrices or perfect multicollinearity.\\n\")\n",
    "    stats_available_poly = False\n",
    "    std_errors_poly = [None] * len(coefficients_poly)\n",
    "    t_values_poly = [None] * len(coefficients_poly)\n",
    "    p_values_poly = [None] * len(coefficients_poly)\n",
    "    ci_lower_poly = [None] * len(coefficients_poly)\n",
    "    ci_upper_poly = [None] * len(coefficients_poly)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE COEFFICIENT TABLE WITH FEATURE NAMES\n",
    "# ============================================================================\n",
    "\n",
    "# Create feature names for polynomial model\n",
    "num_msa_categories = df_clean.select('MSA_NAME').distinct().count() - 1\n",
    "num_remote_categories = df_clean.select('REMOTE_TYPE_NAME').distinct().count() - 1\n",
    "\n",
    "feature_names_poly = ['MIN_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE_SQ', 'MAX_YEARS_EXPERIENCE']\n",
    "feature_names_poly += [f'MSA_{i}' for i in range(num_msa_categories)]\n",
    "feature_names_poly += [f'REMOTE_{i}' for i in range(num_remote_categories)]\n",
    "\n",
    "# Create DataFrame for coefficient analysis\n",
    "coef_data_poly = []\n",
    "for i, (name, coef) in enumerate(zip(feature_names_poly, coefficients_poly)):\n",
    "    row_data = {\n",
    "        'Feature': name,\n",
    "        'Coefficient': float(coef)\n",
    "    }\n",
    "    \n",
    "    if stats_available_poly:\n",
    "        row_data.update({\n",
    "            'Std_Error': float(std_errors_poly[i]),\n",
    "            'T_Value': float(t_values_poly[i]),\n",
    "            'P_Value': float(p_values_poly[i]),\n",
    "            'CI_Lower': float(ci_lower_poly[i]),\n",
    "            'CI_Upper': float(ci_upper_poly[i]),\n",
    "            'Significant': '***' if p_values_poly[i] < 0.001 else '**' if p_values_poly[i] < 0.01 else '*' if p_values_poly[i] < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    coef_data_poly.append(row_data)\n",
    "\n",
    "# Convert to Pandas for better display\n",
    "coef_df_poly = pd.DataFrame(coef_data_poly)\n",
    "\n",
    "print(\"\\n=== POLYNOMIAL MODEL COEFFICIENT ANALYSIS TABLE (TOP 20 FEATURES) ===\")\n",
    "if stats_available_poly:\n",
    "    print(coef_df_poly.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(coef_df_poly.head(20).to_string(index=False))\n",
    "    print(\"\\nNote: Statistical tests not available\")\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRET RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POLYNOMIAL MODEL INTERPRETATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"📊 POLYNOMIAL FEATURES INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if stats_available_poly:\n",
    "    # Interpret the first 3 numerical features (including polynomial term)\n",
    "    for i in range(min(3, len(coef_df_poly))):\n",
    "        row = coef_df_poly.iloc[i]\n",
    "        name = row['Feature']\n",
    "        coef = row['Coefficient']\n",
    "        p_val = row['P_Value']\n",
    "        sig = row['Significant']\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  • Coefficient: ${coef:,.2f} {sig}\")\n",
    "        \n",
    "        if 'SQ' in name:\n",
    "            print(f\"  • Interpretation: This is the SQUARED term for MIN_YEARS_EXPERIENCE\")\n",
    "            if coef > 0:\n",
    "                print(f\"  • Effect: Creates an ACCELERATING (convex) relationship\")\n",
    "                print(f\"  • Meaning: Each additional year of experience has INCREASING marginal value\")\n",
    "            else:\n",
    "                print(f\"  • Effect: Creates a DECELERATING (concave) relationship\")\n",
    "                print(f\"  • Meaning: Each additional year of experience has DECREASING marginal value\")\n",
    "        else:\n",
    "            print(f\"  • Interpretation: Linear effect on salary\")\n",
    "            print(f\"    Each additional year {'increases' if coef > 0 else 'decreases'} salary by ${abs(coef):,.2f}\")\n",
    "        \n",
    "        print(f\"  • Statistical Significance: {sig} (p={p_val:.4f})\")\n",
    "        print(f\"  • 95% CI: [${row['CI_Lower']:,.2f}, ${row['CI_Upper']:,.2f}]\")\n",
    "else:\n",
    "    for i in range(min(3, len(coef_df_poly))):\n",
    "        row = coef_df_poly.iloc[i]\n",
    "        name = row['Feature']\n",
    "        coef = row['Coefficient']\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  • Coefficient: ${coef:,.2f}\")\n",
    "        \n",
    "        if 'SQ' in name:\n",
    "            print(f\"  • This is the SQUARED term - captures non-linear relationship\")\n",
    "        else:\n",
    "            print(f\"  • Linear effect on salary\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n📈 POLYNOMIAL MODEL PERFORMANCE METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n1. R² (R-squared) = {r2_poly:.4f}\")\n",
    "print(f\"   • Interpretation: The polynomial model explains {r2_poly*100:.2f}% of variance in salary\")\n",
    "print(f\"   • Comparison to linear model: R² = {r2:.4f} (linear) vs {r2_poly:.4f} (polynomial)\")\n",
    "r2_improvement = (r2_poly - r2) * 100\n",
    "if r2_improvement > 0:\n",
    "    print(f\"   • Improvement: +{r2_improvement:.2f} percentage points\")\n",
    "    if r2_improvement > 2:\n",
    "        print(f\"   • Assessment: Polynomial terms provide MEANINGFUL improvement\")\n",
    "    else:\n",
    "        print(f\"   • Assessment: Minimal improvement - polynomial may not be necessary\")\n",
    "else:\n",
    "    print(f\"   • Assessment: Polynomial model performs WORSE - overfitting likely\")\n",
    "\n",
    "print(f\"\\n2. RMSE (Root Mean Squared Error) = ${rmse_poly:,.2f}\")\n",
    "print(f\"   • Interpretation: Average prediction error is ${rmse_poly:,.2f}\")\n",
    "print(f\"   • Comparison to linear model: ${rmse:,.2f} (linear) vs ${rmse_poly:,.2f} (polynomial)\")\n",
    "rmse_improvement = ((rmse - rmse_poly) / rmse) * 100\n",
    "if rmse_improvement > 0:\n",
    "    print(f\"   • Improvement: {rmse_improvement:.2f}% reduction in error\")\n",
    "else:\n",
    "    print(f\"   • Degradation: {abs(rmse_improvement):.2f}% increase in error\")\n",
    "\n",
    "print(f\"\\n3. MAE (Mean Absolute Error) = ${mae_poly:,.2f}\")\n",
    "print(f\"   • Interpretation: Average absolute prediction error is ${mae_poly:,.2f}\")\n",
    "print(f\"   • Comparison to linear model: ${mae:,.2f} (linear) vs ${mae_poly:,.2f} (polynomial)\")\n",
    "mae_improvement = ((mae - mae_poly) / mae) * 100\n",
    "if mae_improvement > 0:\n",
    "    print(f\"   • Improvement: {mae_improvement:.2f}% reduction in error\")\n",
    "else:\n",
    "    print(f\"   • Degradation: {abs(mae_improvement):.2f}% increase in error\")\n",
    "\n",
    "# Calculate test set performance\n",
    "evaluator_r2_poly = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_rmse_poly = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae_poly = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "test_r2_poly = evaluator_r2_poly.evaluate(predictions_poly)\n",
    "test_rmse_poly = evaluator_rmse_poly.evaluate(predictions_poly)\n",
    "test_mae_poly = evaluator_mae_poly.evaluate(predictions_poly)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n🎯 TEST SET PERFORMANCE (POLYNOMIAL MODEL):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Test R²: {test_r2_poly:.4f}\")\n",
    "print(f\"Test RMSE: ${test_rmse_poly:,.2f}\")\n",
    "print(f\"Test MAE: ${test_mae_poly:,.2f}\")\n",
    "\n",
    "# Compare training vs test performance\n",
    "print(\"\\n📊 TRAINING vs TEST COMPARISON (POLYNOMIAL MODEL):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training R²: {r2_poly:.4f} | Test R²: {test_r2_poly:.4f} | Difference: {abs(r2_poly-test_r2_poly):.4f}\")\n",
    "if abs(r2_poly - test_r2_poly) < 0.05:\n",
    "    print(\"✓ Excellent generalization - minimal overfitting\")\n",
    "elif abs(r2_poly - test_r2_poly) < 0.10:\n",
    "    print(\"✓ Good generalization - acceptable overfitting\")\n",
    "elif abs(r2_poly - test_r2_poly) < 0.15:\n",
    "    print(\"⚠ Moderate overfitting detected - consider regularization\")\n",
    "else:\n",
    "    print(\"❌ Significant overfitting - polynomial model may not generalize well\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON: LINEAR vs POLYNOMIAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: LINEAR vs POLYNOMIAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['R² (Training)', 'R² (Test)', 'RMSE (Training)', 'RMSE (Test)', \n",
    "               'MAE (Training)', 'MAE (Test)'],\n",
    "    'Linear Model': [f'{r2:.4f}', f'{test_r2:.4f}', f'${rmse:,.2f}', f'${test_rmse:,.2f}',\n",
    "                     f'${mae:,.2f}', f'${test_mae:,.2f}'],\n",
    "    'Polynomial Model': [f'{r2_poly:.4f}', f'{test_r2_poly:.4f}', f'${rmse_poly:,.2f}', \n",
    "                         f'${test_rmse_poly:,.2f}', f'${mae_poly:,.2f}', f'${test_mae_poly:,.2f}']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "if stats_available_poly:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"\\n🔍 STATISTICAL INSIGHTS (POLYNOMIAL MODEL):\")\n",
    "    print(\"-\" * 80)\n",
    "    sig_features_poly = coef_df_poly[coef_df_poly['Significant'] != 'No'] if 'Significant' in coef_df_poly.columns else pd.DataFrame()\n",
    "    if len(sig_features_poly) > 0:\n",
    "        print(f\"Number of significant features (p < 0.05): {len(sig_features_poly)}\")\n",
    "        print(f\"Total features: {len(coef_df_poly)}\")\n",
    "        print(f\"Percentage significant: {len(sig_features_poly)/len(coef_df_poly)*100:.1f}%\")\n",
    "    \n",
    "    adj_r2_poly = 1 - (1-r2_poly)*(n_poly-1)/(n_poly-k_poly-1)\n",
    "    print(f\"\\nAdjusted R² (Polynomial): {adj_r2_poly:.4f}\")\n",
    "    print(f\"  • Accounts for number of predictors\")\n",
    "    print(f\"  • Penalizes model complexity\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. POLYNOMIAL TERM EFFECT:\")\n",
    "print(f\"   • MIN_YEARS_EXPERIENCE_SQ coefficient: ${coef_df_poly.iloc[1]['Coefficient']:,.2f}\")\n",
    "if coef_df_poly.iloc[1]['Coefficient'] > 0:\n",
    "    print(f\"   • Positive squared term indicates ACCELERATING returns to experience\")\n",
    "    print(f\"   • Each additional year of experience adds MORE value than the previous year\")\n",
    "else:\n",
    "    print(f\"   • Negative squared term indicates DIMINISHING returns to experience\")\n",
    "    print(f\"   • Each additional year of experience adds LESS value than the previous year\")\n",
    "\n",
    "print(\"\\n2. MODEL SELECTION:\")\n",
    "test_r2_diff = test_r2_poly - test_r2\n",
    "if test_r2_diff > 0.02:\n",
    "    print(f\"   ✓ POLYNOMIAL MODEL RECOMMENDED\")\n",
    "    print(f\"   • Test R² improved by {test_r2_diff:.4f}\")\n",
    "    print(f\"   • Better captures non-linear relationships\")\n",
    "elif test_r2_diff > -0.01:\n",
    "    print(f\"   ≈ MODELS PERFORM SIMILARLY\")\n",
    "    print(f\"   • Consider LINEAR MODEL for simplicity\")\n",
    "    print(f\"   • Polynomial adds complexity without substantial benefit\")\n",
    "else:\n",
    "    print(f\"   ✓ LINEAR MODEL RECOMMENDED\")\n",
    "    print(f\"   • Polynomial model shows overfitting\")\n",
    "    print(f\"   • Simpler linear model generalizes better\")\n",
    "\n",
    "print(\"\\n3. DATA LEAKAGE RESOLUTION:\")\n",
    "print(\"   ✓ SALARY_FROM excluded from both models\")\n",
    "print(\"   ✓ Models use only pre-salary information\")\n",
    "print(\"   ✓ Results reflect realistic prediction scenarios\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ POLYNOMIAL MODEL TRAINING AND EVALUATION COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a9e9c",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49df2f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RANDOM FOREST REGRESSOR MODEL\n",
      "================================================================================\n",
      "\n",
      "⚠️  DATA LEAKAGE ISSUE:\n",
      "================================================================================\n",
      "  • Original 'features' column includes SALARY_FROM\n",
      "  • SALARY_FROM is part of the same salary range as target (SALARY)\n",
      "  • Using 'features_clean' instead (without SALARY_FROM)\n",
      "================================================================================\n",
      "\n",
      "✓ Using 'features_clean' column WITHOUT SALARY_FROM\n",
      "  Features included: ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER SELECTION\n",
      "================================================================================\n",
      "\n",
      "📊 HYPERPARAMETER CHOICES:\n",
      "--------------------------------------------------------------------------------\n",
      "Number of Trees (numTrees): 200\n",
      "  • Range: 100-500 trees\n",
      "  • Rationale: 200 trees provides good ensemble diversity\n",
      "  • More trees = more stable predictions, longer training time\n",
      "  • Diminishing returns typically after 200-300 trees\n",
      "\n",
      "Maximum Depth (maxDepth): 7\n",
      "  • Range: 4-10 levels\n",
      "  • Rationale: Depth 7 balances complexity and generalization\n",
      "  • Deeper trees = more complex patterns, higher overfitting risk\n",
      "  • Shallower trees = simpler patterns, potential underfitting\n",
      "\n",
      "🔄 INVERSE RELATIONSHIP:\n",
      "--------------------------------------------------------------------------------\n",
      "  • High trees (400-500) → Use shallow depth (4-5)\n",
      "  •   → Many simple trees average out noise\n",
      "  • Low trees (100-150) → Use deeper depth (8-10)\n",
      "  •   → Fewer trees need more complexity each\n",
      "  • BALANCED APPROACH (chosen): 200 trees × depth 7\n",
      "  •   → Moderate ensemble with moderate complexity per tree\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING RANDOM FOREST MODEL\n",
      "================================================================================\n",
      "\n",
      "Training Random Forest with 200 trees and max depth 7...\n",
      "This may take a few moments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 23:16:46 WARN DAGScheduler: Broadcasting large task binary with size 1456.8 KiB\n",
      "25/10/08 23:16:47 WARN DAGScheduler: Broadcasting large task binary with size 2040.7 KiB\n",
      "25/10/08 23:16:49 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random Forest model training completed!\n",
      "\n",
      "================================================================================\n",
      "RANDOM FOREST MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "=== SAMPLE PREDICTIONS (RANDOM FOREST) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "|SALARY|       prediction|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|            MSA_NAME|REMOTE_TYPE_NAME|\n",
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "| 49547|77453.18977253485|                   0|                   0|Riverside-San Ber...|          Onsite|\n",
      "| 41600|97816.58825942391|                   0|                   0|    Jacksonville, FL|          Onsite|\n",
      "| 66500|78277.45251293929|                   0|                   0|Houston-The Woodl...|          Onsite|\n",
      "| 48880| 77626.2387262881|                   0|                   0|Denver-Aurora-Lak...|          Onsite|\n",
      "| 50960|78204.00909099524|                   0|                   0|Chicago-Napervill...|          Onsite|\n",
      "| 61328| 78276.6119052895|                   0|                   0|Dallas-Fort Worth...|          Onsite|\n",
      "| 48922|83616.58316176658|                   0|                   0|    Raleigh-Cary, NC|          Onsite|\n",
      "| 62400|81171.54541119808|                   0|                   0|Boston-Cambridge-...|          Remote|\n",
      "| 62400|81148.92021296493|                   0|                   0|Chicago-Napervill...|          Remote|\n",
      "| 62400|81603.03087474761|                   0|                   0|Los Angeles-Long ...|          Remote|\n",
      "+------+-----------------+--------------------+--------------------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 TRAINING SET PERFORMANCE:\n",
      "--------------------------------------------------------------------------------\n",
      "R² (R-squared): 0.4498\n",
      "  • Interpretation: Model explains 44.98% of variance on training data\n",
      "RMSE: $27,979.78\n",
      "  • Average prediction error on training data\n",
      "MAE: $21,139.79\n",
      "  • Average absolute error on training data\n",
      "\n",
      "🎯 TEST SET PERFORMANCE:\n",
      "--------------------------------------------------------------------------------\n",
      "R² (R-squared): 0.3851\n",
      "  • Interpretation: Model explains 38.51% of variance on test data\n",
      "RMSE: $26,450.42\n",
      "  • Average prediction error on test data\n",
      "MAE: $21,263.83\n",
      "  • Average absolute error on test data\n",
      "\n",
      "📈 OVERFITTING ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "Training R²: 0.4498\n",
      "Test R²: 0.3851\n",
      "Difference: 0.0647\n",
      "✓ Good generalization - acceptable overfitting\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP 20 MOST IMPORTANT FEATURES ===\n",
      "             Feature  Importance  Importance_Pct\n",
      "MIN_YEARS_EXPERIENCE    0.468651       46.865076\n",
      "MAX_YEARS_EXPERIENCE    0.350747       35.074705\n",
      "               MSA_0    0.026873        2.687331\n",
      "              MSA_14    0.024411        2.441104\n",
      "             MSA_118    0.013282        1.328240\n",
      "               MSA_4    0.010402        1.040214\n",
      "              MSA_10    0.010201        1.020104\n",
      "               MSA_5    0.009260        0.926041\n",
      "              MSA_12    0.007114        0.711409\n",
      "            REMOTE_1    0.005811        0.581069\n",
      "              MSA_17    0.005579        0.557850\n",
      "               MSA_2    0.004436        0.443565\n",
      "              MSA_69    0.003242        0.324175\n",
      "               MSA_1    0.003085        0.308522\n",
      "              MSA_28    0.002564        0.256447\n",
      "             MSA_144    0.002520        0.251960\n",
      "              MSA_13    0.002437        0.243706\n",
      "               MSA_3    0.002384        0.238402\n",
      "              MSA_74    0.002267        0.226685\n",
      "              MSA_18    0.002223        0.222281\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📊 FEATURE IMPORTANCE INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 features account for 88.40% of total importance\n",
      "\n",
      "Key Insights:\n",
      "  • MIN_YEARS_EXPERIENCE: 46.87%\n",
      "    → Experience is a critical predictor of salary\n",
      "  • MAX_YEARS_EXPERIENCE: 35.07%\n",
      "    → Experience is a critical predictor of salary\n",
      "  • MSA_0: 2.69%\n",
      "    → This location significantly impacts salary predictions\n",
      "  • MSA_14: 2.44%\n",
      "    → This location significantly impacts salary predictions\n",
      "  • MSA_118: 1.33%\n",
      "    → This location significantly impacts salary predictions\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: LINEAR vs POLYNOMIAL vs RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "               Metric Linear Model Polynomial Model Random Forest\n",
      "        R² (Training)       0.4205           0.4290        0.4498\n",
      "            R² (Test)       0.3617           0.3723        0.3851\n",
      "      RMSE (Training)   $28,714.10       $28,503.40    $27,979.78\n",
      "          RMSE (Test)   $26,948.87       $26,723.22    $26,450.42\n",
      "       MAE (Training)   $21,077.77       $21,028.48    $21,139.79\n",
      "           MAE (Test)   $21,366.08       $21,256.43    $21,263.83\n",
      "Overfitting (R² diff)       0.0588           0.0567        0.0647\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "💡 KEY INSIGHTS AND RECOMMENDATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🏆 BEST PERFORMING MODEL: Random Forest\n",
      "   Test R²: 0.3851 (38.51% variance explained)\n",
      "\n",
      "📊 MODEL CHARACTERISTICS:\n",
      "\n",
      "1. LINEAR MODEL:\n",
      "   • Test R²: 0.3617\n",
      "   • Pros: Simple, interpretable coefficients, fast training\n",
      "   • Cons: Assumes linear relationships, may miss complex patterns\n",
      "   • Best for: Understanding feature effects, baseline comparisons\n",
      "\n",
      "2. POLYNOMIAL MODEL:\n",
      "   • Test R²: 0.3723\n",
      "   • Pros: Captures non-linear experience effects\n",
      "   • Cons: Only minimal improvement over linear\n",
      "   • Best for: When diminishing returns hypothesis needs testing\n",
      "\n",
      "3. RANDOM FOREST MODEL:\n",
      "   • Test R²: 0.3851\n",
      "   • Pros: Captures complex non-linear patterns, no feature scaling needed\n",
      "   • Cons: Less interpretable, longer training time\n",
      "   • Best for: Maximum predictive accuracy, feature importance analysis\n",
      "\n",
      "🎯 FINAL RECOMMENDATION:\n",
      "   ≈ Random Forest slightly better but consider tradeoffs\n",
      "   • Use RF if accuracy is paramount\n",
      "   • Use Linear if interpretability matters more\n",
      "\n",
      "================================================================================\n",
      "✓ RANDOM FOREST MODEL TRAINING AND EVALUATION COMPLETED!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST REGRESSOR MODEL TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST REGRESSOR MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL ISSUE: DATA LEAKAGE (Same as before)\n",
    "# The 'features' column includes SALARY_FROM which creates data leakage\n",
    "# We need to use 'features_clean' instead (already created in linear model)\n",
    "\n",
    "print(\"\\n⚠️  DATA LEAKAGE ISSUE:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  • Original 'features' column includes SALARY_FROM\")\n",
    "print(\"  • SALARY_FROM is part of the same salary range as target (SALARY)\")\n",
    "print(\"  • Using 'features_clean' instead (without SALARY_FROM)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Verify features_clean exists or create it\n",
    "feature_cols_clean = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE',\n",
    "                      'MSA_NAME_VEC', 'REMOTE_TYPE_NAME_VEC']\n",
    "\n",
    "assembler_clean = VectorAssembler(inputCols=feature_cols_clean, \n",
    "                                  outputCol='features_clean',\n",
    "                                  handleInvalid='keep')\n",
    "\n",
    "# Transform data with clean features (if not already done)\n",
    "if 'features_clean' not in df_train.columns:\n",
    "    df_train = assembler_clean.transform(train_data)\n",
    "    df_test = assembler_clean.transform(test_data)\n",
    "\n",
    "print(\"✓ Using 'features_clean' column WITHOUT SALARY_FROM\")\n",
    "print(f\"  Features included: {feature_cols_clean}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER SELECTION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER SELECTION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Key insight: numTrees and maxDepth are inversely proportional\n",
    "# More trees → can use shallower trees (less overfitting per tree)\n",
    "# Fewer trees → need deeper trees (capture more complexity per tree)\n",
    "\n",
    "# Strategy: Use moderate number of trees with moderate depth for balance\n",
    "NUM_TREES = 200  # Mid-range: good balance of performance and training time\n",
    "MAX_DEPTH = 7    # Mid-range: captures complexity without severe overfitting\n",
    "\n",
    "print(\"📊 HYPERPARAMETER CHOICES:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Number of Trees (numTrees): {NUM_TREES}\")\n",
    "print(f\"  • Range: 100-500 trees\")\n",
    "print(f\"  • Rationale: 200 trees provides good ensemble diversity\")\n",
    "print(f\"  • More trees = more stable predictions, longer training time\")\n",
    "print(f\"  • Diminishing returns typically after 200-300 trees\\n\")\n",
    "\n",
    "print(f\"Maximum Depth (maxDepth): {MAX_DEPTH}\")\n",
    "print(f\"  • Range: 4-10 levels\")\n",
    "print(f\"  • Rationale: Depth 7 balances complexity and generalization\")\n",
    "print(f\"  • Deeper trees = more complex patterns, higher overfitting risk\")\n",
    "print(f\"  • Shallower trees = simpler patterns, potential underfitting\\n\")\n",
    "\n",
    "print(\"🔄 INVERSE RELATIONSHIP:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  • High trees (400-500) → Use shallow depth (4-5)\")\n",
    "print(\"  •   → Many simple trees average out noise\")\n",
    "print(\"  • Low trees (100-150) → Use deeper depth (8-10)\")\n",
    "print(\"  •   → Fewer trees need more complexity each\")\n",
    "print(\"  • BALANCED APPROACH (chosen): 200 trees × depth 7\")\n",
    "print(\"  •   → Moderate ensemble with moderate complexity per tree\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE AND TRAIN RANDOM FOREST MODEL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features_clean',\n",
    "    labelCol='SALARY',\n",
    "    numTrees=NUM_TREES,\n",
    "    maxDepth=MAX_DEPTH,\n",
    "    minInstancesPerNode=1,  # Minimum samples required at leaf node\n",
    "    subsamplingRate=1.0,     # Use 100% of data for each tree (bagging)\n",
    "    seed=42,                 # For reproducibility\n",
    "    maxBins=32              # For handling categorical variables\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training Random Forest with {NUM_TREES} trees and max depth {MAX_DEPTH}...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "rf_model = rf.fit(df_train)\n",
    "\n",
    "print(\"✓ Random Forest model training completed!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST MODEL EVALUATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Make predictions on training data\n",
    "train_predictions_rf = rf_model.transform(df_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions_rf = rf_model.transform(df_test)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"=== SAMPLE PREDICTIONS (RANDOM FOREST) ===\")\n",
    "test_predictions_rf.select('SALARY', 'prediction', 'MIN_YEARS_EXPERIENCE', \n",
    "                           'MAX_YEARS_EXPERIENCE', 'MSA_NAME', 'REMOTE_TYPE_NAME').show(10)\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATE PERFORMANCE METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# Training metrics\n",
    "train_r2_rf = evaluator_r2.evaluate(train_predictions_rf)\n",
    "train_rmse_rf = evaluator_rmse.evaluate(train_predictions_rf)\n",
    "train_mae_rf = evaluator_mae.evaluate(train_predictions_rf)\n",
    "\n",
    "# Test metrics\n",
    "test_r2_rf = evaluator_r2.evaluate(test_predictions_rf)\n",
    "test_rmse_rf = evaluator_rmse.evaluate(test_predictions_rf)\n",
    "test_mae_rf = evaluator_mae.evaluate(test_predictions_rf)\n",
    "\n",
    "print(\"📊 TRAINING SET PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"R² (R-squared): {train_r2_rf:.4f}\")\n",
    "print(f\"  • Interpretation: Model explains {train_r2_rf*100:.2f}% of variance on training data\")\n",
    "print(f\"RMSE: ${train_rmse_rf:,.2f}\")\n",
    "print(f\"  • Average prediction error on training data\")\n",
    "print(f\"MAE: ${train_mae_rf:,.2f}\")\n",
    "print(f\"  • Average absolute error on training data\")\n",
    "\n",
    "print(\"\\n🎯 TEST SET PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"R² (R-squared): {test_r2_rf:.4f}\")\n",
    "print(f\"  • Interpretation: Model explains {test_r2_rf*100:.2f}% of variance on test data\")\n",
    "print(f\"RMSE: ${test_rmse_rf:,.2f}\")\n",
    "print(f\"  • Average prediction error on test data\")\n",
    "print(f\"MAE: ${test_mae_rf:,.2f}\")\n",
    "print(f\"  • Average absolute error on test data\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\n📈 OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "r2_diff = train_r2_rf - test_r2_rf\n",
    "print(f\"Training R²: {train_r2_rf:.4f}\")\n",
    "print(f\"Test R²: {test_r2_rf:.4f}\")\n",
    "print(f\"Difference: {r2_diff:.4f}\")\n",
    "\n",
    "if r2_diff < 0.05:\n",
    "    print(\"✓ Excellent generalization - minimal overfitting\")\n",
    "elif r2_diff < 0.10:\n",
    "    print(\"✓ Good generalization - acceptable overfitting\")\n",
    "elif r2_diff < 0.20:\n",
    "    print(\"⚠ Moderate overfitting - consider reducing maxDepth or increasing minInstancesPerNode\")\n",
    "else:\n",
    "    print(\"❌ Significant overfitting - model memorizing training data\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Create feature names (same as linear model)\n",
    "num_msa_categories = df_clean.select('MSA_NAME').distinct().count() - 1\n",
    "num_remote_categories = df_clean.select('REMOTE_TYPE_NAME').distinct().count() - 1\n",
    "\n",
    "feature_names_rf = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\n",
    "feature_names_rf += [f'MSA_{i}' for i in range(num_msa_categories)]\n",
    "feature_names_rf += [f'REMOTE_{i}' for i in range(num_remote_categories)]\n",
    "\n",
    "# Create DataFrame for feature importance\n",
    "importance_data = []\n",
    "for i, (name, importance) in enumerate(zip(feature_names_rf, feature_importances)):\n",
    "    importance_data.append({\n",
    "        'Feature': name,\n",
    "        'Importance': float(importance),\n",
    "        'Importance_Pct': float(importance) * 100\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=== TOP 20 MOST IMPORTANT FEATURES ===\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n📊 FEATURE IMPORTANCE INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze top features\n",
    "top_5 = importance_df.head(5)\n",
    "total_top_5_importance = top_5['Importance_Pct'].sum()\n",
    "\n",
    "print(f\"\\nTop 5 features account for {total_top_5_importance:.2f}% of total importance\")\n",
    "print(\"\\nKey Insights:\")\n",
    "for idx, row in top_5.iterrows():\n",
    "    feature = row['Feature']\n",
    "    importance = row['Importance_Pct']\n",
    "    print(f\"  • {feature}: {importance:.2f}%\")\n",
    "    \n",
    "    if 'EXPERIENCE' in feature:\n",
    "        print(f\"    → Experience is a critical predictor of salary\")\n",
    "    elif 'MSA' in feature:\n",
    "        print(f\"    → This location significantly impacts salary predictions\")\n",
    "    elif 'REMOTE' in feature:\n",
    "        print(f\"    → Work arrangement is an important factor\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON: LINEAR vs POLYNOMIAL vs RANDOM FOREST\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: LINEAR vs POLYNOMIAL vs RANDOM FOREST\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['R² (Training)', 'R² (Test)', 'RMSE (Training)', 'RMSE (Test)', \n",
    "               'MAE (Training)', 'MAE (Test)', 'Overfitting (R² diff)'],\n",
    "    'Linear Model': [f'{r2:.4f}', f'{test_r2:.4f}', f'${rmse:,.2f}', f'${test_rmse:,.2f}',\n",
    "                     f'${mae:,.2f}', f'${test_mae:,.2f}', f'{abs(r2-test_r2):.4f}'],\n",
    "    'Polynomial Model': [f'{r2_poly:.4f}', f'{test_r2_poly:.4f}', f'${rmse_poly:,.2f}', \n",
    "                         f'${test_rmse_poly:,.2f}', f'${mae_poly:,.2f}', f'${test_mae_poly:,.2f}',\n",
    "                         f'{abs(r2_poly-test_r2_poly):.4f}'],\n",
    "    'Random Forest': [f'{train_r2_rf:.4f}', f'{test_r2_rf:.4f}', f'${train_rmse_rf:,.2f}',\n",
    "                      f'${test_rmse_rf:,.2f}', f'${train_mae_rf:,.2f}', f'${test_mae_rf:,.2f}',\n",
    "                      f'{r2_diff:.4f}']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n💡 KEY INSIGHTS AND RECOMMENDATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Determine best model based on test R²\n",
    "models = {\n",
    "    'Linear': test_r2,\n",
    "    'Polynomial': test_r2_poly,\n",
    "    'Random Forest': test_r2_rf\n",
    "}\n",
    "\n",
    "best_model = max(models, key=models.get)\n",
    "best_r2 = models[best_model]\n",
    "\n",
    "print(f\"\\n🏆 BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"   Test R²: {best_r2:.4f} ({best_r2*100:.2f}% variance explained)\")\n",
    "\n",
    "print(\"\\n📊 MODEL CHARACTERISTICS:\")\n",
    "\n",
    "print(\"\\n1. LINEAR MODEL:\")\n",
    "print(f\"   • Test R²: {test_r2:.4f}\")\n",
    "print(f\"   • Pros: Simple, interpretable coefficients, fast training\")\n",
    "print(f\"   • Cons: Assumes linear relationships, may miss complex patterns\")\n",
    "print(f\"   • Best for: Understanding feature effects, baseline comparisons\")\n",
    "\n",
    "print(\"\\n2. POLYNOMIAL MODEL:\")\n",
    "print(f\"   • Test R²: {test_r2_poly:.4f}\")\n",
    "print(f\"   • Pros: Captures non-linear experience effects\")\n",
    "print(f\"   • Cons: Only minimal improvement over linear\")\n",
    "print(f\"   • Best for: When diminishing returns hypothesis needs testing\")\n",
    "\n",
    "print(\"\\n3. RANDOM FOREST MODEL:\")\n",
    "print(f\"   • Test R²: {test_r2_rf:.4f}\")\n",
    "print(f\"   • Pros: Captures complex non-linear patterns, no feature scaling needed\")\n",
    "print(f\"   • Cons: Less interpretable, longer training time\")\n",
    "print(f\"   • Best for: Maximum predictive accuracy, feature importance analysis\")\n",
    "\n",
    "print(\"\\n🎯 FINAL RECOMMENDATION:\")\n",
    "if test_r2_rf > test_r2 + 0.05:\n",
    "    print(\"   ✓ Use RANDOM FOREST for production predictions\")\n",
    "    print(\"   ✓ Use LINEAR MODEL for interpretability and explanations\")\n",
    "elif test_r2_rf > test_r2 + 0.02:\n",
    "    print(\"   ≈ Random Forest slightly better but consider tradeoffs\")\n",
    "    print(\"   • Use RF if accuracy is paramount\")\n",
    "    print(\"   • Use Linear if interpretability matters more\")\n",
    "else:\n",
    "    print(\"   ✓ Use LINEAR MODEL - simpler with similar performance\")\n",
    "    print(\"   • Random Forest doesn't justify added complexity\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ RANDOM FOREST MODEL TRAINING AND EVALUATION COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924f26a",
   "metadata": {},
   "source": [
    "# Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0087c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RANDOM FOREST FEATURE IMPORTANCE VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "Total features: 214\n",
      "Feature importance array length: 216\n",
      "\n",
      "=== TOP 10 MOST IMPORTANT FEATURES ===\n",
      "                              Feature  Importance  Importance_Pct\n",
      "                 MIN_YEARS_EXPERIENCE    0.468651       46.865076\n",
      "                 MAX_YEARS_EXPERIENCE    0.350747       35.074705\n",
      "New York-Newark-Jersey City, NY-NJ-PA    0.026873        2.687331\n",
      "   San Jose-Sunnyvale-Santa Clara, CA    0.024411        2.441104\n",
      "          Omaha-Council Bluffs, NE-IA    0.013282        1.328240\n",
      "   San Francisco-Oakland-Berkeley, CA    0.010402        1.040214\n",
      "          Seattle-Tacoma-Bellevue, WA    0.010201        1.020104\n",
      "   Chicago-Naperville-Elgin, IL-IN-WI    0.009260        0.926041\n",
      "     Austin-Round Rock-Georgetown, TX    0.007114        0.711409\n",
      "                               Remote    0.005811        0.581069\n",
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "Saving plot to file...\n",
      "✓ Kaleido found\n",
      "✓ Plot saved successfully to: _output/rf_feature_importance.png\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "📊 TOP FEATURES ANALYSIS:\n",
      "\n",
      "1. MIN_YEARS_EXPERIENCE\n",
      "   Importance: 46.87%\n",
      "   Cumulative: 46.87%\n",
      "   → Experience variables are critical predictors\n",
      "\n",
      "2. MAX_YEARS_EXPERIENCE\n",
      "   Importance: 35.07%\n",
      "   Cumulative: 81.94%\n",
      "   → Experience variables are critical predictors\n",
      "\n",
      "3. New York-Newark-Jersey City, NY-NJ-PA\n",
      "   Importance: 2.69%\n",
      "   Cumulative: 84.63%\n",
      "   → Major tech hub with significant salary premium\n",
      "\n",
      "4. San Jose-Sunnyvale-Santa Clara, CA\n",
      "   Importance: 2.44%\n",
      "   Cumulative: 87.07%\n",
      "   → Major tech hub with significant salary premium\n",
      "\n",
      "5. Omaha-Council Bluffs, NE-IA\n",
      "   Importance: 1.33%\n",
      "   Cumulative: 88.40%\n",
      "   → Location-specific salary variation\n",
      "\n",
      "6. San Francisco-Oakland-Berkeley, CA\n",
      "   Importance: 1.04%\n",
      "   Cumulative: 89.44%\n",
      "   → Major tech hub with significant salary premium\n",
      "\n",
      "7. Seattle-Tacoma-Bellevue, WA\n",
      "   Importance: 1.02%\n",
      "   Cumulative: 90.46%\n",
      "   → Major tech hub with significant salary premium\n",
      "\n",
      "8. Chicago-Naperville-Elgin, IL-IN-WI\n",
      "   Importance: 0.93%\n",
      "   Cumulative: 91.38%\n",
      "   → Location-specific salary variation\n",
      "\n",
      "9. Austin-Round Rock-Georgetown, TX\n",
      "   Importance: 0.71%\n",
      "   Cumulative: 92.09%\n",
      "   → Location-specific salary variation\n",
      "\n",
      "10. Remote\n",
      "   Importance: 0.58%\n",
      "   Cumulative: 92.68%\n",
      "   → Work arrangement impacts salary significantly\n",
      "\n",
      "Top 10 features account for 92.68% of total importance\n",
      "\n",
      "✓ Top 10 features capture majority of predictive power\n",
      "  → Model relies on relatively few key factors\n",
      "\n",
      "================================================================================\n",
      "✓ FEATURE IMPORTANCE VISUALIZATION COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "📁 View the plot at: _output/rf_feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT AND VISUALIZE RANDOM FOREST FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST FEATURE IMPORTANCE VISUALIZATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Extract feature importances from the trained model\n",
    "feature_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Get feature names\n",
    "# Extract actual category names from the pipeline\n",
    "msa_model = pipeline_model.stages[0]  # msa_indexer\n",
    "remote_model = pipeline_model.stages[1]  # remote_indexer\n",
    "\n",
    "msa_labels = msa_model.labels\n",
    "remote_labels = remote_model.labels\n",
    "\n",
    "# Create detailed feature names\n",
    "feature_names_detailed = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\n",
    "\n",
    "# Add MSA features with actual location names (excluding the last one which is baseline)\n",
    "for i in range(len(msa_labels) - 1):\n",
    "    feature_names_detailed.append(f'{msa_labels[i]}')\n",
    "\n",
    "# Add Remote Type features with actual names (excluding the last one which is baseline)\n",
    "for i in range(len(remote_labels) - 1):\n",
    "    feature_names_detailed.append(f'{remote_labels[i]}')\n",
    "\n",
    "print(f\"Total features: {len(feature_names_detailed)}\")\n",
    "print(f\"Feature importance array length: {len(feature_importances)}\")\n",
    "\n",
    "# Create DataFrame for feature importance\n",
    "importance_data = []\n",
    "for i, (name, importance) in enumerate(zip(feature_names_detailed, feature_importances)):\n",
    "    importance_data.append({\n",
    "        'Feature': name,\n",
    "        'Importance': float(importance),\n",
    "        'Importance_Pct': float(importance) * 100\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Get top 10 features\n",
    "top_10 = importance_df.head(10).copy()\n",
    "\n",
    "print(\"\\n=== TOP 10 MOST IMPORTANT FEATURES ===\")\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE PLOTLY BAR CHART\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Reverse order for horizontal bar chart (highest at top)\n",
    "top_10_reversed = top_10.iloc[::-1]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=top_10_reversed['Feature'],\n",
    "    x=top_10_reversed['Importance_Pct'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=top_10_reversed['Importance_Pct'],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(\n",
    "            title=\"Importance %\",\n",
    "            thickness=15,\n",
    "            len=0.7\n",
    "        )\n",
    "    ),\n",
    "    text=[f'{val:.2f}%' for val in top_10_reversed['Importance_Pct']],\n",
    "    textposition='outside',\n",
    "    textfont=dict(size=11),\n",
    "    hovertemplate='<b>%{y}</b><br>' +\n",
    "                  'Importance: %{x:.2f}%<br>' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'Top 10 Most Important Features - Random Forest Model',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2c3e50'}\n",
    "    },\n",
    "    xaxis_title='Feature Importance (%)',\n",
    "    yaxis_title='Feature',\n",
    "    font=dict(size=12, family='Arial, sans-serif'),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=250, r=100, t=80, b=80),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        gridwidth=0.5,\n",
    "        zeroline=True,\n",
    "        zerolinecolor='gray',\n",
    "        zerolinewidth=1\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        tickfont=dict(size=11)\n",
    "    ),\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=12,\n",
    "        font_family=\"Arial, sans-serif\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE THE PLOT\n",
    "# ============================================================================\n",
    "print(\"Saving plot to file...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"✓ Created directory: {output_dir}/\")\n",
    "\n",
    "# Save as PNG\n",
    "output_path = os.path.join(output_dir, 'rf_feature_importance.png')\n",
    "\n",
    "# Check if kaleido is available\n",
    "try:\n",
    "    import kaleido\n",
    "    print(\"✓ Kaleido found\")\n",
    "except ImportError:\n",
    "    print(\"❌ Kaleido not found. Please install: pip install kaleido\")\n",
    "    raise\n",
    "\n",
    "# Save the figure\n",
    "try:\n",
    "    fig.write_image(output_path, width=1000, height=600, scale=2)\n",
    "    print(f\"✓ Plot saved successfully to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not save as PNG: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"   1. Restart your Python kernel/session\")\n",
    "    print(\"   2. Reinstall kaleido: pip uninstall kaleido && pip install kaleido\")\n",
    "    print(\"   3. Try: pip install -U kaleido plotly\")\n",
    "    raise\n",
    "\n",
    "# Display the plot (if in interactive environment)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE INSIGHTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Calculate cumulative importance\n",
    "cumulative_importance = 0\n",
    "print(\"📊 TOP FEATURES ANALYSIS:\\n\")\n",
    "for idx, row in top_10.iterrows():\n",
    "    cumulative_importance += row['Importance_Pct']\n",
    "    print(f\"{top_10.index.get_loc(idx) + 1}. {row['Feature']}\")\n",
    "    print(f\"   Importance: {row['Importance_Pct']:.2f}%\")\n",
    "    print(f\"   Cumulative: {cumulative_importance:.2f}%\")\n",
    "    \n",
    "    # Add interpretation\n",
    "    if 'EXPERIENCE' in row['Feature']:\n",
    "        print(f\"   → Experience variables are critical predictors\")\n",
    "    elif any(city in row['Feature'] for city in ['New York', 'San Francisco', 'San Jose', 'Seattle', 'Washington']):\n",
    "        print(f\"   → Major tech hub with significant salary premium\")\n",
    "    elif 'Onsite' in row['Feature'] or 'Remote' in row['Feature']:\n",
    "        print(f\"   → Work arrangement impacts salary significantly\")\n",
    "    else:\n",
    "        print(f\"   → Location-specific salary variation\")\n",
    "    print()\n",
    "\n",
    "print(f\"Top 10 features account for {cumulative_importance:.2f}% of total importance\")\n",
    "\n",
    "if cumulative_importance > 50:\n",
    "    print(\"\\n✓ Top 10 features capture majority of predictive power\")\n",
    "    print(\"  → Model relies on relatively few key factors\")\n",
    "else:\n",
    "    print(\"\\n⚠ Top 10 features capture less than 50% of importance\")\n",
    "    print(\"  → Predictions distributed across many features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FEATURE IMPORTANCE VISUALIZATION COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: Interactive display removed for compatibility\n",
    "# The plot has been saved to the _output/ directory\n",
    "print(f\"\\n📁 View the plot at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
